<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/github.min.css"/><title>Finetune MoE with LoRA</title><meta name="next-head-count" content="4"/><link rel="preload" href="/_next/static/css/ab65974685f462b8.css" as="style"/><link rel="stylesheet" href="/_next/static/css/ab65974685f462b8.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-6cbe6e332df95288.js" defer=""></script><script src="/_next/static/chunks/main-26f9f36b33181737.js" defer=""></script><script src="/_next/static/chunks/pages/_app-2d6bf7b3192a8752.js" defer=""></script><script src="/_next/static/chunks/73-96e6cbd54826b874.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bid%5D-4b57e02c440700a5.js" defer=""></script><script src="/_next/static/4Vgqy559KM2HjheYodWT6/_buildManifest.js" defer=""></script><script src="/_next/static/4Vgqy559KM2HjheYodWT6/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="max-w-4xl px-4 mt-12 mb-24 mx-auto"><main><article><h1 class="text-3xl font-medium my-4 border-b-0">Finetune MoE with LoRA</h1><div class="text-gray-500 mb-8 pb-2 border-b-2 border-solid border-slate-300"><time dateTime="2024-08-29">August 29, 2024</time></div><div><h2>Finetune MoE with LoRA</h2>
<p>用 LoRA 训练 MoE 非常的慢，在 H 卡上 gpu 利用率不到 20%，profile 后发现主要是因为 experts 的计算太慢了。每一层有 64 个 expert，每个 expert 都是一个 mlp，包含一个 gate linear layer、up linear layer 和 down linear layer。</p>
<p><img src="/moe_lora/image.png" alt="alt text"></p>
<p><img src="/moe_lora/image-2.png" alt="alt text"></p>
<p>对于每一个 linear layer，LoRA 会再注入两个小 linear layer，如下图。比如原本的是 <code>nn.Linear(1024, 1024)</code>，rank 为 16 的 LoRA 会注入 <code>nn.Linear(1024, 16)</code> 和 <code>nn.Linear(16, 1024)</code>。</p>
<p><img src="/moe_lora/image-1.png" alt="alt text"></p>
<p><img src="/moe_lora/image-4.png" alt="alt text"></p>
<p>LoRA 的 linear 是一些更小的 linear layer，对于 GPU 来说是 memory bound 的，导致 GPU 的利用率很低。</p>
<p>优化这一块，很容易想到使用 grouped gemm，这也是 MoE 的基操。</p>
<h2>Grouped GEMM</h2>
<p>当前 experts 是一个 <code>SequentialMLP</code>，假设有 64 个 expert，for 循环这些 expert，且分配给每个 expert 的 token 数不一样， 使得 expert 接收的输入 shape 不一样</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">SequentialMLP</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config</span>):
        <span class="hljs-built_in">super</span>().__init__()
        self.config = config
        self.experts = nn.ModuleList(
            [MLP(config) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(config.moe_num_experts)]
        )

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, permuted_tokens, tokens_per_expert</span>):
        output = torch.zeros_like(permuted_tokens)

        cumsum_num_tokens = torch.cumsum(tokens_per_expert, dim=<span class="hljs-number">0</span>)
        <span class="hljs-comment"># Insert zero at the begining for offset index's convenience</span>
        zero_tensor = torch.zeros(<span class="hljs-number">1</span>, dtype=torch.long, device=cumsum_num_tokens.device)
        cumsum_num_tokens = torch.cat((zero_tensor, cumsum_num_tokens))

        <span class="hljs-keyword">for</span> expert_num, expert <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(self.experts):
            start = cumsum_num_tokens[expert_num]
            end = cumsum_num_tokens[expert_num + <span class="hljs-number">1</span>]
            tokens = permuted_tokens[start:end]

            out = expert(tokens)
            output[start:end] = out
        <span class="hljs-keyword">return</span> output
</code></pre>
<p>这样做有两个性能问题：一是非常多的 kernel launch，二是每个 expert 接收的输入 shape 不一样，每个 expert 的 workload 不一样，而且使用 LoRA 后这个问题更加严重，
每个 LoRA layer 都是很小的 linear，会导致更多的 SM 空闲。这里的问题就是我们上一篇文章中讨论的 <a href="https://xffxff.github.io/posts/how_to_write_a_cuda_program">Wave Quantization</a></p>
<p><img src="/moe_lora/image-5.png" alt="alt text"></p>
<p>那如果我们所有 expert 的 workload 合到一起，只需一次 kernel launch，work load 也变大了，在 SM 上有更好的 load balance，就可以极大减少 SM 的空闲。</p>
<p><img src="/moe_lora/image-6.png" alt="alt text"></p>
<p>可以参考 <a href="https://triton-lang.org/main/getting-started/tutorials/08-grouped-gemm.html">Grouped GEMM</a> 的 triton 实现，Grouped GEMM 非常符合 <a href="https://xffxff.github.io/posts/how_to_write_a_cuda_program">上一篇文章</a> 中 “Don't map threads to data; map data to threads” 的思想，map data to SM！</p>
<h2>Finetune with Grouped GEMM</h2>
<pre><code class="hljs language-python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">GroupedMLP</span>(nn.Module):

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config: MoYIConfig</span>):
        <span class="hljs-built_in">super</span>().__init__()
        self.config = config

        fc1_output_size = config.moe_intermediate_size * config.moe_num_experts
        <span class="hljs-keyword">if</span> config.hidden_act == <span class="hljs-string">"silu"</span>:
            fc1_output_size *= <span class="hljs-number">2</span>

        fc2_input_size = config.moe_intermediate_size * config.moe_num_experts
        self.weight1 = nn.Parameter(torch.empty(config.hidden_size, fc1_output_size))

        self.weight2 = nn.Parameter(torch.empty(fc2_input_size, config.hidden_size))

        <span class="hljs-keyword">def</span> <span class="hljs-title function_">glu</span>(<span class="hljs-params">x</span>):
            x = torch.chunk(x, <span class="hljs-number">2</span>, dim=-<span class="hljs-number">1</span>)
            <span class="hljs-keyword">return</span> F.silu(x[<span class="hljs-number">0</span>]) * x[<span class="hljs-number">1</span>]

        self.activation_func = glu

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, permuted_tokens, tokens_per_expert</span>):
        <span class="hljs-keyword">from</span> grouped_gemm <span class="hljs-keyword">import</span> ops

        w1 = self.weight1.view(self.config.moe_num_experts, self.config.hidden_size, -<span class="hljs-number">1</span>)
        w2 = self.weight2.view(self.config.moe_num_experts, -<span class="hljs-number">1</span>, self.config.hidden_size)

        fc1_output = ops.gmm(permuted_tokens, w1, tokens_per_expert, trans_b=<span class="hljs-literal">False</span>)

        fc1_output = self.activation_func(fc1_output)

        fc2_output = ops.gmm(fc1_output, w2, tokens_per_expert, trans_b=<span class="hljs-literal">False</span>)
        <span class="hljs-keyword">return</span> fc2_output
</code></pre>
<p>但是使用 Grouped GEMM 的话，<a href="https://huggingface.co/docs/peft/index">PEFT</a> 这个库的 LoRA 不支持 <code>GroupedMLP</code> 这个 moudle。尝试使用 <a href="https://huggingface.co/docs/peft/v0.12.0/en/developer_guides/custom_models#experimental-support-for-dynamic-dispatch-of-custom-modules-in-lora">custom models</a>，经过一番尝试，我们对 <code>GroupedMLP</code> 进行了修改</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">GroupedGEMM</span>(nn.Module):

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_features, out_features, groups</span>):
        <span class="hljs-built_in">super</span>().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.groups = groups
        self.weight = nn.Parameter(torch.empty(groups, in_features, out_features))

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span>, tokens_per_expert</span>):
        <span class="hljs-keyword">from</span> grouped_gemm <span class="hljs-keyword">import</span> ops
        <span class="hljs-keyword">return</span> ops.gmm(<span class="hljs-built_in">input</span>, self.weight, tokens_per_expert)


<span class="hljs-keyword">class</span> <span class="hljs-title class_">GroupedMLP</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config: MoYIConfig</span>) -> <span class="hljs-literal">None</span>:
        <span class="hljs-built_in">super</span>().__init__()
        self.config = config
        self.fc1 = GroupedGEMM(
            config.hidden_size, config.moe_intermediate_size * <span class="hljs-number">2</span>, config.moe_num_experts
        )
        self.fc2 = GroupedGEMM(
            config.moe_intermediate_size, config.hidden_size, config.moe_num_experts
        )

        <span class="hljs-keyword">def</span> <span class="hljs-title function_">glu</span>(<span class="hljs-params">x</span>):
            x = torch.chunk(x, <span class="hljs-number">2</span>, dim=-<span class="hljs-number">1</span>)
            <span class="hljs-keyword">return</span> F.silu(x[<span class="hljs-number">0</span>]) * x[<span class="hljs-number">1</span>]

        self.activation_func = glu

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, permuted_tokens, tokens_per_expert</span>):
        fc1_output = self.fc1(permuted_tokens, tokens_per_expert)
        fc1_output = self.activation_func(fc1_output)
        fc2_output = self.fc2(fc1_output, tokens_per_expert)
        <span class="hljs-keyword">return</span> fc2_output
</code></pre>
<p>定义了一个 <code>GroupedGEMM</code>，这个 module 和 <code>nn.Linear</code> 的接口相似，下面就可以仿照 <a href="https://github.com/huggingface/peft/blob/850eeb5c3a5cf692f5612c7c733b13fde184e05d/src/peft/tuners/lora/layer.py#L374"><code>lora.layer.Linear</code></a> 去定义 <code>GroupedGEMM</code> 的 LoRA 了。</p>
<h2>Reference</h2>
<ul>
<li><a href="https://triton-lang.org/main/getting-started/tutorials/08-grouped-gemm.html">Grouped GEMM implemented in Triton</a></li>
<li><a href="https://triton-lang.org/main/getting-started/tutorials/08-grouped-gemm.html">video: Write Grouped GEMMs in Triton Nvidia</a></li>
</ul></div></article></main><div class="mt-12"><a href="/">← 返回首页</a></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"postData":{"id":"finetune_moe_with_lora","contentHtml":"\u003ch2\u003eFinetune MoE with LoRA\u003c/h2\u003e\n\u003cp\u003e用 LoRA 训练 MoE 非常的慢，在 H 卡上 gpu 利用率不到 20%，profile 后发现主要是因为 experts 的计算太慢了。每一层有 64 个 expert，每个 expert 都是一个 mlp，包含一个 gate linear layer、up linear layer 和 down linear layer。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/moe_lora/image.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/moe_lora/image-2.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cp\u003e对于每一个 linear layer，LoRA 会再注入两个小 linear layer，如下图。比如原本的是 \u003ccode\u003enn.Linear(1024, 1024)\u003c/code\u003e，rank 为 16 的 LoRA 会注入 \u003ccode\u003enn.Linear(1024, 16)\u003c/code\u003e 和 \u003ccode\u003enn.Linear(16, 1024)\u003c/code\u003e。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/moe_lora/image-1.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/moe_lora/image-4.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cp\u003eLoRA 的 linear 是一些更小的 linear layer，对于 GPU 来说是 memory bound 的，导致 GPU 的利用率很低。\u003c/p\u003e\n\u003cp\u003e优化这一块，很容易想到使用 grouped gemm，这也是 MoE 的基操。\u003c/p\u003e\n\u003ch2\u003eGrouped GEMM\u003c/h2\u003e\n\u003cp\u003e当前 experts 是一个 \u003ccode\u003eSequentialMLP\u003c/code\u003e，假设有 64 个 expert，for 循环这些 expert，且分配给每个 expert 的 token 数不一样， 使得 expert 接收的输入 shape 不一样\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eSequentialMLP\u003c/span\u003e(nn.Module):\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, config\u003c/span\u003e):\n        \u003cspan class=\"hljs-built_in\"\u003esuper\u003c/span\u003e().__init__()\n        self.config = config\n        self.experts = nn.ModuleList(\n            [MLP(config) \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e _ \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003erange\u003c/span\u003e(config.moe_num_experts)]\n        )\n\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eforward\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, permuted_tokens, tokens_per_expert\u003c/span\u003e):\n        output = torch.zeros_like(permuted_tokens)\n\n        cumsum_num_tokens = torch.cumsum(tokens_per_expert, dim=\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)\n        \u003cspan class=\"hljs-comment\"\u003e# Insert zero at the begining for offset index's convenience\u003c/span\u003e\n        zero_tensor = torch.zeros(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, dtype=torch.long, device=cumsum_num_tokens.device)\n        cumsum_num_tokens = torch.cat((zero_tensor, cumsum_num_tokens))\n\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e expert_num, expert \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003eenumerate\u003c/span\u003e(self.experts):\n            start = cumsum_num_tokens[expert_num]\n            end = cumsum_num_tokens[expert_num + \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]\n            tokens = permuted_tokens[start:end]\n\n            out = expert(tokens)\n            output[start:end] = out\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e output\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e这样做有两个性能问题：一是非常多的 kernel launch，二是每个 expert 接收的输入 shape 不一样，每个 expert 的 workload 不一样，而且使用 LoRA 后这个问题更加严重，\n每个 LoRA layer 都是很小的 linear，会导致更多的 SM 空闲。这里的问题就是我们上一篇文章中讨论的 \u003ca href=\"https://xffxff.github.io/posts/how_to_write_a_cuda_program\"\u003eWave Quantization\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/moe_lora/image-5.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cp\u003e那如果我们所有 expert 的 workload 合到一起，只需一次 kernel launch，work load 也变大了，在 SM 上有更好的 load balance，就可以极大减少 SM 的空闲。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/moe_lora/image-6.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cp\u003e可以参考 \u003ca href=\"https://triton-lang.org/main/getting-started/tutorials/08-grouped-gemm.html\"\u003eGrouped GEMM\u003c/a\u003e 的 triton 实现，Grouped GEMM 非常符合 \u003ca href=\"https://xffxff.github.io/posts/how_to_write_a_cuda_program\"\u003e上一篇文章\u003c/a\u003e 中 “Don't map threads to data; map data to threads” 的思想，map data to SM！\u003c/p\u003e\n\u003ch2\u003eFinetune with Grouped GEMM\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eGroupedMLP\u003c/span\u003e(nn.Module):\n\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, config: MoYIConfig\u003c/span\u003e):\n        \u003cspan class=\"hljs-built_in\"\u003esuper\u003c/span\u003e().__init__()\n        self.config = config\n\n        fc1_output_size = config.moe_intermediate_size * config.moe_num_experts\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e config.hidden_act == \u003cspan class=\"hljs-string\"\u003e\"silu\"\u003c/span\u003e:\n            fc1_output_size *= \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e\n\n        fc2_input_size = config.moe_intermediate_size * config.moe_num_experts\n        self.weight1 = nn.Parameter(torch.empty(config.hidden_size, fc1_output_size))\n\n        self.weight2 = nn.Parameter(torch.empty(fc2_input_size, config.hidden_size))\n\n        \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eglu\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003ex\u003c/span\u003e):\n            x = torch.chunk(x, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, dim=-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\n            \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e F.silu(x[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]) * x[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]\n\n        self.activation_func = glu\n\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eforward\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, permuted_tokens, tokens_per_expert\u003c/span\u003e):\n        \u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e grouped_gemm \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e ops\n\n        w1 = self.weight1.view(self.config.moe_num_experts, self.config.hidden_size, -\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\n        w2 = self.weight2.view(self.config.moe_num_experts, -\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, self.config.hidden_size)\n\n        fc1_output = ops.gmm(permuted_tokens, w1, tokens_per_expert, trans_b=\u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e)\n\n        fc1_output = self.activation_func(fc1_output)\n\n        fc2_output = ops.gmm(fc1_output, w2, tokens_per_expert, trans_b=\u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e)\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e fc2_output\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e但是使用 Grouped GEMM 的话，\u003ca href=\"https://huggingface.co/docs/peft/index\"\u003ePEFT\u003c/a\u003e 这个库的 LoRA 不支持 \u003ccode\u003eGroupedMLP\u003c/code\u003e 这个 moudle。尝试使用 \u003ca href=\"https://huggingface.co/docs/peft/v0.12.0/en/developer_guides/custom_models#experimental-support-for-dynamic-dispatch-of-custom-modules-in-lora\"\u003ecustom models\u003c/a\u003e，经过一番尝试，我们对 \u003ccode\u003eGroupedMLP\u003c/code\u003e 进行了修改\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eGroupedGEMM\u003c/span\u003e(nn.Module):\n\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, in_features, out_features, groups\u003c/span\u003e):\n        \u003cspan class=\"hljs-built_in\"\u003esuper\u003c/span\u003e().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.groups = groups\n        self.weight = nn.Parameter(torch.empty(groups, in_features, out_features))\n\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eforward\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, \u003cspan class=\"hljs-built_in\"\u003einput\u003c/span\u003e, tokens_per_expert\u003c/span\u003e):\n        \u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e grouped_gemm \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e ops\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e ops.gmm(\u003cspan class=\"hljs-built_in\"\u003einput\u003c/span\u003e, self.weight, tokens_per_expert)\n\n\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eGroupedMLP\u003c/span\u003e(nn.Module):\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, config: MoYIConfig\u003c/span\u003e) -\u003e \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e:\n        \u003cspan class=\"hljs-built_in\"\u003esuper\u003c/span\u003e().__init__()\n        self.config = config\n        self.fc1 = GroupedGEMM(\n            config.hidden_size, config.moe_intermediate_size * \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, config.moe_num_experts\n        )\n        self.fc2 = GroupedGEMM(\n            config.moe_intermediate_size, config.hidden_size, config.moe_num_experts\n        )\n\n        \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eglu\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003ex\u003c/span\u003e):\n            x = torch.chunk(x, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, dim=-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\n            \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e F.silu(x[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]) * x[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]\n\n        self.activation_func = glu\n\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eforward\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, permuted_tokens, tokens_per_expert\u003c/span\u003e):\n        fc1_output = self.fc1(permuted_tokens, tokens_per_expert)\n        fc1_output = self.activation_func(fc1_output)\n        fc2_output = self.fc2(fc1_output, tokens_per_expert)\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e fc2_output\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e定义了一个 \u003ccode\u003eGroupedGEMM\u003c/code\u003e，这个 module 和 \u003ccode\u003enn.Linear\u003c/code\u003e 的接口相似，下面就可以仿照 \u003ca href=\"https://github.com/huggingface/peft/blob/850eeb5c3a5cf692f5612c7c733b13fde184e05d/src/peft/tuners/lora/layer.py#L374\"\u003e\u003ccode\u003elora.layer.Linear\u003c/code\u003e\u003c/a\u003e 去定义 \u003ccode\u003eGroupedGEMM\u003c/code\u003e 的 LoRA 了。\u003c/p\u003e\n\u003ch2\u003eReference\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://triton-lang.org/main/getting-started/tutorials/08-grouped-gemm.html\"\u003eGrouped GEMM implemented in Triton\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://triton-lang.org/main/getting-started/tutorials/08-grouped-gemm.html\"\u003evideo: Write Grouped GEMMs in Triton Nvidia\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","title":"Finetune MoE with LoRA","date":"2024-08-29"}},"__N_SSG":true},"page":"/posts/[id]","query":{"id":"finetune_moe_with_lora"},"buildId":"4Vgqy559KM2HjheYodWT6","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>