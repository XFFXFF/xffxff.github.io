<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/github.min.css"/><title>RWKV</title><meta name="next-head-count" content="4"/><link rel="preload" href="/_next/static/css/ab65974685f462b8.css" as="style"/><link rel="stylesheet" href="/_next/static/css/ab65974685f462b8.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-6cbe6e332df95288.js" defer=""></script><script src="/_next/static/chunks/main-26f9f36b33181737.js" defer=""></script><script src="/_next/static/chunks/pages/_app-2d6bf7b3192a8752.js" defer=""></script><script src="/_next/static/chunks/73-96e6cbd54826b874.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bid%5D-4b57e02c440700a5.js" defer=""></script><script src="/_next/static/LgI_2aaq_4GSrY0V2y1Bt/_buildManifest.js" defer=""></script><script src="/_next/static/LgI_2aaq_4GSrY0V2y1Bt/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="max-w-4xl px-4 mt-12 mb-24 mx-auto"><main><article><h1 class="text-3xl font-medium my-4 border-b-0">RWKV</h1><div class="text-gray-500 mb-8 pb-2 border-b-2 border-solid border-slate-300"><time dateTime="2024-01-06">January 6, 2024</time></div><div><p>Transformer self-attention 的时间复杂度是 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>T</mi><mn>2</mn></msup><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(T^2d)</annotation></semantics></math></span>， 空间复杂度是 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>T</mi><mn>2</mn></msup><mo>+</mo><mi>T</mi><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(T^2 + Td)</annotation></semantics></math></span>， T 是序列长度，d 是 hidden size。而 RWKV 可以将<strong>推理</strong>的时间复杂度降低到 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>T</mi><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(Td)</annotation></semantics></math></span>，空间复杂度降低到 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(d)</annotation></semantics></math></span>。</p>
<blockquote>
<p>这里强调了“推理”，训练的话，我还搞不清楚</p>
</blockquote>
<p>在 RWKV 之前，有一些对不同 Transformer 架构也是为了降低 attention 的时间复杂度或者空间复杂度。比如 Linear Transformer 和 AFT（Attention Free Transformer）。</p>
<p><img src="/different_transformers.png" alt="">
<em>图片来源：<a href="https://arxiv.org/pdf/2305.13048.pdf">https://arxiv.org/pdf/2305.13048.pdf</a></em></p>
<p>Linear Transformer 的核心思想去掉 scaled-dot attention 中的 softmax，这样计算 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup><mi>V</mi></mrow><annotation encoding="application/x-tex">QK^TV</annotation></semantics></math></span> 时，可以先算 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>K</mi><mi>T</mi></msup><mi>V</mi></mrow><annotation encoding="application/x-tex">K^TV</annotation></semantics></math></span>，这样时间复杂度就变成了 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>T</mi><msup><mi>d</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(Td^2)</annotation></semantics></math></span>。建议阅读 <a href="https://spaces.ac.cn/archives/7546">线性Attention的探索：Attention必须有个Softmax吗？</a></p>
<p>AFT 的一个改变是在做 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span>，<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span>，<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span> 的相关计算时，不要用 dot product 了，而是改用 element-wise product。</p>
<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>Y</mi><mi>t</mi></msub><mo>=</mo><msub><mi>σ</mi><mi>q</mi></msub><mo stretchy="false">(</mo><msub><mi>Q</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo>⊙</mo><mfrac><mrow><munderover><mo>∑</mo><mrow><msup><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>K</mi><msup><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></msub><mo>+</mo><msub><mi>w</mi><mrow><mi>t</mi><mo separator="true">,</mo><msup><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow></msub><mo stretchy="false">)</mo><mo>⊙</mo><msub><mi>V</mi><msup><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></msub></mrow><mrow><munderover><mo>∑</mo><mrow><msup><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>K</mi><msup><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></msub><mo>+</mo><msub><mi>w</mi><mrow><mi>t</mi><mo separator="true">,</mo><msup><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow></msub><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">Y_t = \sigma_q(Q_t) \odot \frac{\sum_{t'=1}^T \exp(K_{t'} + w_{t,t'}) \odot V_{t'}}{\sum_{t'=1}^T \exp(K_{t'} + w_{t,t'})}</annotation></semantics></math></span>
<p>这样做相对于 Transformer 的时间复杂度并没有改变，仍然是 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>T</mi><mn>2</mn></msup><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(T^2d)</annotation></semantics></math></span>（<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">K_t</annotation></semantics></math></span> 和 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>V</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">V_t</annotation></semantics></math></span> element wise 的乘法是 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(d)</annotation></semantics></math></span>，对 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span> 求和是 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(T)</annotation></semantics></math></span>，有 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span> 个 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Y</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">Y_t</annotation></semantics></math></span>，所以是 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>T</mi><mn>2</mn></msup><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(T^2d)</annotation></semantics></math></span>），但是空间复杂度降低到 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>T</mi><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(Td)</annotation></semantics></math></span>，因为不需要保存 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">QK^T</annotation></semantics></math></span> 生成的 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo>×</mo><mi>T</mi></mrow><annotation encoding="application/x-tex">T \times T</annotation></semantics></math></span> 的矩阵。建议阅读 <a href="https://zhuanlan.zhihu.com/p/614311961">RWKV的RNN CNN二象性</a> 中关于 AFT 的部分。</p>
<blockquote>
<p>只有在 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span> 足够大的时候，Transformer 的 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>T</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">T^2</annotation></semantics></math></span> 时间复杂度和空间复杂度才是值得重视的问题，我们上面都是在分析 Transformer attention 相关的复杂度，实际上在 LLM 中，当 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span> 没有足够大的时候，FFN 层的计算量可能会比 attention 层的计算量大很多。推荐阅读：<a href="https://spaces.ac.cn/archives/8610">线性Transformer应该不是你要等的那个模型</a></p>
</blockquote>
<h2>RWKV</h2>
<h3>RWKV 和 RNN 的关系</h3>
<p><em>Reinventing RNNs for the Transformer Era</em>，RWKV 的标题非常霸气，RWKV 真的是一个传统 RNN 模型吗？</p>
<p>RWKV 的新颖之处在于它的 “attention”（RWKV 中叫做 WKV） 可以写成 RNN 的形式</p>
<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>w</mi><mi>k</mi><msub><mi>v</mi><mi>t</mi></msub><mo>=</mo><mfrac><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></munderover><msup><mi>e</mi><mrow><mo>−</mo><mo stretchy="false">(</mo><mi>t</mi><mo>−</mo><mn>1</mn><mo>−</mo><mi>i</mi><mo stretchy="false">)</mo><mi>w</mi><mo>+</mo><msub><mi>k</mi><mi>i</mi></msub></mrow></msup><mo>⊙</mo><msub><mi>v</mi><mi>i</mi></msub><mo>+</mo><msup><mi>e</mi><mrow><mi>u</mi><mo>+</mo><msub><mi>k</mi><mi>t</mi></msub></mrow></msup><mo>⊙</mo><msub><mi>v</mi><mi>t</mi></msub></mrow><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></munderover><msup><mi>e</mi><mrow><mo>−</mo><mo stretchy="false">(</mo><mi>t</mi><mo>−</mo><mn>1</mn><mo>−</mo><mi>i</mi><mo stretchy="false">)</mo><mi>w</mi><mo>+</mo><msub><mi>k</mi><mi>i</mi></msub></mrow></msup><mo>+</mo><msup><mi>e</mi><mrow><mi>u</mi><mo>+</mo><msub><mi>k</mi><mi>t</mi></msub></mrow></msup></mrow></mfrac><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">wk v_t = \frac{\sum_{i=1}^{t-1} e^{-(t-1-i)w+k_i} \odot v_i + e^{u+k_t} \odot v_t}{\sum_{i=1}^{t-1} e^{-(t-1-i)w+k_i} + e^{u+k_t}}.</annotation></semantics></math></span>
<p>令</p>
<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>α</mi><mi>t</mi></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></munderover><msup><mi>e</mi><mrow><mo>−</mo><mo stretchy="false">(</mo><mi>t</mi><mo>−</mo><mi>i</mi><mo stretchy="false">)</mo><mi>w</mi><mo>+</mo><msub><mi>k</mi><mi>i</mi></msub></mrow></msup><mo>⊙</mo><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\alpha_{t} = \sum_{i=1}^{t} e^{-(t-i)w+k_i} \odot v_i </annotation></semantics></math></span>
<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>β</mi><mi>t</mi></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></munderover><msup><mi>e</mi><mrow><mo>−</mo><mo stretchy="false">(</mo><mi>t</mi><mo>−</mo><mi>i</mi><mo stretchy="false">)</mo><mi>w</mi><mo>+</mo><msub><mi>k</mi><mi>i</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\beta_{t} = \sum_{i=1}^{t} e^{-(t-i)w+k_i}</annotation></semantics></math></span>
<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>w</mi><mi>k</mi><msub><mi>v</mi><mi>t</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>α</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><msup><mi>e</mi><mrow><mi>u</mi><mo>+</mo><msub><mi>k</mi><mi>t</mi></msub></mrow></msup><mo>⊙</mo><msub><mi>v</mi><mi>t</mi></msub></mrow><mrow><msub><mi>β</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><msup><mi>e</mi><mrow><mi>u</mi><mo>+</mo><msub><mi>k</mi><mi>t</mi></msub></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">wk v_t = \frac{\alpha_{t-1} + e^{u+k_t} \odot v_t}{\beta_{t-1} + e^{u+k_t}}</annotation></semantics></math></span>
<p>写成 RNN 形式的好处是计算 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mi>k</mi><msub><mi>v</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">wk v_t</annotation></semantics></math></span> 的时候可以利用之前的状态 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\alpha_{t-1}</annotation></semantics></math></span> 和 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\beta_{t-1}</annotation></semantics></math></span>，这样计算 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mi>k</mi><msub><mi>v</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">wk v_t</annotation></semantics></math></span> 的时间复杂度就是 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(d)</annotation></semantics></math></span>，空间复杂度也是 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(d)</annotation></semantics></math></span>。</p>
<p>推荐阅读 <a href="https://zhuanlan.zhihu.com/p/614311961">RWKV的RNN CNN二象性</a> 中关于 RWKV 的部分，作者清晰的解释了 RWKV 和 AFT 的关系，以及如何直观理解 wkv。</p>
<h3>Token Shift</h3>
<p>传统的 Transformer 在 self-attention 之前会对输入 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span> 做 linear projections 得到 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span>，<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span>，<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span>。RWKV 的不同之处在于并不是直接对 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span> 做 linear projection，而是对 current inputs 和 previous inputs 做一个线性插值后再做 linear projection。这个线性插值的过程就是 token shift。即：</p>
<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>r</mi><mi>t</mi></msub><mo>=</mo><msub><mi>W</mi><mi>r</mi></msub><mo>⋅</mo><mo stretchy="false">(</mo><msub><mi>μ</mi><mi>r</mi></msub><mo>⊙</mo><msub><mi>x</mi><mi>t</mi></msub><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msub><mi>μ</mi><mi>r</mi></msub><mo stretchy="false">)</mo><mo>⊙</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">r_t = W_{r} \cdot (\mu_r \odot x_t + (1 - \mu_r) \odot x_{t-1})</annotation></semantics></math></span>
<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>k</mi><mi>t</mi></msub><mo>=</mo><msub><mi>W</mi><mi>k</mi></msub><mo>⋅</mo><mo stretchy="false">(</mo><msub><mi>μ</mi><mi>k</mi></msub><mo>⊙</mo><msub><mi>x</mi><mi>t</mi></msub><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msub><mi>μ</mi><mi>k</mi></msub><mo stretchy="false">)</mo><mo>⊙</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">k_t = W_{k} \cdot (\mu_k \odot x_t + (1 - \mu_k) \odot x_{t-1})</annotation></semantics></math></span>
<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>v</mi><mi>t</mi></msub><mo>=</mo><msub><mi>W</mi><mi>v</mi></msub><mo>⋅</mo><mo stretchy="false">(</mo><msub><mi>μ</mi><mi>v</mi></msub><mo>⊙</mo><msub><mi>x</mi><mi>t</mi></msub><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msub><mi>μ</mi><mi>v</mi></msub><mo stretchy="false">)</mo><mo>⊙</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">v_t = W_{v} \cdot (\mu_v \odot x_t + (1 - \mu_v) \odot x_{t-1})</annotation></semantics></math></span>
<p>感觉 token shift 很像 kernel size 为 2 的卷积</p>
<p><img src="/token_shift.png" alt=""></p>
<p>作者几年前就提出了 token shift 的想法，参见 <a href="https://zhuanlan.zhihu.com/p/399480671">Time-shift: 一行代码，免费提高 Transformer 性能（无参数，无耗时）</a></p>
<h3>局限性</h3>
<ul>
<li>超长上下文任务上效果受限：RWKV 这种递归架构限制了它回顾之前 token 的能力，不像 self-attention 可以保留所有 token 的信息。</li>
<li>对 prompt 比较敏感：用苏剑林的话说，RWKV 只会做闭卷考试，不会做开卷考试（不会往前翻书），prompt 中在一开始描述任务比较好，带着问题去阅读后续内容。Prompt "For the document below do X" 好于 "For the document above do X"。参考<a href="https://www.zhihu.com/question/602564718/answer/3062973388">苏剑林的回答</a></li>
</ul></div></article></main><div class="mt-12"><a href="/">← 返回首页</a></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"postData":{"id":"rwkv","contentHtml":"\u003cp\u003eTransformer self-attention 的时间复杂度是 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003eT\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msup\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(T^2d)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e， 空间复杂度是 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003eT\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msup\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmi\u003eT\u003c/mi\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(T^2 + Td)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e， T 是序列长度，d 是 hidden size。而 RWKV 可以将\u003cstrong\u003e推理\u003c/strong\u003e的时间复杂度降低到 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003eT\u003c/mi\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(Td)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e，空间复杂度降低到 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(d)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e这里强调了“推理”，训练的话，我还搞不清楚\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e在 RWKV 之前，有一些对不同 Transformer 架构也是为了降低 attention 的时间复杂度或者空间复杂度。比如 Linear Transformer 和 AFT（Attention Free Transformer）。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/different_transformers.png\" alt=\"\"\u003e\n\u003cem\u003e图片来源：\u003ca href=\"https://arxiv.org/pdf/2305.13048.pdf\"\u003ehttps://arxiv.org/pdf/2305.13048.pdf\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eLinear Transformer 的核心思想去掉 scaled-dot attention 中的 softmax，这样计算 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eQ\u003c/mi\u003e\u003cmsup\u003e\u003cmi\u003eK\u003c/mi\u003e\u003cmi\u003eT\u003c/mi\u003e\u003c/msup\u003e\u003cmi\u003eV\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eQK^TV\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e 时，可以先算 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsup\u003e\u003cmi\u003eK\u003c/mi\u003e\u003cmi\u003eT\u003c/mi\u003e\u003c/msup\u003e\u003cmi\u003eV\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eK^TV\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e，这样时间复杂度就变成了 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003eT\u003c/mi\u003e\u003cmsup\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msup\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(Td^2)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e。建议阅读 \u003ca href=\"https://spaces.ac.cn/archives/7546\"\u003e线性Attention的探索：Attention必须有个Softmax吗？\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eAFT 的一个改变是在做 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eQ\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eQ\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e，\u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eK\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eK\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e，\u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eV\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eV\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e 的相关计算时，不要用 dot product 了，而是改用 element-wise product。\u003c/p\u003e\n\u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eY\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eσ\u003c/mi\u003e\u003cmi\u003eq\u003c/mi\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eQ\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo\u003e⊙\u003c/mo\u003e\u003cmfrac\u003e\u003cmrow\u003e\u003cmunderover\u003e\u003cmo\u003e∑\u003c/mo\u003e\u003cmrow\u003e\u003cmsup\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\"\u003e′\u003c/mo\u003e\u003c/msup\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cmi\u003eT\u003c/mi\u003e\u003c/munderover\u003e\u003cmi\u003eexp\u003c/mi\u003e\u003cmo\u003e⁡\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eK\u003c/mi\u003e\u003cmsup\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\"\u003e′\u003c/mo\u003e\u003c/msup\u003e\u003c/msub\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ew\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\"\u003e′\u003c/mo\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo\u003e⊙\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eV\u003c/mi\u003e\u003cmsup\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\"\u003e′\u003c/mo\u003e\u003c/msup\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cmrow\u003e\u003cmunderover\u003e\u003cmo\u003e∑\u003c/mo\u003e\u003cmrow\u003e\u003cmsup\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\"\u003e′\u003c/mo\u003e\u003c/msup\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cmi\u003eT\u003c/mi\u003e\u003c/munderover\u003e\u003cmi\u003eexp\u003c/mi\u003e\u003cmo\u003e⁡\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eK\u003c/mi\u003e\u003cmsup\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\"\u003e′\u003c/mo\u003e\u003c/msup\u003e\u003c/msub\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ew\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\"\u003e′\u003c/mo\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003c/mfrac\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eY_t = \\sigma_q(Q_t) \\odot \\frac{\\sum_{t'=1}^T \\exp(K_{t'} + w_{t,t'}) \\odot V_{t'}}{\\sum_{t'=1}^T \\exp(K_{t'} + w_{t,t'})}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\n\u003cp\u003e这样做相对于 Transformer 的时间复杂度并没有改变，仍然是 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003eT\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msup\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(T^2d)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e（\u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eK\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eK_t\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e 和 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eV\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eV_t\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e element wise 的乘法是 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(d)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e，对 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eT\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eT\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e 求和是 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003eT\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(T)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e，有 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eT\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eT\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e 个 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eY\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eY_t\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e，所以是 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003eT\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msup\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(T^2d)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e），但是空间复杂度降低到 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003eT\u003c/mi\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(Td)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e，因为不需要保存 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eQ\u003c/mi\u003e\u003cmsup\u003e\u003cmi\u003eK\u003c/mi\u003e\u003cmi\u003eT\u003c/mi\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eQK^T\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e 生成的 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eT\u003c/mi\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmi\u003eT\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eT \\times T\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e 的矩阵。建议阅读 \u003ca href=\"https://zhuanlan.zhihu.com/p/614311961\"\u003eRWKV的RNN CNN二象性\u003c/a\u003e 中关于 AFT 的部分。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e只有在 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eT\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eT\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e 足够大的时候，Transformer 的 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsup\u003e\u003cmi\u003eT\u003c/mi\u003e\u003cmn\u003e2\u003c/mn\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eT^2\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e 时间复杂度和空间复杂度才是值得重视的问题，我们上面都是在分析 Transformer attention 相关的复杂度，实际上在 LLM 中，当 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eT\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eT\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e 没有足够大的时候，FFN 层的计算量可能会比 attention 层的计算量大很多。推荐阅读：\u003ca href=\"https://spaces.ac.cn/archives/8610\"\u003e线性Transformer应该不是你要等的那个模型\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003eRWKV\u003c/h2\u003e\n\u003ch3\u003eRWKV 和 RNN 的关系\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eReinventing RNNs for the Transformer Era\u003c/em\u003e，RWKV 的标题非常霸气，RWKV 真的是一个传统 RNN 模型吗？\u003c/p\u003e\n\u003cp\u003eRWKV 的新颖之处在于它的 “attention”（RWKV 中叫做 WKV） 可以写成 RNN 的形式\u003c/p\u003e\n\u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ew\u003c/mi\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmsub\u003e\u003cmi\u003ev\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmfrac\u003e\u003cmrow\u003e\u003cmunderover\u003e\u003cmo\u003e∑\u003c/mo\u003e\u003cmrow\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cmrow\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/munderover\u003e\u003cmsup\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmrow\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmi\u003ew\u003c/mi\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003cmo\u003e⊙\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ev\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003eu\u003c/mi\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003cmo\u003e⊙\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ev\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cmrow\u003e\u003cmunderover\u003e\u003cmo\u003e∑\u003c/mo\u003e\u003cmrow\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cmrow\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/munderover\u003e\u003cmsup\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmrow\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmi\u003ew\u003c/mi\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003eu\u003c/mi\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003c/mfrac\u003e\u003cmi mathvariant=\"normal\"\u003e.\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ewk v_t = \\frac{\\sum_{i=1}^{t-1} e^{-(t-1-i)w+k_i} \\odot v_i + e^{u+k_t} \\odot v_t}{\\sum_{i=1}^{t-1} e^{-(t-1-i)w+k_i} + e^{u+k_t}}.\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\n\u003cp\u003e令\u003c/p\u003e\n\u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eα\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmunderover\u003e\u003cmo\u003e∑\u003c/mo\u003e\u003cmrow\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/munderover\u003e\u003cmsup\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmrow\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmi\u003ew\u003c/mi\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003cmo\u003e⊙\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ev\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\alpha_{t} = \\sum_{i=1}^{t} e^{-(t-i)w+k_i} \\odot v_i \u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\n\u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eβ\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmunderover\u003e\u003cmo\u003e∑\u003c/mo\u003e\u003cmrow\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/munderover\u003e\u003cmsup\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmrow\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmi\u003ew\u003c/mi\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\beta_{t} = \\sum_{i=1}^{t} e^{-(t-i)w+k_i}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\n\u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ew\u003c/mi\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmsub\u003e\u003cmi\u003ev\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmfrac\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eα\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003eu\u003c/mi\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003cmo\u003e⊙\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ev\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eβ\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003eu\u003c/mi\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003c/mfrac\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ewk v_t = \\frac{\\alpha_{t-1} + e^{u+k_t} \\odot v_t}{\\beta_{t-1} + e^{u+k_t}}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\n\u003cp\u003e写成 RNN 形式的好处是计算 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ew\u003c/mi\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmsub\u003e\u003cmi\u003ev\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ewk v_t\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e 的时候可以利用之前的状态 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eα\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\alpha_{t-1}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e 和 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003eβ\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\beta_{t-1}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e，这样计算 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ew\u003c/mi\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmsub\u003e\u003cmi\u003ev\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ewk v_t\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e 的时间复杂度就是 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(d)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e，空间复杂度也是 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eO\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eO(d)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e。\u003c/p\u003e\n\u003cp\u003e推荐阅读 \u003ca href=\"https://zhuanlan.zhihu.com/p/614311961\"\u003eRWKV的RNN CNN二象性\u003c/a\u003e 中关于 RWKV 的部分，作者清晰的解释了 RWKV 和 AFT 的关系，以及如何直观理解 wkv。\u003c/p\u003e\n\u003ch3\u003eToken Shift\u003c/h3\u003e\n\u003cp\u003e传统的 Transformer 在 self-attention 之前会对输入 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ex\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ex\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e 做 linear projections 得到 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eQ\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eQ\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e，\u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eK\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eK\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e，\u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eV\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eV\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e。RWKV 的不同之处在于并不是直接对 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ex\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ex\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e 做 linear projection，而是对 current inputs 和 previous inputs 做一个线性插值后再做 linear projection。这个线性插值的过程就是 token shift。即：\u003c/p\u003e\n\u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003er\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eW\u003c/mi\u003e\u003cmi\u003er\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e⋅\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eμ\u003c/mi\u003e\u003cmi\u003er\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e⊙\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eμ\u003c/mi\u003e\u003cmi\u003er\u003c/mi\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo\u003e⊙\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003er_t = W_{r} \\cdot (\\mu_r \\odot x_t + (1 - \\mu_r) \\odot x_{t-1})\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\n\u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eW\u003c/mi\u003e\u003cmi\u003ek\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e⋅\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eμ\u003c/mi\u003e\u003cmi\u003ek\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e⊙\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eμ\u003c/mi\u003e\u003cmi\u003ek\u003c/mi\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo\u003e⊙\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ek_t = W_{k} \\cdot (\\mu_k \\odot x_t + (1 - \\mu_k) \\odot x_{t-1})\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\n\u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ev\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eW\u003c/mi\u003e\u003cmi\u003ev\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e⋅\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eμ\u003c/mi\u003e\u003cmi\u003ev\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e⊙\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003eμ\u003c/mi\u003e\u003cmi\u003ev\u003c/mi\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo\u003e⊙\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ev_t = W_{v} \\cdot (\\mu_v \\odot x_t + (1 - \\mu_v) \\odot x_{t-1})\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\n\u003cp\u003e感觉 token shift 很像 kernel size 为 2 的卷积\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/token_shift.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e作者几年前就提出了 token shift 的想法，参见 \u003ca href=\"https://zhuanlan.zhihu.com/p/399480671\"\u003eTime-shift: 一行代码，免费提高 Transformer 性能（无参数，无耗时）\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e局限性\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e超长上下文任务上效果受限：RWKV 这种递归架构限制了它回顾之前 token 的能力，不像 self-attention 可以保留所有 token 的信息。\u003c/li\u003e\n\u003cli\u003e对 prompt 比较敏感：用苏剑林的话说，RWKV 只会做闭卷考试，不会做开卷考试（不会往前翻书），prompt 中在一开始描述任务比较好，带着问题去阅读后续内容。Prompt \"For the document below do X\" 好于 \"For the document above do X\"。参考\u003ca href=\"https://www.zhihu.com/question/602564718/answer/3062973388\"\u003e苏剑林的回答\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","title":"RWKV","date":"2024-01-06"}},"__N_SSG":true},"page":"/posts/[id]","query":{"id":"rwkv"},"buildId":"LgI_2aaq_4GSrY0V2y1Bt","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>