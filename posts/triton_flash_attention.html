<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/github.min.css"/><title>Triton Puzzles: 从 softmax 到 flash attention</title><meta name="next-head-count" content="4"/><link rel="preload" href="/_next/static/css/ab65974685f462b8.css" as="style"/><link rel="stylesheet" href="/_next/static/css/ab65974685f462b8.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-6cbe6e332df95288.js" defer=""></script><script src="/_next/static/chunks/main-26f9f36b33181737.js" defer=""></script><script src="/_next/static/chunks/pages/_app-2d6bf7b3192a8752.js" defer=""></script><script src="/_next/static/chunks/73-96e6cbd54826b874.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bid%5D-4b57e02c440700a5.js" defer=""></script><script src="/_next/static/639Ox_e6KcvOQ7l5p__VL/_buildManifest.js" defer=""></script><script src="/_next/static/639Ox_e6KcvOQ7l5p__VL/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="max-w-4xl px-4 mt-12 mb-24 mx-auto"><main><article><h1 class="text-3xl font-medium my-4 border-b-0">Triton Puzzles: 从 softmax 到 flash attention</h1><div class="text-gray-500 mb-8 pb-2 border-b-2 border-solid border-slate-300"><time dateTime="2024-03-29">March 29, 2024</time></div><div><p>受 <a href="https://github.com/srush/Triton-Puzzles">Triton Puzzles</a> 启发，把 flash attention 的实现过程拆分成一系列的 puzzle，逐步用 triton 去实现，以一个更平滑的学习曲线来学习 triton 和 flash attention。但是</p>
<ul>
<li>本文不会介绍 flash attention 的原理，阅读本文最好是对 flash attention 有一定了解，但不是必须的。</li>
<li>本文不会介绍 triton 的一些基本用法，阅读本文至少得看懂 triton 官方的第一个 tutorial：<a href="https://triton-lang.org/main/getting-started/tutorials/01-vector-add.html">Vector Addition</a></li>
</ul>
<p>Attention 的计算公式如下</p>
<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Attention</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>softmax</mtext><mo stretchy="false">(</mo><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><mo stretchy="false">)</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">\text{Attention}(Q, K, V) = \text{softmax}({QK^T})V</annotation></semantics></math></span>
<p>flash attention 的核心思路是通过分块计算，将中间计算 fuse 在一起，避免来回读写中间结果（<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">QK^T</annotation></semantics></math></span>, softmax)，减少访问 HBM 的次数，提高计算效率。</p>
<p><img src="/image-6.png" alt="alt text"></p>
<p>我们的最终目标是用 triton 实现 flash attention v2 的 forward，即下面这个伪代码</p>
<h3>Puzzle 1：softmax</h3>
<p>对一个 2D 矩阵，计算 softmax</p>
<p>我们先用 pytorch 实现</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax</span>(<span class="hljs-params">x</span>):
    x_max = x.<span class="hljs-built_in">max</span>(dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]
    x = x - x_max
    x_exp = x.exp()
    x_exp_sum = x_exp.<span class="hljs-built_in">sum</span>(dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)
    <span class="hljs-keyword">return</span> x_exp / x_exp_sum
</code></pre>
<p><img src="/image-3.png" alt="alt text"></p>
<p>每个 program instance 计算一行的 softmax</p>
<pre><code class="hljs language-python"><span class="hljs-meta">@triton.jit</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax_kernel</span>(<span class="hljs-params">x_ptr, output_ptr, row_stride, n_cols, BLOCK_SIZE: tl.constexpr</span>):
    pid = tl.program_id(<span class="hljs-number">0</span>)

    <span class="hljs-comment"># BLOCK_SIZE is bigger than the number of columns</span>
    col_range = tl.arange(<span class="hljs-number">0</span>, BLOCK_SIZE)
    col_mask = col_range &#x3C; n_cols
    x = tl.load(x_ptr + pid * row_stride + col_range, mask=col_mask)
    x_max = tl.<span class="hljs-built_in">max</span>(x, axis=-<span class="hljs-number">1</span>)
    x = x - x_max
    x_exp = tl.exp(x)
    x_exp_sum = tl.<span class="hljs-built_in">sum</span>(x_exp, axis=-<span class="hljs-number">1</span>)
    tl.store(output_ptr + pid * row_stride + col_range, x_exp / x_exp_sum, mask=col_mask)

<span class="hljs-keyword">def</span> <span class="hljs-title function_">triton_softmax</span>(<span class="hljs-params">x</span>):
    n_rows, n_cols = x.shape
    BLOCK_SIZE = triton.next_power_of_2(n_cols)
    output = torch.empty_like(x)
    softmax_kernel[(n_rows,)](
        x,
        output,
        x.stride(<span class="hljs-number">0</span>),
        n_cols,
        BLOCK_SIZE,
    )
    <span class="hljs-keyword">return</span> output
</code></pre>
<p>这个实现非常简单，可以是看做对 pytorch 代码的一对一翻译</p>
<p><img src="/torch_to_triton.jpg" alt=""></p>
<h3>Puzzle 2: 分块算 softmax</h3>
<p>上面我们一次 load 了一整行的数据，如果我们一次只 load 一行的一部分数据呢？</p>
<p><img src="/image-4.png" alt="alt text"></p>
<p>是不是简单加个 for loop 就行了？</p>
<pre><code class="hljs language-python"><span class="hljs-meta">@triton.jit</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax_kernel</span>(<span class="hljs-params">x_ptr, output_ptr, row_stride, n_cols, BLOCK_SIZE: tl.constexpr</span>):
    pid = tl.program_id(<span class="hljs-number">0</span>)

    <span class="hljs-keyword">for</span> offset <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, n_cols, BLOCK_SIZE):
        col_range = tl.arange(<span class="hljs-number">0</span>, BLOCK_SIZE)
        col_mask = col_range + offset &#x3C; n_cols
        x = tl.load(x_ptr + pid * row_stride + col_range + offset, mask=col_mask)
        x_max = tl.<span class="hljs-built_in">max</span>(x, axis=-<span class="hljs-number">1</span>)
        x = x - x_max
        x_exp = tl.exp(x)
        x_exp_sum = tl.<span class="hljs-built_in">sum</span>(x_exp, axis=-<span class="hljs-number">1</span>)
        tl.store(output_ptr + pid * row_stride + col_range + offset, x_exp / x_exp_sum, mask=col_mask)
</code></pre>
<p>稍加思考，我们就会发现这个实现是有问题的，因为我们计算 x_max 和 x_exp_sum 的时候只考虑了当前的 block，而期望得到的是整行的 x_max 和 x_exp_sum。</p>
<p>我们再用两个 for loop，一个计算 x_max，一个计算 x_exp_sum，然后再计算 softmax</p>
<pre><code class="hljs language-python"><span class="hljs-meta">@triton.jit</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax_kernel_v2</span>(<span class="hljs-params">x_ptr, output_ptr, row_stride, n_cols, BLOCK_SIZE: tl.constexpr</span>):
    pid = tl.program_id(<span class="hljs-number">0</span>)

    x_max = -<span class="hljs-built_in">float</span>(<span class="hljs-string">'inf'</span>)
    <span class="hljs-keyword">for</span> offset <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, n_cols, BLOCK_SIZE):
        col_range = tl.arange(<span class="hljs-number">0</span>, BLOCK_SIZE)
        col_mask = col_range + offset &#x3C; n_cols
        x = tl.load(x_ptr + pid * row_stride + col_range + offset, mask=col_mask, other=-<span class="hljs-built_in">float</span>(<span class="hljs-string">'inf'</span>))
        x_max = tl.maximum(x_max, tl.<span class="hljs-built_in">max</span>(x, axis=-<span class="hljs-number">1</span>))
    
    x_exp_sum = <span class="hljs-number">0.0</span>
    <span class="hljs-keyword">for</span> offset <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, n_cols, BLOCK_SIZE):
        col_range = tl.arange(<span class="hljs-number">0</span>, BLOCK_SIZE)
        col_mask = col_range + offset &#x3C; n_cols
        x = tl.load(x_ptr + pid * row_stride + col_range + offset, mask=col_mask, other=-<span class="hljs-built_in">float</span>(<span class="hljs-string">'inf'</span>))
        x_exp_sum = x_exp_sum + tl.<span class="hljs-built_in">sum</span>(tl.exp(x - x_max), axis=-<span class="hljs-number">1</span>)
    
    <span class="hljs-keyword">for</span> offset <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, n_cols, BLOCK_SIZE):
        col_range = tl.arange(<span class="hljs-number">0</span>, BLOCK_SIZE)
        col_mask = col_range + offset &#x3C; n_cols
        x = tl.load(x_ptr + pid * row_stride + col_range + offset, mask=col_mask)
        x_exp = tl.exp(x - x_max)
        tl.store(output_ptr + pid * row_stride + col_range + offset, x_exp / x_exp_sum, mask=col_mask)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">triton_softmax_v2</span>(<span class="hljs-params">x</span>):
    n_rows, n_cols = x.shape
    output = torch.empty_like(x)
    BLOCK_SIZE = <span class="hljs-number">256</span>
    softmax_kernel_v2[(n_rows,)](
        x,
        output,
        x.stride(<span class="hljs-number">0</span>),
        n_cols,
        BLOCK_SIZE
    )
    <span class="hljs-keyword">return</span> output
</code></pre>
<h3>Puzzle 3: online softmax</h3>
<p>一个更聪明的做法是，我们可以在一次 for loop 中计算 x_max 和 x_exp_sum，这样我们就可以减少一次 for loop。参考 paper <a href="https://arxiv.org/pdf/1805.02867.pdf">Online normalizer calculation for softmax</a></p>
<p><img src="/image-1.png" alt="alt text"></p>
<p>我们可以使用归纳法证明这个算法的正确性。</p>
<p>当 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">V = 1</annotation></semantics></math></span> 时（即向量只有一个元素），<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mn>1</mn></msub><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">m_1 = x_1</annotation></semantics></math></span> 是输入向量中的最大值，<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mn>1</mn></msub><mo>=</mo><msup><mi>e</mi><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>−</mo><msub><mi>m</mi><mn>1</mn></msub></mrow></msup><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">d_1 = e^{x_1 - m_1} = 1</annotation></semantics></math></span>，符合softmax函数的定义。</p>
<p>假设对于 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo>=</mo><mi>S</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">V = S - 1</annotation></semantics></math></span>，上述方法正确地计算了 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mrow><mi>S</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><msubsup><mrow><mi>max</mi><mo>⁡</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>S</mi><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mi>x</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">m_{S-1} = \max_{k=1}^{S-1} x_k</annotation></semantics></math></span> 和 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mrow><mi>S</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>S</mi><mo>−</mo><mn>1</mn></mrow></msubsup><msup><mi>e</mi><mrow><msub><mi>x</mi><mi>j</mi></msub><mo>−</mo><msub><mi>m</mi><mrow><mi>S</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></msup></mrow><annotation encoding="application/x-tex">d_{S-1} = \sum_{j=1}^{S-1} e^{x_j - m_{S-1}}</annotation></semantics></math></span>。</p>
<p>然后对于 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo>=</mo><mi>S</mi></mrow><annotation encoding="application/x-tex">V = S</annotation></semantics></math></span>，我们需要证明算法也能准确计算 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mi>S</mi></msub></mrow><annotation encoding="application/x-tex">m_S</annotation></semantics></math></span> 和 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>S</mi></msub></mrow><annotation encoding="application/x-tex">d_S</annotation></semantics></math></span>。</p>
<p><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mi>S</mi></msub><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>m</mi><mrow><mi>S</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi>x</mi><mi>S</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msubsup><mrow><mi>max</mi><mo>⁡</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></msubsup><msub><mi>x</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">m_S = \max(m_{S-1}, x_S) = \max_{k=1}^{S} x_k</annotation></semantics></math></span></p>
<p>这表明更新后的 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mi>S</mi></msub></mrow><annotation encoding="application/x-tex">m_S</annotation></semantics></math></span> 正确地表示了前 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span> 个输入元素的最大值。</p>
<p><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>S</mi></msub><mo>=</mo><msub><mi>d</mi><mrow><mi>S</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>×</mo><msup><mi>e</mi><mrow><msub><mi>m</mi><mrow><mi>S</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>−</mo><msub><mi>m</mi><mi>S</mi></msub></mrow></msup><mo>+</mo><msup><mi>e</mi><mrow><msub><mi>x</mi><mi>S</mi></msub><mo>−</mo><msub><mi>m</mi><mi>S</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">d_S = d_{S-1} \times e^{m_{S-1} - m_S} + e^{x_S - m_S}</annotation></semantics></math></span>
根据归纳假设，可以将 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mrow><mi>S</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">d_{S-1}</annotation></semantics></math></span> 展开为 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>S</mi><mo>−</mo><mn>1</mn></mrow></msubsup><msup><mi>e</mi><mrow><msub><mi>x</mi><mi>j</mi></msub><mo>−</mo><msub><mi>m</mi><mrow><mi>S</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\sum_{j=1}^{S-1} e^{x_j - m_{S-1}}</annotation></semantics></math></span>，通过替换和简化，我们可以得到：
<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>S</mi></msub><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></msubsup><msup><mi>e</mi><mrow><msub><mi>x</mi><mi>j</mi></msub><mo>−</mo><msub><mi>m</mi><mi>S</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">d_S = \sum_{j=1}^{S} e^{x_j - m_S}</annotation></semantics></math></span></p>
<p>这表明更新后的 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>S</mi></msub></mrow><annotation encoding="application/x-tex">d_S</annotation></semantics></math></span> 正确地计算了前 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span> 个元素，相对于新的最大值 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mi>S</mi></msub></mrow><annotation encoding="application/x-tex">m_S</annotation></semantics></math></span> 的归一化因子。</p>
<p>对应的 triton 代码如下：</p>
<pre><code class="hljs language-python"><span class="hljs-meta">@triton.jit</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax_kernel_v3</span>(<span class="hljs-params">x_ptr, output_ptr, row_stride, n_cols, BLOCK_SIZE: tl.constexpr</span>):
    pid = tl.program_id(<span class="hljs-number">0</span>)

    x_max = -<span class="hljs-built_in">float</span>(<span class="hljs-string">'inf'</span>)
    x_exp_sum = <span class="hljs-number">0.0</span>
    <span class="hljs-keyword">for</span> offset <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, n_cols, BLOCK_SIZE):
        col_range = tl.arange(<span class="hljs-number">0</span>, BLOCK_SIZE)
        col_mask = col_range + offset &#x3C; n_cols
        x = tl.load(x_ptr + pid * row_stride + col_range + offset, mask=col_mask, other=-<span class="hljs-built_in">float</span>(<span class="hljs-string">'inf'</span>))
        x_max_new = tl.maximum(x_max, tl.<span class="hljs-built_in">max</span>(x, axis=-<span class="hljs-number">1</span>))
        x_exp_sum = x_exp_sum * tl.exp(x_max - x_max_new) + tl.<span class="hljs-built_in">sum</span>(tl.exp(x - x_max_new), axis=-<span class="hljs-number">1</span>)
        x_max = x_max_new
    
    <span class="hljs-keyword">for</span> offset <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, n_cols, BLOCK_SIZE):
        col_range = tl.arange(<span class="hljs-number">0</span>, BLOCK_SIZE)
        col_mask = col_range + offset &#x3C; n_cols
        x = tl.load(x_ptr + pid * row_stride + col_range + offset, mask=col_mask)
        x_exp = tl.exp(x - x_max)
        tl.store(output_ptr + pid * row_stride + col_range + offset, x_exp / x_exp_sum, mask=col_mask)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">triton_softmax_v3</span>(<span class="hljs-params">x</span>):
    n_rows, n_cols = x.shape
    output = torch.empty_like(x)
    BLOCK_SIZE = <span class="hljs-number">256</span>
    softmax_kernel_v3[(n_rows,)](
        x,
        output,
        x.stride(<span class="hljs-number">0</span>),
        n_cols,
        BLOCK_SIZE
    )
    <span class="hljs-keyword">return</span> output
</code></pre>
<h3>Puzzle 4: 更通用的分块策略</h3>
<p>前面我们都是按行分块，更通用的分块策略是按行和列都分块</p>
<p><img src="/image-5.png" alt="alt text"></p>
<pre><code class="hljs language-python"><span class="hljs-meta">@triton.jit</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax_kernel_v4</span>(<span class="hljs-params">x_ptr, output_ptr, row_stride, n_rows, n_cols, BLOCK_ROW: tl.constexpr, BLOCK_COL: tl.constexpr</span>):
    pid = tl.program_id(<span class="hljs-number">0</span>)

    x_max = tl.full((BLOCK_ROW, ), -<span class="hljs-built_in">float</span>(<span class="hljs-string">'inf'</span>), dtype=tl.float32)
    x_exp_sum = tl.full((BLOCK_ROW, ), <span class="hljs-number">0.0</span>, dtype=tl.float32)
    <span class="hljs-keyword">for</span> offset <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, n_cols, BLOCK_COL):
        row_range = tl.arange(<span class="hljs-number">0</span>, BLOCK_ROW) + pid * BLOCK_ROW
        col_range = tl.arange(<span class="hljs-number">0</span>, BLOCK_COL) + offset
        x_mask = (row_range[:, <span class="hljs-literal">None</span>] &#x3C; n_rows) &#x26; (col_range &#x3C; n_cols)
        x_range = row_range[:, <span class="hljs-literal">None</span>] * row_stride + col_range
        x = tl.load(x_ptr + x_range, mask=x_mask, other=-<span class="hljs-built_in">float</span>(<span class="hljs-string">'inf'</span>))
        x_max_new = tl.maximum(x_max, tl.<span class="hljs-built_in">max</span>(x, axis=-<span class="hljs-number">1</span>))
        x_exp_sum = tl.exp(x_max - x_max_new) * x_exp_sum + tl.<span class="hljs-built_in">sum</span>(tl.exp(x - x_max_new[:, <span class="hljs-literal">None</span>]), axis=-<span class="hljs-number">1</span>)
        x_max = x_max_new
    
    <span class="hljs-keyword">for</span> offset <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, n_cols, BLOCK_COL):
        row_range = tl.arange(<span class="hljs-number">0</span>, BLOCK_ROW) + pid * BLOCK_ROW
        col_range = tl.arange(<span class="hljs-number">0</span>, BLOCK_COL) + offset
        x_mask = (row_range[:, <span class="hljs-literal">None</span>] &#x3C; n_rows) &#x26; (col_range &#x3C; n_cols)
        x_range = row_range[:, <span class="hljs-literal">None</span>] * row_stride + col_range
        x = tl.load(x_ptr + x_range, mask=x_mask)
        x_exp = tl.exp(x - x_max[:, <span class="hljs-literal">None</span>])
        tl.store(output_ptr + x_range, x_exp / x_exp_sum[:, <span class="hljs-literal">None</span>], mask=x_mask)



<span class="hljs-keyword">def</span> <span class="hljs-title function_">triton_softmax_v4</span>(<span class="hljs-params">x</span>):
    n_rows, n_cols = x.shape
    output = torch.empty_like(x)
    grid = <span class="hljs-keyword">lambda</span> meta: (triton.cdiv(n_rows, meta[<span class="hljs-string">'BLOCK_ROW'</span>]), )
    softmax_kernel_v4[grid](
        x,
        output,
        x.stride(<span class="hljs-number">0</span>),
        n_rows,
        n_cols,
        <span class="hljs-number">32</span>,
        <span class="hljs-number">256</span>
    )
    <span class="hljs-keyword">return</span> output
</code></pre>
<h3>flash attention 的实现</h3>
<p>最后，我们可以按照 flash attention 的伪代码实现 forward</p>
<p><img src="/flash_attention_forward_pseudocode.png" alt=""></p>
<p><img src="/image.png" alt="alt text"></p>
<pre><code class="hljs language-python"><span class="hljs-meta">@triton.jit</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">flash_attention_kernel</span>(<span class="hljs-params">q_ptr, k_ptr, v_ptr, output_ptr, n, d: tl.constexpr, BR: tl.constexpr, BC: tl.constexpr</span>):
    pid = tl.program_id(<span class="hljs-number">0</span>)

    q_row_range = tl.arange(<span class="hljs-number">0</span>, BR) + pid * BR
    q_range = q_row_range[:, <span class="hljs-literal">None</span>] * d + tl.arange(<span class="hljs-number">0</span>, d)
    q_mask = (q_row_range[:, <span class="hljs-literal">None</span>] &#x3C; n) &#x26; (tl.arange(<span class="hljs-number">0</span>, d) &#x3C; d)
    q = tl.load(q_ptr + q_range, mask=q_mask)

    o = tl.full((BR, d), <span class="hljs-number">0</span>, dtype=tl.float32)
    l = tl.full((BR,), <span class="hljs-number">0</span>, dtype=tl.float32)
    m = tl.full((BR,), -<span class="hljs-built_in">float</span>(<span class="hljs-string">'inf'</span>), dtype=tl.float32)

    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, n, BC):
        kv_row_range = tl.arange(<span class="hljs-number">0</span>, BC) + j
        kv_range = kv_row_range[:, <span class="hljs-literal">None</span>] * d + tl.arange(<span class="hljs-number">0</span>, d)
        kv_mask = (kv_row_range[:, <span class="hljs-literal">None</span>] &#x3C; n) &#x26; (tl.arange(<span class="hljs-number">0</span>, d) &#x3C; d)
        k = tl.load(k_ptr + kv_range, mask=kv_mask, other=<span class="hljs-number">0</span>)
        v = tl.load(v_ptr + kv_range, mask=kv_mask, other=<span class="hljs-number">0</span>)
        s = tl.dot(q, tl.trans(k))
        s_mask = (q_row_range[:, <span class="hljs-literal">None</span>] &#x3C; n) &#x26; (kv_row_range &#x3C; n)
        s = tl.where(s_mask, s, -<span class="hljs-built_in">float</span>(<span class="hljs-string">'inf'</span>))
        m_new = tl.maximum(m, tl.<span class="hljs-built_in">max</span>(s, axis=-<span class="hljs-number">1</span>))
        p = tl.exp(s - m_new[:, <span class="hljs-literal">None</span>])
        l = tl.exp(m - m_new) * l + tl.<span class="hljs-built_in">sum</span>(p, axis=-<span class="hljs-number">1</span>)
        o = tl.exp(m - m_new)[:, <span class="hljs-literal">None</span>] * o + tl.dot(p, v)
        m = m_new

    tl.store(output_ptr + q_range, o / l[:, <span class="hljs-literal">None</span>], mask=q_mask)



<span class="hljs-keyword">def</span> <span class="hljs-title function_">flash_attention</span>(<span class="hljs-params">q, k, v</span>):
    n, d = q.shape
    BR = <span class="hljs-number">32</span>
    BC = <span class="hljs-number">64</span>
    output = torch.empty((n, d), device=q.device)
    grid = <span class="hljs-keyword">lambda</span> meta: (triton.cdiv(n, BR),)
    flash_attention_kernel[grid](
        q,
        k,
        v,
        output,
        n,
        d,
        BR,
        BC,
    )
    <span class="hljs-keyword">return</span> output
</code></pre></div></article></main><div class="mt-12"><a href="/">← 返回首页</a></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"postData":{"id":"triton_flash_attention","contentHtml":"\u003cp\u003e受 \u003ca href=\"https://github.com/srush/Triton-Puzzles\"\u003eTriton Puzzles\u003c/a\u003e 启发，把 flash attention 的实现过程拆分成一系列的 puzzle，逐步用 triton 去实现，以一个更平滑的学习曲线来学习 triton 和 flash attention。但是\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e本文不会介绍 flash attention 的原理，阅读本文最好是对 flash attention 有一定了解，但不是必须的。\u003c/li\u003e\n\u003cli\u003e本文不会介绍 triton 的一些基本用法，阅读本文至少得看懂 triton 官方的第一个 tutorial：\u003ca href=\"https://triton-lang.org/main/getting-started/tutorials/01-vector-add.html\"\u003eVector Addition\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAttention 的计算公式如下\u003c/p\u003e\n\u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmtext\u003eAttention\u003c/mtext\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003eQ\u003c/mi\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmi\u003eK\u003c/mi\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmi\u003eV\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmtext\u003esoftmax\u003c/mtext\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmrow\u003e\u003cmi\u003eQ\u003c/mi\u003e\u003cmsup\u003e\u003cmi\u003eK\u003c/mi\u003e\u003cmi\u003eT\u003c/mi\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmi\u003eV\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\text{Attention}(Q, K, V) = \\text{softmax}({QK^T})V\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\n\u003cp\u003eflash attention 的核心思路是通过分块计算，将中间计算 fuse 在一起，避免来回读写中间结果（\u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eQ\u003c/mi\u003e\u003cmsup\u003e\u003cmi\u003eK\u003c/mi\u003e\u003cmi\u003eT\u003c/mi\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eQK^T\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e, softmax)，减少访问 HBM 的次数，提高计算效率。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/image-6.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cp\u003e我们的最终目标是用 triton 实现 flash attention v2 的 forward，即下面这个伪代码\u003c/p\u003e\n\u003ch3\u003ePuzzle 1：softmax\u003c/h3\u003e\n\u003cp\u003e对一个 2D 矩阵，计算 softmax\u003c/p\u003e\n\u003cp\u003e我们先用 pytorch 实现\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003esoftmax\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003ex\u003c/span\u003e):\n    x_max = x.\u003cspan class=\"hljs-built_in\"\u003emax\u003c/span\u003e(dim=-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, keepdim=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]\n    x = x - x_max\n    x_exp = x.exp()\n    x_exp_sum = x_exp.\u003cspan class=\"hljs-built_in\"\u003esum\u003c/span\u003e(dim=-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, keepdim=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e x_exp / x_exp_sum\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/image-3.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cp\u003e每个 program instance 计算一行的 softmax\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-meta\"\u003e@triton.jit\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003esoftmax_kernel\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003ex_ptr, output_ptr, row_stride, n_cols, BLOCK_SIZE: tl.constexpr\u003c/span\u003e):\n    pid = tl.program_id(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)\n\n    \u003cspan class=\"hljs-comment\"\u003e# BLOCK_SIZE is bigger than the number of columns\u003c/span\u003e\n    col_range = tl.arange(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, BLOCK_SIZE)\n    col_mask = col_range \u0026#x3C; n_cols\n    x = tl.load(x_ptr + pid * row_stride + col_range, mask=col_mask)\n    x_max = tl.\u003cspan class=\"hljs-built_in\"\u003emax\u003c/span\u003e(x, axis=-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\n    x = x - x_max\n    x_exp = tl.exp(x)\n    x_exp_sum = tl.\u003cspan class=\"hljs-built_in\"\u003esum\u003c/span\u003e(x_exp, axis=-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\n    tl.store(output_ptr + pid * row_stride + col_range, x_exp / x_exp_sum, mask=col_mask)\n\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003etriton_softmax\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003ex\u003c/span\u003e):\n    n_rows, n_cols = x.shape\n    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n    output = torch.empty_like(x)\n    softmax_kernel[(n_rows,)](\n        x,\n        output,\n        x.stride(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e),\n        n_cols,\n        BLOCK_SIZE,\n    )\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e output\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e这个实现非常简单，可以是看做对 pytorch 代码的一对一翻译\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/torch_to_triton.jpg\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch3\u003ePuzzle 2: 分块算 softmax\u003c/h3\u003e\n\u003cp\u003e上面我们一次 load 了一整行的数据，如果我们一次只 load 一行的一部分数据呢？\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/image-4.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cp\u003e是不是简单加个 for loop 就行了？\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-meta\"\u003e@triton.jit\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003esoftmax_kernel\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003ex_ptr, output_ptr, row_stride, n_cols, BLOCK_SIZE: tl.constexpr\u003c/span\u003e):\n    pid = tl.program_id(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)\n\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e offset \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, n_cols, BLOCK_SIZE):\n        col_range = tl.arange(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, BLOCK_SIZE)\n        col_mask = col_range + offset \u0026#x3C; n_cols\n        x = tl.load(x_ptr + pid * row_stride + col_range + offset, mask=col_mask)\n        x_max = tl.\u003cspan class=\"hljs-built_in\"\u003emax\u003c/span\u003e(x, axis=-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\n        x = x - x_max\n        x_exp = tl.exp(x)\n        x_exp_sum = tl.\u003cspan class=\"hljs-built_in\"\u003esum\u003c/span\u003e(x_exp, axis=-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\n        tl.store(output_ptr + pid * row_stride + col_range + offset, x_exp / x_exp_sum, mask=col_mask)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e稍加思考，我们就会发现这个实现是有问题的，因为我们计算 x_max 和 x_exp_sum 的时候只考虑了当前的 block，而期望得到的是整行的 x_max 和 x_exp_sum。\u003c/p\u003e\n\u003cp\u003e我们再用两个 for loop，一个计算 x_max，一个计算 x_exp_sum，然后再计算 softmax\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-meta\"\u003e@triton.jit\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003esoftmax_kernel_v2\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003ex_ptr, output_ptr, row_stride, n_cols, BLOCK_SIZE: tl.constexpr\u003c/span\u003e):\n    pid = tl.program_id(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)\n\n    x_max = -\u003cspan class=\"hljs-built_in\"\u003efloat\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'inf'\u003c/span\u003e)\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e offset \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, n_cols, BLOCK_SIZE):\n        col_range = tl.arange(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, BLOCK_SIZE)\n        col_mask = col_range + offset \u0026#x3C; n_cols\n        x = tl.load(x_ptr + pid * row_stride + col_range + offset, mask=col_mask, other=-\u003cspan class=\"hljs-built_in\"\u003efloat\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'inf'\u003c/span\u003e))\n        x_max = tl.maximum(x_max, tl.\u003cspan class=\"hljs-built_in\"\u003emax\u003c/span\u003e(x, axis=-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e))\n    \n    x_exp_sum = \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e offset \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, n_cols, BLOCK_SIZE):\n        col_range = tl.arange(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, BLOCK_SIZE)\n        col_mask = col_range + offset \u0026#x3C; n_cols\n        x = tl.load(x_ptr + pid * row_stride + col_range + offset, mask=col_mask, other=-\u003cspan class=\"hljs-built_in\"\u003efloat\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'inf'\u003c/span\u003e))\n        x_exp_sum = x_exp_sum + tl.\u003cspan class=\"hljs-built_in\"\u003esum\u003c/span\u003e(tl.exp(x - x_max), axis=-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\n    \n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e offset \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, n_cols, BLOCK_SIZE):\n        col_range = tl.arange(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, BLOCK_SIZE)\n        col_mask = col_range + offset \u0026#x3C; n_cols\n        x = tl.load(x_ptr + pid * row_stride + col_range + offset, mask=col_mask)\n        x_exp = tl.exp(x - x_max)\n        tl.store(output_ptr + pid * row_stride + col_range + offset, x_exp / x_exp_sum, mask=col_mask)\n\n\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003etriton_softmax_v2\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003ex\u003c/span\u003e):\n    n_rows, n_cols = x.shape\n    output = torch.empty_like(x)\n    BLOCK_SIZE = \u003cspan class=\"hljs-number\"\u003e256\u003c/span\u003e\n    softmax_kernel_v2[(n_rows,)](\n        x,\n        output,\n        x.stride(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e),\n        n_cols,\n        BLOCK_SIZE\n    )\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e output\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003ePuzzle 3: online softmax\u003c/h3\u003e\n\u003cp\u003e一个更聪明的做法是，我们可以在一次 for loop 中计算 x_max 和 x_exp_sum，这样我们就可以减少一次 for loop。参考 paper \u003ca href=\"https://arxiv.org/pdf/1805.02867.pdf\"\u003eOnline normalizer calculation for softmax\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/image-1.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cp\u003e我们可以使用归纳法证明这个算法的正确性。\u003c/p\u003e\n\u003cp\u003e当 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eV\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eV = 1\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e 时（即向量只有一个元素），\u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003em\u003c/mi\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003em_1 = x_1\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e 是输入向量中的最大值，\u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/msub\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003em\u003c/mi\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ed_1 = e^{x_1 - m_1} = 1\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e，符合softmax函数的定义。\u003c/p\u003e\n\u003cp\u003e假设对于 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eV\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi\u003eS\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eV = S - 1\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e，上述方法正确地计算了 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003em\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003eS\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmsubsup\u003e\u003cmrow\u003e\u003cmi\u003emax\u003c/mi\u003e\u003cmo\u003e⁡\u003c/mo\u003e\u003c/mrow\u003e\u003cmrow\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cmrow\u003e\u003cmi\u003eS\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msubsup\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ek\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003em_{S-1} = \\max_{k=1}^{S-1} x_k\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e 和 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003eS\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmsubsup\u003e\u003cmo\u003e∑\u003c/mo\u003e\u003cmrow\u003e\u003cmi\u003ej\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cmrow\u003e\u003cmi\u003eS\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msubsup\u003e\u003cmsup\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ej\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003em\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003eS\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ed_{S-1} = \\sum_{j=1}^{S-1} e^{x_j - m_{S-1}}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e。\u003c/p\u003e\n\u003cp\u003e然后对于 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eV\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi\u003eS\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eV = S\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e，我们需要证明算法也能准确计算 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003em\u003c/mi\u003e\u003cmi\u003eS\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003em_S\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e 和 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmi\u003eS\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ed_S\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e。\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003em\u003c/mi\u003e\u003cmi\u003eS\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi\u003emax\u003c/mi\u003e\u003cmo\u003e⁡\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003em\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003eS\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo separator=\"true\"\u003e,\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003eS\u003c/mi\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmsubsup\u003e\u003cmrow\u003e\u003cmi\u003emax\u003c/mi\u003e\u003cmo\u003e⁡\u003c/mo\u003e\u003c/mrow\u003e\u003cmrow\u003e\u003cmi\u003ek\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cmi\u003eS\u003c/mi\u003e\u003c/msubsup\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ek\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003em_S = \\max(m_{S-1}, x_S) = \\max_{k=1}^{S} x_k\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e这表明更新后的 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003em\u003c/mi\u003e\u003cmi\u003eS\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003em_S\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e 正确地表示了前 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eS\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eS\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e 个输入元素的最大值。\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmi\u003eS\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003eS\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo\u003e×\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003em\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003eS\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003em\u003c/mi\u003e\u003cmi\u003eS\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003cmo\u003e+\u003c/mo\u003e\u003cmsup\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003eS\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003em\u003c/mi\u003e\u003cmi\u003eS\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ed_S = d_{S-1} \\times e^{m_{S-1} - m_S} + e^{x_S - m_S}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\n根据归纳假设，可以将 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003eS\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ed_{S-1}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e 展开为 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsubsup\u003e\u003cmo\u003e∑\u003c/mo\u003e\u003cmrow\u003e\u003cmi\u003ej\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cmrow\u003e\u003cmi\u003eS\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msubsup\u003e\u003cmsup\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ej\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003em\u003c/mi\u003e\u003cmrow\u003e\u003cmi\u003eS\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\sum_{j=1}^{S-1} e^{x_j - m_{S-1}}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e，通过替换和简化，我们可以得到：\n\u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmi\u003eS\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmsubsup\u003e\u003cmo\u003e∑\u003c/mo\u003e\u003cmrow\u003e\u003cmi\u003ej\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cmi\u003eS\u003c/mi\u003e\u003c/msubsup\u003e\u003cmsup\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ej\u003c/mi\u003e\u003c/msub\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003em\u003c/mi\u003e\u003cmi\u003eS\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ed_S = \\sum_{j=1}^{S} e^{x_j - m_S}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e这表明更新后的 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmi\u003eS\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ed_S\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e 正确地计算了前 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eS\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eS\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e 个元素，相对于新的最大值 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsub\u003e\u003cmi\u003em\u003c/mi\u003e\u003cmi\u003eS\u003c/mi\u003e\u003c/msub\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003em_S\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e 的归一化因子。\u003c/p\u003e\n\u003cp\u003e对应的 triton 代码如下：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-meta\"\u003e@triton.jit\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003esoftmax_kernel_v3\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003ex_ptr, output_ptr, row_stride, n_cols, BLOCK_SIZE: tl.constexpr\u003c/span\u003e):\n    pid = tl.program_id(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)\n\n    x_max = -\u003cspan class=\"hljs-built_in\"\u003efloat\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'inf'\u003c/span\u003e)\n    x_exp_sum = \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e offset \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, n_cols, BLOCK_SIZE):\n        col_range = tl.arange(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, BLOCK_SIZE)\n        col_mask = col_range + offset \u0026#x3C; n_cols\n        x = tl.load(x_ptr + pid * row_stride + col_range + offset, mask=col_mask, other=-\u003cspan class=\"hljs-built_in\"\u003efloat\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'inf'\u003c/span\u003e))\n        x_max_new = tl.maximum(x_max, tl.\u003cspan class=\"hljs-built_in\"\u003emax\u003c/span\u003e(x, axis=-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e))\n        x_exp_sum = x_exp_sum * tl.exp(x_max - x_max_new) + tl.\u003cspan class=\"hljs-built_in\"\u003esum\u003c/span\u003e(tl.exp(x - x_max_new), axis=-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\n        x_max = x_max_new\n    \n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e offset \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, n_cols, BLOCK_SIZE):\n        col_range = tl.arange(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, BLOCK_SIZE)\n        col_mask = col_range + offset \u0026#x3C; n_cols\n        x = tl.load(x_ptr + pid * row_stride + col_range + offset, mask=col_mask)\n        x_exp = tl.exp(x - x_max)\n        tl.store(output_ptr + pid * row_stride + col_range + offset, x_exp / x_exp_sum, mask=col_mask)\n\n\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003etriton_softmax_v3\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003ex\u003c/span\u003e):\n    n_rows, n_cols = x.shape\n    output = torch.empty_like(x)\n    BLOCK_SIZE = \u003cspan class=\"hljs-number\"\u003e256\u003c/span\u003e\n    softmax_kernel_v3[(n_rows,)](\n        x,\n        output,\n        x.stride(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e),\n        n_cols,\n        BLOCK_SIZE\n    )\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e output\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003ePuzzle 4: 更通用的分块策略\u003c/h3\u003e\n\u003cp\u003e前面我们都是按行分块，更通用的分块策略是按行和列都分块\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/image-5.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-meta\"\u003e@triton.jit\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003esoftmax_kernel_v4\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003ex_ptr, output_ptr, row_stride, n_rows, n_cols, BLOCK_ROW: tl.constexpr, BLOCK_COL: tl.constexpr\u003c/span\u003e):\n    pid = tl.program_id(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)\n\n    x_max = tl.full((BLOCK_ROW, ), -\u003cspan class=\"hljs-built_in\"\u003efloat\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'inf'\u003c/span\u003e), dtype=tl.float32)\n    x_exp_sum = tl.full((BLOCK_ROW, ), \u003cspan class=\"hljs-number\"\u003e0.0\u003c/span\u003e, dtype=tl.float32)\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e offset \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, n_cols, BLOCK_COL):\n        row_range = tl.arange(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, BLOCK_ROW) + pid * BLOCK_ROW\n        col_range = tl.arange(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, BLOCK_COL) + offset\n        x_mask = (row_range[:, \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e] \u0026#x3C; n_rows) \u0026#x26; (col_range \u0026#x3C; n_cols)\n        x_range = row_range[:, \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e] * row_stride + col_range\n        x = tl.load(x_ptr + x_range, mask=x_mask, other=-\u003cspan class=\"hljs-built_in\"\u003efloat\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'inf'\u003c/span\u003e))\n        x_max_new = tl.maximum(x_max, tl.\u003cspan class=\"hljs-built_in\"\u003emax\u003c/span\u003e(x, axis=-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e))\n        x_exp_sum = tl.exp(x_max - x_max_new) * x_exp_sum + tl.\u003cspan class=\"hljs-built_in\"\u003esum\u003c/span\u003e(tl.exp(x - x_max_new[:, \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e]), axis=-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\n        x_max = x_max_new\n    \n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e offset \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, n_cols, BLOCK_COL):\n        row_range = tl.arange(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, BLOCK_ROW) + pid * BLOCK_ROW\n        col_range = tl.arange(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, BLOCK_COL) + offset\n        x_mask = (row_range[:, \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e] \u0026#x3C; n_rows) \u0026#x26; (col_range \u0026#x3C; n_cols)\n        x_range = row_range[:, \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e] * row_stride + col_range\n        x = tl.load(x_ptr + x_range, mask=x_mask)\n        x_exp = tl.exp(x - x_max[:, \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e])\n        tl.store(output_ptr + x_range, x_exp / x_exp_sum[:, \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e], mask=x_mask)\n\n\n\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003etriton_softmax_v4\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003ex\u003c/span\u003e):\n    n_rows, n_cols = x.shape\n    output = torch.empty_like(x)\n    grid = \u003cspan class=\"hljs-keyword\"\u003elambda\u003c/span\u003e meta: (triton.cdiv(n_rows, meta[\u003cspan class=\"hljs-string\"\u003e'BLOCK_ROW'\u003c/span\u003e]), )\n    softmax_kernel_v4[grid](\n        x,\n        output,\n        x.stride(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e),\n        n_rows,\n        n_cols,\n        \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e,\n        \u003cspan class=\"hljs-number\"\u003e256\u003c/span\u003e\n    )\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e output\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003eflash attention 的实现\u003c/h3\u003e\n\u003cp\u003e最后，我们可以按照 flash attention 的伪代码实现 forward\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/flash_attention_forward_pseudocode.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/image.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-meta\"\u003e@triton.jit\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eflash_attention_kernel\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eq_ptr, k_ptr, v_ptr, output_ptr, n, d: tl.constexpr, BR: tl.constexpr, BC: tl.constexpr\u003c/span\u003e):\n    pid = tl.program_id(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)\n\n    q_row_range = tl.arange(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, BR) + pid * BR\n    q_range = q_row_range[:, \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e] * d + tl.arange(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, d)\n    q_mask = (q_row_range[:, \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e] \u0026#x3C; n) \u0026#x26; (tl.arange(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, d) \u0026#x3C; d)\n    q = tl.load(q_ptr + q_range, mask=q_mask)\n\n    o = tl.full((BR, d), \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, dtype=tl.float32)\n    l = tl.full((BR,), \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, dtype=tl.float32)\n    m = tl.full((BR,), -\u003cspan class=\"hljs-built_in\"\u003efloat\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'inf'\u003c/span\u003e), dtype=tl.float32)\n\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e j \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, n, BC):\n        kv_row_range = tl.arange(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, BC) + j\n        kv_range = kv_row_range[:, \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e] * d + tl.arange(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, d)\n        kv_mask = (kv_row_range[:, \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e] \u0026#x3C; n) \u0026#x26; (tl.arange(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, d) \u0026#x3C; d)\n        k = tl.load(k_ptr + kv_range, mask=kv_mask, other=\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)\n        v = tl.load(v_ptr + kv_range, mask=kv_mask, other=\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)\n        s = tl.dot(q, tl.trans(k))\n        s_mask = (q_row_range[:, \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e] \u0026#x3C; n) \u0026#x26; (kv_row_range \u0026#x3C; n)\n        s = tl.where(s_mask, s, -\u003cspan class=\"hljs-built_in\"\u003efloat\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'inf'\u003c/span\u003e))\n        m_new = tl.maximum(m, tl.\u003cspan class=\"hljs-built_in\"\u003emax\u003c/span\u003e(s, axis=-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e))\n        p = tl.exp(s - m_new[:, \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e])\n        l = tl.exp(m - m_new) * l + tl.\u003cspan class=\"hljs-built_in\"\u003esum\u003c/span\u003e(p, axis=-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\n        o = tl.exp(m - m_new)[:, \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e] * o + tl.dot(p, v)\n        m = m_new\n\n    tl.store(output_ptr + q_range, o / l[:, \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e], mask=q_mask)\n\n\n\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eflash_attention\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eq, k, v\u003c/span\u003e):\n    n, d = q.shape\n    BR = \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e\n    BC = \u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e\n    output = torch.empty((n, d), device=q.device)\n    grid = \u003cspan class=\"hljs-keyword\"\u003elambda\u003c/span\u003e meta: (triton.cdiv(n, BR),)\n    flash_attention_kernel[grid](\n        q,\n        k,\n        v,\n        output,\n        n,\n        d,\n        BR,\n        BC,\n    )\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e output\n\u003c/code\u003e\u003c/pre\u003e","title":"Triton Puzzles: 从 softmax 到 flash attention","date":"2024-03-29"}},"__N_SSG":true},"page":"/posts/[id]","query":{"id":"triton_flash_attention"},"buildId":"639Ox_e6KcvOQ7l5p__VL","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>