<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/github.min.css"/><title>Perplexity</title><meta name="next-head-count" content="4"/><link rel="preload" href="/_next/static/css/ab65974685f462b8.css" as="style"/><link rel="stylesheet" href="/_next/static/css/ab65974685f462b8.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-6cbe6e332df95288.js" defer=""></script><script src="/_next/static/chunks/main-26f9f36b33181737.js" defer=""></script><script src="/_next/static/chunks/pages/_app-2d6bf7b3192a8752.js" defer=""></script><script src="/_next/static/chunks/73-96e6cbd54826b874.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bid%5D-4b57e02c440700a5.js" defer=""></script><script src="/_next/static/LgI_2aaq_4GSrY0V2y1Bt/_buildManifest.js" defer=""></script><script src="/_next/static/LgI_2aaq_4GSrY0V2y1Bt/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="max-w-4xl px-4 mt-12 mb-24 mx-auto"><main><article><h1 class="text-3xl font-medium my-4 border-b-0">Perplexity</h1><div class="text-gray-500 mb-8 pb-2 border-b-2 border-solid border-slate-300"><time dateTime="2023-12-24">December 24, 2023</time></div><div><p>在大模型评测中，经常会看到 perplexity 这个指标，我只是知道 perplexity 越小越好，但是不知道它的具体含义，本文尝试深入理解 perplexity。</p>
<h2>直观理解“困惑”</h2>
<p>现在有两个句子</p>
<ul>
<li>The cat sat on the mat</li>
<li>The cat sat on the Thursday</li>
</ul>
<p>显然第一个句子很容易理解，第二个句子让人困惑。如果 model 生成了第二个句子，那么这个 model 是不尽如人意的。</p>
<h2>设计一个评测</h2>
<p>给 model 一段高质量的文本，比如 wikipedia 的一段文章，去测试 model 对这段文本的困惑程度，一个优秀的 model 应该对这段文本的“困惑程度”很小。</p>
<p>怎么去量化 model 对一段文本的困惑程度呢？</p>
<h2>如何量化“困惑”</h2>
<p>最容易想到的就是计算 model 生成这个句子的概率。如果 model 生成某一文本的概率大，就说 model 对这一文本的困惑度小，反之困惑度大。</p>
<p>现在基于 decoder only 架构的大模型生成一个新的 token 会依赖于之前生成的 token，如果我们想要计算 model 生成某一句子的概率，就是一个联合概率的问题，即：</p>
<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mn>1</mn><mo>:</mo><mi>L</mi></mrow></msub><mo stretchy="false">)</mo><mo>=</mo><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi>p</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mrow><mn>1</mn><mo>:</mo><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(x_{1:L}) = \prod_{i=1}^{L} p(x_i|x_{1:i-1})</annotation></semantics></math></span>
<p>但是这样做有一个问题，就是如果句子很长，那 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mn>1</mn><mo>:</mo><mi>L</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(x_{1:L})</annotation></semantics></math></span> 就会很小，这样的话，我们就很难比较两个句子的概率了。</p>
<p>那如果给每个 token 的概率 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mrow><mn>1</mn><mo>:</mo><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(x_i|x_{1:i-1})</annotation></semantics></math></span> 取平均值呢？考虑句子 “the cat sat on the mat”，假设每个 token 的概率都是 0.5，那么用平均值计算出来的结果就是 0.5，现在假设 model 认为 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>m</mi><mi>a</mi><mi>t</mi><mi mathvariant="normal">∣</mi><mi>t</mi><mi>h</mi><mi>e</mi><mspace width="1em"></mspace><mi>c</mi><mi>a</mi><mi>t</mi><mspace width="1em"></mspace><mi>s</mi><mi>a</mi><mi>t</mi><mspace width="1em"></mspace><mi>o</mi><mi>n</mi><mspace width="1em"></mspace><mi>t</mi><mi>h</mi><mi>e</mi><mo stretchy="false">)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">p(mat|the \quad cat \quad sat \quad on \quad the) = 0</annotation></semantics></math></span>，这时候用平均值计算出来的结果就是 (0.5 * 5 + 0) / 6 = 0.4167，这个值确实比 0.5 小，但是这个指标并没有给到 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>m</mi><mi>a</mi><mi>t</mi><mi mathvariant="normal">∣</mi><mi>t</mi><mi>h</mi><mi>e</mi><mspace width="1em"></mspace><mi>c</mi><mi>a</mi><mi>t</mi><mspace width="1em"></mspace><mi>s</mi><mi>a</mi><mi>t</mi><mspace width="1em"></mspace><mi>o</mi><mi>n</mi><mspace width="1em"></mspace><mi>t</mi><mi>h</mi><mi>e</mi><mo stretchy="false">)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">p(mat|the \quad cat \quad sat \quad on \quad the) = 0</annotation></semantics></math></span> 这个显然不合理的概率一个足够大的惩罚。即这个句子应该是很有可能出现的，并没有什么让人困惑的地方，如果对这个句子感到困惑，就应该在评测指标上给予足够的惩罚。</p>
<p>几何平均是一个更合理的计算方式，几何平均值的计算公式是：</p>
<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mroot><mrow><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi>p</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mrow><mn>1</mn><mo>:</mo><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><mi>L</mi></mroot></mrow><annotation encoding="application/x-tex">\sqrt[L]{\prod_{i=1}^{L} p(x_i|x_{1:i-1})}</annotation></semantics></math></span>
<p>困惑度应该和生成概率成反比，对概率取倒数，就是 perplexity 的定义：</p>
<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Perplexity</mtext><mo>=</mo><mfrac><mn>1</mn><mroot><mrow><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi>p</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mrow><mn>1</mn><mo>:</mo><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><mi>L</mi></mroot></mfrac></mrow><annotation encoding="application/x-tex">\text{Perplexity} = \frac{1}{\sqrt[L]{\prod_{i=1}^{L} p(x_i|x_{1:i-1})}}</annotation></semantics></math></span>
<p>如果某一 token 的概率很小，比如说为 0，那么 perplexity 就会变得无穷大，这样就给了这个 token 一个足够大的惩罚。</p>
<h2>Perplexity 的信息论解释</h2>
<p>对 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mi>e</mi><mi>r</mi><mi>p</mi><mi>l</mi><mi>e</mi><mi>x</mi><mi>i</mi><mi>t</mi><mi>y</mi></mrow><annotation encoding="application/x-tex">Perplexity</annotation></semantics></math></span> 取对数，然后指数化，可以得到：</p>
<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mtext>Perplexity</mtext></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mfrac><mn>1</mn><mroot><mrow><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi>p</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mrow><mn>1</mn><mo>:</mo><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><mi>L</mi></mroot></mfrac></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi>exp</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mi>log</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mfrac><mn>1</mn><mroot><mrow><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi>p</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mrow><mn>1</mn><mo>:</mo><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><mi>L</mi></mroot></mfrac><mo fence="true">)</mo></mrow><mo fence="true">)</mo></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi>exp</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mi>log</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><msup><mrow><mo fence="true">(</mo><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi>p</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mrow><mn>1</mn><mo>:</mo><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo fence="true">)</mo></mrow><mrow><mo>−</mo><mfrac><mn>1</mn><mi>L</mi></mfrac></mrow></msup><mo fence="true">)</mo></mrow><mo fence="true">)</mo></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi>exp</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mo>−</mo><mfrac><mn>1</mn><mi>L</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mrow><mn>1</mn><mo>:</mo><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo fence="true">)</mo></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi>exp</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mfrac><mn>1</mn><mi>L</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi>log</mi><mo>⁡</mo><mfrac><mn>1</mn><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mrow><mn>1</mn><mo>:</mo><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow></mfrac><mo fence="true">)</mo></mrow></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align*}
\text{Perplexity} &#x26;= \frac{1}{\sqrt[L]{\prod_{i=1}^{L} p(x_i|x_{1:i-1})}} \\
&#x26;= \exp\left(\log\left(\frac{1}{\sqrt[L]{\prod_{i=1}^{L} p(x_i|x_{1:i-1})}}\right)\right) \\
&#x26;= \exp\left(\log\left(\left(\prod_{i=1}^{L} p(x_i|x_{1:i-1})\right)^{-\frac{1}{L}}\right)\right) \\
&#x26;= \exp\left(-\frac{1}{L} \sum_{i=1}^{L} \log(p(x_i|x_{1:i-1}))\right) \\
&#x26;= \exp\left(\frac{1}{L} \sum_{i=1}^{L} \log \frac{1}{p(x_i|x_{1:i-1})}\right)
\end{align*}</annotation></semantics></math></span>
<p>其中 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mfrac><mn>1</mn><mi>L</mi></mfrac><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></msubsup><mi>log</mi><mo>⁡</mo><mfrac><mn>1</mn><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mrow><mn>1</mn><mo>:</mo><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow></mfrac><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\exp(\frac{1}{L} \sum_{i=1}^{L} \log \frac{1}{p(x_i|x_{1:i-1})})</annotation></semantics></math></span> 就是交叉熵，即：</p>
<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Perplexity</mtext><mo>=</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mtext>CrossEntropy</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{Perplexity} = \exp(\text{CrossEntropy})</annotation></semantics></math></span>
<p>在信息论中，一个事件的信息量与该事件发生的概率成反比在信息论中，概率的倒数被称为概率的“信息量”。信息论告诉我们，对于一个特定事件，其最优编码长度应该与其信息量相等。所以 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mi>L</mi></mfrac><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></msubsup><mi>log</mi><mo>⁡</mo><mfrac><mn>1</mn><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mrow><mn>1</mn><mo>:</mo><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{L} \sum_{i=1}^{L} \log \frac{1}{p(x_i|x_{1:i-1})}</annotation></semantics></math></span> 表示使用 model 预测的分布来编码的平均长度。<span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mi>a</mi><mi>v</mi><mi>e</mi><mi>r</mi><mi>a</mi><mi>g</mi><mi>e</mi><mi>c</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi><mi>e</mi><mi>n</mi><mi>g</mi><mi>t</mi><mi>h</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">exp(average code length)</annotation></semantics></math></span> 可以看作是 model 在预测下一个 token 时平均有多少种选择。类似排列组合问题，每一位都有 0 1 两种选择，一共有 3 位，那么一共有 <span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">2^3</annotation></semantics></math></span> 种选择。</p>
<h2>参考资料</h2>
<p><a href="https://stanford-cs324.github.io/winter2022/lectures/capabilities/#language-modeling">https://stanford-cs324.github.io/winter2022/lectures/capabilities/#language-modeling</a></p>
<p><a href="https://www.zhihu.com/tardis/zm/ans/244557337?source_id=1003">为什么交叉熵（cross-entropy）可以用于计算代价？</a></p></div></article></main><div class="mt-12"><a href="/">← 返回首页</a></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"postData":{"id":"perplexity","contentHtml":"\u003cp\u003e在大模型评测中，经常会看到 perplexity 这个指标，我只是知道 perplexity 越小越好，但是不知道它的具体含义，本文尝试深入理解 perplexity。\u003c/p\u003e\n\u003ch2\u003e直观理解“困惑”\u003c/h2\u003e\n\u003cp\u003e现在有两个句子\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe cat sat on the mat\u003c/li\u003e\n\u003cli\u003eThe cat sat on the Thursday\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e显然第一个句子很容易理解，第二个句子让人困惑。如果 model 生成了第二个句子，那么这个 model 是不尽如人意的。\u003c/p\u003e\n\u003ch2\u003e设计一个评测\u003c/h2\u003e\n\u003cp\u003e给 model 一段高质量的文本，比如 wikipedia 的一段文章，去测试 model 对这段文本的困惑程度，一个优秀的 model 应该对这段文本的“困惑程度”很小。\u003c/p\u003e\n\u003cp\u003e怎么去量化 model 对一段文本的困惑程度呢？\u003c/p\u003e\n\u003ch2\u003e如何量化“困惑”\u003c/h2\u003e\n\u003cp\u003e最容易想到的就是计算 model 生成这个句子的概率。如果 model 生成某一文本的概率大，就说 model 对这一文本的困惑度小，反之困惑度大。\u003c/p\u003e\n\u003cp\u003e现在基于 decoder only 架构的大模型生成一个新的 token 会依赖于之前生成的 token，如果我们想要计算 model 生成某一句子的概率，就是一个联合概率的问题，即：\u003c/p\u003e\n\u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmrow\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo\u003e:\u003c/mo\u003e\u003cmi\u003eL\u003c/mi\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmunderover\u003e\u003cmo\u003e∏\u003c/mo\u003e\u003cmrow\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cmi\u003eL\u003c/mi\u003e\u003c/munderover\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmi mathvariant=\"normal\"\u003e∣\u003c/mi\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmrow\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo\u003e:\u003c/mo\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ep(x_{1:L}) = \\prod_{i=1}^{L} p(x_i|x_{1:i-1})\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\n\u003cp\u003e但是这样做有一个问题，就是如果句子很长，那 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmrow\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo\u003e:\u003c/mo\u003e\u003cmi\u003eL\u003c/mi\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ep(x_{1:L})\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e 就会很小，这样的话，我们就很难比较两个句子的概率了。\u003c/p\u003e\n\u003cp\u003e那如果给每个 token 的概率 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmi mathvariant=\"normal\"\u003e∣\u003c/mi\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmrow\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo\u003e:\u003c/mo\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ep(x_i|x_{1:i-1})\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e 取平均值呢？考虑句子 “the cat sat on the mat”，假设每个 token 的概率都是 0.5，那么用平均值计算出来的结果就是 0.5，现在假设 model 认为 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003em\u003c/mi\u003e\u003cmi\u003ea\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmi mathvariant=\"normal\"\u003e∣\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmi\u003eh\u003c/mi\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmspace width=\"1em\"\u003e\u003c/mspace\u003e\u003cmi\u003ec\u003c/mi\u003e\u003cmi\u003ea\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmspace width=\"1em\"\u003e\u003c/mspace\u003e\u003cmi\u003es\u003c/mi\u003e\u003cmi\u003ea\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmspace width=\"1em\"\u003e\u003c/mspace\u003e\u003cmi\u003eo\u003c/mi\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmspace width=\"1em\"\u003e\u003c/mspace\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmi\u003eh\u003c/mi\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e0\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ep(mat|the \\quad cat \\quad sat \\quad on \\quad the) = 0\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e，这时候用平均值计算出来的结果就是 (0.5 * 5 + 0) / 6 = 0.4167，这个值确实比 0.5 小，但是这个指标并没有给到 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003em\u003c/mi\u003e\u003cmi\u003ea\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmi mathvariant=\"normal\"\u003e∣\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmi\u003eh\u003c/mi\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmspace width=\"1em\"\u003e\u003c/mspace\u003e\u003cmi\u003ec\u003c/mi\u003e\u003cmi\u003ea\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmspace width=\"1em\"\u003e\u003c/mspace\u003e\u003cmi\u003es\u003c/mi\u003e\u003cmi\u003ea\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmspace width=\"1em\"\u003e\u003c/mspace\u003e\u003cmi\u003eo\u003c/mi\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmspace width=\"1em\"\u003e\u003c/mspace\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmi\u003eh\u003c/mi\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e0\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ep(mat|the \\quad cat \\quad sat \\quad on \\quad the) = 0\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e 这个显然不合理的概率一个足够大的惩罚。即这个句子应该是很有可能出现的，并没有什么让人困惑的地方，如果对这个句子感到困惑，就应该在评测指标上给予足够的惩罚。\u003c/p\u003e\n\u003cp\u003e几何平均是一个更合理的计算方式，几何平均值的计算公式是：\u003c/p\u003e\n\u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmroot\u003e\u003cmrow\u003e\u003cmunderover\u003e\u003cmo\u003e∏\u003c/mo\u003e\u003cmrow\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cmi\u003eL\u003c/mi\u003e\u003c/munderover\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmi mathvariant=\"normal\"\u003e∣\u003c/mi\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmrow\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo\u003e:\u003c/mo\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cmi\u003eL\u003c/mi\u003e\u003c/mroot\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\sqrt[L]{\\prod_{i=1}^{L} p(x_i|x_{1:i-1})}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\n\u003cp\u003e困惑度应该和生成概率成反比，对概率取倒数，就是 perplexity 的定义：\u003c/p\u003e\n\u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmtext\u003ePerplexity\u003c/mtext\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmfrac\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmroot\u003e\u003cmrow\u003e\u003cmunderover\u003e\u003cmo\u003e∏\u003c/mo\u003e\u003cmrow\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cmi\u003eL\u003c/mi\u003e\u003c/munderover\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmi mathvariant=\"normal\"\u003e∣\u003c/mi\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmrow\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo\u003e:\u003c/mo\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cmi\u003eL\u003c/mi\u003e\u003c/mroot\u003e\u003c/mfrac\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\text{Perplexity} = \\frac{1}{\\sqrt[L]{\\prod_{i=1}^{L} p(x_i|x_{1:i-1})}}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\n\u003cp\u003e如果某一 token 的概率很小，比如说为 0，那么 perplexity 就会变得无穷大，这样就给了这个 token 一个足够大的惩罚。\u003c/p\u003e\n\u003ch2\u003ePerplexity 的信息论解释\u003c/h2\u003e\n\u003cp\u003e对 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eP\u003c/mi\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmi\u003er\u003c/mi\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmi\u003el\u003c/mi\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmi\u003ey\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003ePerplexity\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e 取对数，然后指数化，可以得到：\u003c/p\u003e\n\u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmtable rowspacing=\"0.25em\" columnalign=\"right left\" columnspacing=\"0em\"\u003e\u003cmtr\u003e\u003cmtd\u003e\u003cmstyle scriptlevel=\"0\" displaystyle=\"true\"\u003e\u003cmtext\u003ePerplexity\u003c/mtext\u003e\u003c/mstyle\u003e\u003c/mtd\u003e\u003cmtd\u003e\u003cmstyle scriptlevel=\"0\" displaystyle=\"true\"\u003e\u003cmrow\u003e\u003cmrow\u003e\u003c/mrow\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmfrac\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmroot\u003e\u003cmrow\u003e\u003cmunderover\u003e\u003cmo\u003e∏\u003c/mo\u003e\u003cmrow\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cmi\u003eL\u003c/mi\u003e\u003c/munderover\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmi mathvariant=\"normal\"\u003e∣\u003c/mi\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmrow\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo\u003e:\u003c/mo\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cmi\u003eL\u003c/mi\u003e\u003c/mroot\u003e\u003c/mfrac\u003e\u003c/mrow\u003e\u003c/mstyle\u003e\u003c/mtd\u003e\u003c/mtr\u003e\u003cmtr\u003e\u003cmtd\u003e\u003cmstyle scriptlevel=\"0\" displaystyle=\"true\"\u003e\u003cmrow\u003e\u003c/mrow\u003e\u003c/mstyle\u003e\u003c/mtd\u003e\u003cmtd\u003e\u003cmstyle scriptlevel=\"0\" displaystyle=\"true\"\u003e\u003cmrow\u003e\u003cmrow\u003e\u003c/mrow\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi\u003eexp\u003c/mi\u003e\u003cmo\u003e⁡\u003c/mo\u003e\u003cmrow\u003e\u003cmo fence=\"true\"\u003e(\u003c/mo\u003e\u003cmi\u003elog\u003c/mi\u003e\u003cmo\u003e⁡\u003c/mo\u003e\u003cmrow\u003e\u003cmo fence=\"true\"\u003e(\u003c/mo\u003e\u003cmfrac\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmroot\u003e\u003cmrow\u003e\u003cmunderover\u003e\u003cmo\u003e∏\u003c/mo\u003e\u003cmrow\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cmi\u003eL\u003c/mi\u003e\u003c/munderover\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmi mathvariant=\"normal\"\u003e∣\u003c/mi\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmrow\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo\u003e:\u003c/mo\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cmi\u003eL\u003c/mi\u003e\u003c/mroot\u003e\u003c/mfrac\u003e\u003cmo fence=\"true\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cmo fence=\"true\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003c/mrow\u003e\u003c/mstyle\u003e\u003c/mtd\u003e\u003c/mtr\u003e\u003cmtr\u003e\u003cmtd\u003e\u003cmstyle scriptlevel=\"0\" displaystyle=\"true\"\u003e\u003cmrow\u003e\u003c/mrow\u003e\u003c/mstyle\u003e\u003c/mtd\u003e\u003cmtd\u003e\u003cmstyle scriptlevel=\"0\" displaystyle=\"true\"\u003e\u003cmrow\u003e\u003cmrow\u003e\u003c/mrow\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi\u003eexp\u003c/mi\u003e\u003cmo\u003e⁡\u003c/mo\u003e\u003cmrow\u003e\u003cmo fence=\"true\"\u003e(\u003c/mo\u003e\u003cmi\u003elog\u003c/mi\u003e\u003cmo\u003e⁡\u003c/mo\u003e\u003cmrow\u003e\u003cmo fence=\"true\"\u003e(\u003c/mo\u003e\u003cmsup\u003e\u003cmrow\u003e\u003cmo fence=\"true\"\u003e(\u003c/mo\u003e\u003cmunderover\u003e\u003cmo\u003e∏\u003c/mo\u003e\u003cmrow\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cmi\u003eL\u003c/mi\u003e\u003c/munderover\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmi mathvariant=\"normal\"\u003e∣\u003c/mi\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmrow\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo\u003e:\u003c/mo\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo fence=\"true\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cmrow\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmfrac\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmi\u003eL\u003c/mi\u003e\u003c/mfrac\u003e\u003c/mrow\u003e\u003c/msup\u003e\u003cmo fence=\"true\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cmo fence=\"true\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003c/mrow\u003e\u003c/mstyle\u003e\u003c/mtd\u003e\u003c/mtr\u003e\u003cmtr\u003e\u003cmtd\u003e\u003cmstyle scriptlevel=\"0\" displaystyle=\"true\"\u003e\u003cmrow\u003e\u003c/mrow\u003e\u003c/mstyle\u003e\u003c/mtd\u003e\u003cmtd\u003e\u003cmstyle scriptlevel=\"0\" displaystyle=\"true\"\u003e\u003cmrow\u003e\u003cmrow\u003e\u003c/mrow\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi\u003eexp\u003c/mi\u003e\u003cmo\u003e⁡\u003c/mo\u003e\u003cmrow\u003e\u003cmo fence=\"true\"\u003e(\u003c/mo\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmfrac\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmi\u003eL\u003c/mi\u003e\u003c/mfrac\u003e\u003cmunderover\u003e\u003cmo\u003e∑\u003c/mo\u003e\u003cmrow\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cmi\u003eL\u003c/mi\u003e\u003c/munderover\u003e\u003cmi\u003elog\u003c/mi\u003e\u003cmo\u003e⁡\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmi mathvariant=\"normal\"\u003e∣\u003c/mi\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmrow\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo\u003e:\u003c/mo\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003cmo fence=\"true\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003c/mrow\u003e\u003c/mstyle\u003e\u003c/mtd\u003e\u003c/mtr\u003e\u003cmtr\u003e\u003cmtd\u003e\u003cmstyle scriptlevel=\"0\" displaystyle=\"true\"\u003e\u003cmrow\u003e\u003c/mrow\u003e\u003c/mstyle\u003e\u003c/mtd\u003e\u003cmtd\u003e\u003cmstyle scriptlevel=\"0\" displaystyle=\"true\"\u003e\u003cmrow\u003e\u003cmrow\u003e\u003c/mrow\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi\u003eexp\u003c/mi\u003e\u003cmo\u003e⁡\u003c/mo\u003e\u003cmrow\u003e\u003cmo fence=\"true\"\u003e(\u003c/mo\u003e\u003cmfrac\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmi\u003eL\u003c/mi\u003e\u003c/mfrac\u003e\u003cmunderover\u003e\u003cmo\u003e∑\u003c/mo\u003e\u003cmrow\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cmi\u003eL\u003c/mi\u003e\u003c/munderover\u003e\u003cmi\u003elog\u003c/mi\u003e\u003cmo\u003e⁡\u003c/mo\u003e\u003cmfrac\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmrow\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmi mathvariant=\"normal\"\u003e∣\u003c/mi\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmrow\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo\u003e:\u003c/mo\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003c/mfrac\u003e\u003cmo fence=\"true\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003c/mrow\u003e\u003c/mstyle\u003e\u003c/mtd\u003e\u003c/mtr\u003e\u003c/mtable\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\begin{align*}\n\\text{Perplexity} \u0026#x26;= \\frac{1}{\\sqrt[L]{\\prod_{i=1}^{L} p(x_i|x_{1:i-1})}} \\\\\n\u0026#x26;= \\exp\\left(\\log\\left(\\frac{1}{\\sqrt[L]{\\prod_{i=1}^{L} p(x_i|x_{1:i-1})}}\\right)\\right) \\\\\n\u0026#x26;= \\exp\\left(\\log\\left(\\left(\\prod_{i=1}^{L} p(x_i|x_{1:i-1})\\right)^{-\\frac{1}{L}}\\right)\\right) \\\\\n\u0026#x26;= \\exp\\left(-\\frac{1}{L} \\sum_{i=1}^{L} \\log(p(x_i|x_{1:i-1}))\\right) \\\\\n\u0026#x26;= \\exp\\left(\\frac{1}{L} \\sum_{i=1}^{L} \\log \\frac{1}{p(x_i|x_{1:i-1})}\\right)\n\\end{align*}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\n\u003cp\u003e其中 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003eexp\u003c/mi\u003e\u003cmo\u003e⁡\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmfrac\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmi\u003eL\u003c/mi\u003e\u003c/mfrac\u003e\u003cmsubsup\u003e\u003cmo\u003e∑\u003c/mo\u003e\u003cmrow\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cmi\u003eL\u003c/mi\u003e\u003c/msubsup\u003e\u003cmi\u003elog\u003c/mi\u003e\u003cmo\u003e⁡\u003c/mo\u003e\u003cmfrac\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmrow\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmi mathvariant=\"normal\"\u003e∣\u003c/mi\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmrow\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo\u003e:\u003c/mo\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003c/mfrac\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\exp(\\frac{1}{L} \\sum_{i=1}^{L} \\log \\frac{1}{p(x_i|x_{1:i-1})})\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e 就是交叉熵，即：\u003c/p\u003e\n\u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmtext\u003ePerplexity\u003c/mtext\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmi\u003eexp\u003c/mi\u003e\u003cmo\u003e⁡\u003c/mo\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmtext\u003eCrossEntropy\u003c/mtext\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\text{Perplexity} = \\exp(\\text{CrossEntropy})\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\n\u003cp\u003e在信息论中，一个事件的信息量与该事件发生的概率成反比在信息论中，概率的倒数被称为概率的“信息量”。信息论告诉我们，对于一个特定事件，其最优编码长度应该与其信息量相等。所以 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmfrac\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmi\u003eL\u003c/mi\u003e\u003c/mfrac\u003e\u003cmsubsup\u003e\u003cmo\u003e∑\u003c/mo\u003e\u003cmrow\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003cmi\u003eL\u003c/mi\u003e\u003c/msubsup\u003e\u003cmi\u003elog\u003c/mi\u003e\u003cmo\u003e⁡\u003c/mo\u003e\u003cmfrac\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmrow\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ei\u003c/mi\u003e\u003c/msub\u003e\u003cmi mathvariant=\"normal\"\u003e∣\u003c/mi\u003e\u003cmsub\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmrow\u003e\u003cmn\u003e1\u003c/mn\u003e\u003cmo\u003e:\u003c/mo\u003e\u003cmi\u003ei\u003c/mi\u003e\u003cmo\u003e−\u003c/mo\u003e\u003cmn\u003e1\u003c/mn\u003e\u003c/mrow\u003e\u003c/msub\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003c/mfrac\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e\\frac{1}{L} \\sum_{i=1}^{L} \\log \\frac{1}{p(x_i|x_{1:i-1})}\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e 表示使用 model 预测的分布来编码的平均长度。\u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmi\u003ex\u003c/mi\u003e\u003cmi\u003ep\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e(\u003c/mo\u003e\u003cmi\u003ea\u003c/mi\u003e\u003cmi\u003ev\u003c/mi\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmi\u003er\u003c/mi\u003e\u003cmi\u003ea\u003c/mi\u003e\u003cmi\u003eg\u003c/mi\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmi\u003ec\u003c/mi\u003e\u003cmi\u003eo\u003c/mi\u003e\u003cmi\u003ed\u003c/mi\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmi\u003el\u003c/mi\u003e\u003cmi\u003ee\u003c/mi\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmi\u003eg\u003c/mi\u003e\u003cmi\u003et\u003c/mi\u003e\u003cmi\u003eh\u003c/mi\u003e\u003cmo stretchy=\"false\"\u003e)\u003c/mo\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003eexp(average code length)\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e 可以看作是 model 在预测下一个 token 时平均有多少种选择。类似排列组合问题，每一位都有 0 1 两种选择，一共有 3 位，那么一共有 \u003cspan class=\"katex\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmsup\u003e\u003cmn\u003e2\u003c/mn\u003e\u003cmn\u003e3\u003c/mn\u003e\u003c/msup\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003e2^3\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e 种选择。\u003c/p\u003e\n\u003ch2\u003e参考资料\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://stanford-cs324.github.io/winter2022/lectures/capabilities/#language-modeling\"\u003ehttps://stanford-cs324.github.io/winter2022/lectures/capabilities/#language-modeling\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.zhihu.com/tardis/zm/ans/244557337?source_id=1003\"\u003e为什么交叉熵（cross-entropy）可以用于计算代价？\u003c/a\u003e\u003c/p\u003e","title":"Perplexity","date":"2023-12-24"}},"__N_SSG":true},"page":"/posts/[id]","query":{"id":"perplexity"},"buildId":"LgI_2aaq_4GSrY0V2y1Bt","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>