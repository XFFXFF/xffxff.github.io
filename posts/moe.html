<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/github.min.css"/><title>简单了解什么是 MoE</title><meta name="next-head-count" content="4"/><link rel="preload" href="/_next/static/css/ab65974685f462b8.css" as="style"/><link rel="stylesheet" href="/_next/static/css/ab65974685f462b8.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-6cbe6e332df95288.js" defer=""></script><script src="/_next/static/chunks/main-26f9f36b33181737.js" defer=""></script><script src="/_next/static/chunks/pages/_app-2d6bf7b3192a8752.js" defer=""></script><script src="/_next/static/chunks/73-96e6cbd54826b874.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bid%5D-4b57e02c440700a5.js" defer=""></script><script src="/_next/static/639Ox_e6KcvOQ7l5p__VL/_buildManifest.js" defer=""></script><script src="/_next/static/639Ox_e6KcvOQ7l5p__VL/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="max-w-4xl px-4 mt-12 mb-24 mx-auto"><main><article><h1 class="text-3xl font-medium my-4 border-b-0">简单了解什么是 MoE</h1><div class="text-gray-500 mb-8 pb-2 border-b-2 border-solid border-slate-300"><time dateTime="2023-12-13">December 13, 2023</time></div><div><p>这两天 <a href="https://mistral.ai/news/mixtral-of-experts/">Mixtral 8x7B</a> 发布，效果很不错，于是好奇什么是 MoE。</p>
<p>根据 8x7B 这个名字，我还以为是下面这种架构，有 8 个 7B 的模型，每个模型是一个 expert，然后用一个 gating network 来决定用哪个 expert。</p>
<p><img src="/moe.png" alt=""></p>
<p>实际上并非如此，并不是有 8 个完整的 7B 模型，这 8 个 expert 有一部分参数是共享的，只有一部分参数是专门为这个 expert 专门训练的。以 Mixtral 8x7B 模型为例，这些 expert 只在 Transformer block 的 FFN 层是不同的，其它部分都是共享的。</p>
<p><img src="/switch_transformer.png" alt=""></p>
<p>每次过 FFN 层时，都会有一个 router 来决定某个 token 由哪个或者哪几个 expert 来处理。</p>
<p>Router 其实很简单，一个全连阶层加上 softmax，输出是一个概率分布，每个 expert 对应一个概率，概率越大，这个 expert 被选中的概率越大。</p>
<p>不考虑优化的话 MoE 的逻辑和实现其实很简单，可以直接看看  <a href="https://github.com/huggingface/transformers/blob/371fb0b7dc1b533917e2f85b464a3ec9c74f28b9/src/transformers/models/mixtral/modeling_mixtral.py#L703-L746">huggingface transformers mixtral model 的核心逻辑</a></p>
<p>涉及到 tensor 的一些操作，直接看代码很难 trace 到一些细节，我通常喜欢单步调试。8x7B 模型太大了，我改了 model config，把模型变小，然后单步调试。下面是我调试的代码：</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig
<span class="hljs-keyword">from</span> transformers.models.mixtral <span class="hljs-keyword">import</span> MixtralForCausalLM 
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

config = AutoConfig.from_pretrained(<span class="hljs-string">"mistralai/Mixtral-8x7B-v0.1"</span>)
config.num_hidden_layers = <span class="hljs-number">4</span>
config.num_attention_heads = <span class="hljs-number">16</span>
config.max_position_embeddings = <span class="hljs-number">128</span>
config.intermediate_size = <span class="hljs-number">128</span>
config.hidden_size = <span class="hljs-number">128</span>

tok = AutoTokenizer.from_pretrained(<span class="hljs-string">"mistralai/Mixtral-8x7B-v0.1"</span>)
x = tok.encode(<span class="hljs-string">"The mistral wind in is a phenomenon "</span>, return_tensors=<span class="hljs-string">"pt"</span>).cuda()
model = MixtralForCausalLM(config).to(<span class="hljs-string">"cuda"</span>)

model.generate(x)
</code></pre>
<p><a href="https://huggingface.co/blog/moe">Mixture of Experts Explained</a> 这篇文章非常棒，推荐阅读。</p></div></article></main><div class="mt-12"><a href="/">← 返回首页</a></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"postData":{"id":"moe","contentHtml":"\u003cp\u003e这两天 \u003ca href=\"https://mistral.ai/news/mixtral-of-experts/\"\u003eMixtral 8x7B\u003c/a\u003e 发布，效果很不错，于是好奇什么是 MoE。\u003c/p\u003e\n\u003cp\u003e根据 8x7B 这个名字，我还以为是下面这种架构，有 8 个 7B 的模型，每个模型是一个 expert，然后用一个 gating network 来决定用哪个 expert。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/moe.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e实际上并非如此，并不是有 8 个完整的 7B 模型，这 8 个 expert 有一部分参数是共享的，只有一部分参数是专门为这个 expert 专门训练的。以 Mixtral 8x7B 模型为例，这些 expert 只在 Transformer block 的 FFN 层是不同的，其它部分都是共享的。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/switch_transformer.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003e每次过 FFN 层时，都会有一个 router 来决定某个 token 由哪个或者哪几个 expert 来处理。\u003c/p\u003e\n\u003cp\u003eRouter 其实很简单，一个全连阶层加上 softmax，输出是一个概率分布，每个 expert 对应一个概率，概率越大，这个 expert 被选中的概率越大。\u003c/p\u003e\n\u003cp\u003e不考虑优化的话 MoE 的逻辑和实现其实很简单，可以直接看看  \u003ca href=\"https://github.com/huggingface/transformers/blob/371fb0b7dc1b533917e2f85b464a3ec9c74f28b9/src/transformers/models/mixtral/modeling_mixtral.py#L703-L746\"\u003ehuggingface transformers mixtral model 的核心逻辑\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e涉及到 tensor 的一些操作，直接看代码很难 trace 到一些细节，我通常喜欢单步调试。8x7B 模型太大了，我改了 model config，把模型变小，然后单步调试。下面是我调试的代码：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e transformers \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e AutoConfig\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e transformers.models.mixtral \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e MixtralForCausalLM \n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e transformers \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e AutoTokenizer\n\nconfig = AutoConfig.from_pretrained(\u003cspan class=\"hljs-string\"\u003e\"mistralai/Mixtral-8x7B-v0.1\"\u003c/span\u003e)\nconfig.num_hidden_layers = \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e\nconfig.num_attention_heads = \u003cspan class=\"hljs-number\"\u003e16\u003c/span\u003e\nconfig.max_position_embeddings = \u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e\nconfig.intermediate_size = \u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e\nconfig.hidden_size = \u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e\n\ntok = AutoTokenizer.from_pretrained(\u003cspan class=\"hljs-string\"\u003e\"mistralai/Mixtral-8x7B-v0.1\"\u003c/span\u003e)\nx = tok.encode(\u003cspan class=\"hljs-string\"\u003e\"The mistral wind in is a phenomenon \"\u003c/span\u003e, return_tensors=\u003cspan class=\"hljs-string\"\u003e\"pt\"\u003c/span\u003e).cuda()\nmodel = MixtralForCausalLM(config).to(\u003cspan class=\"hljs-string\"\u003e\"cuda\"\u003c/span\u003e)\n\nmodel.generate(x)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ca href=\"https://huggingface.co/blog/moe\"\u003eMixture of Experts Explained\u003c/a\u003e 这篇文章非常棒，推荐阅读。\u003c/p\u003e","title":"简单了解什么是 MoE","date":"2023-12-13"}},"__N_SSG":true},"page":"/posts/[id]","query":{"id":"moe"},"buildId":"639Ox_e6KcvOQ7l5p__VL","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>