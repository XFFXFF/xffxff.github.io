<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/github.min.css"/><title>LLM.C 中的 CUDA Kernel</title><meta name="next-head-count" content="4"/><link rel="preload" href="/_next/static/css/ab65974685f462b8.css" as="style"/><link rel="stylesheet" href="/_next/static/css/ab65974685f462b8.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-6cbe6e332df95288.js" defer=""></script><script src="/_next/static/chunks/main-26f9f36b33181737.js" defer=""></script><script src="/_next/static/chunks/pages/_app-2d6bf7b3192a8752.js" defer=""></script><script src="/_next/static/chunks/73-96e6cbd54826b874.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bid%5D-4b57e02c440700a5.js" defer=""></script><script src="/_next/static/wTPBy1ZzrXj1NkuaydzLJ/_buildManifest.js" defer=""></script><script src="/_next/static/wTPBy1ZzrXj1NkuaydzLJ/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="max-w-4xl px-4 mt-12 mb-24 mx-auto"><main><article><h1 class="text-3xl font-medium my-4 border-b-0">LLM.C 中的 CUDA Kernel</h1><div class="text-gray-500 mb-8 pb-2 border-b-2 border-solid border-slate-300"><time dateTime="2024-07-17">July 17, 2024</time></div><div><h2>Matmul</h2>
<pre><code class="hljs language-c">__global__ <span class="hljs-type">void</span> __launch_bounds__(<span class="hljs-number">16</span>*<span class="hljs-number">16</span>, <span class="hljs-number">2</span>) matmul_forward_kernel4(<span class="hljs-type">float</span>* out,
                                                                   <span class="hljs-type">const</span> <span class="hljs-type">float</span>* inp, <span class="hljs-type">const</span> <span class="hljs-type">float</span>* weight, <span class="hljs-type">const</span> <span class="hljs-type">float</span>* bias,
                                                                   <span class="hljs-type">int</span> C, <span class="hljs-type">int</span> OC) {
    <span class="hljs-comment">// out is (B,T,OC). OC is short for "output channels", e.g. OC = 4 * C</span>
    <span class="hljs-comment">// inp is (B,T,C), weight is (OC, C), bias is (OC)</span>
    <span class="hljs-comment">// each thread handles 8x8 elements; each block 128 by 128 elements.</span>
    <span class="hljs-type">int</span> oc = <span class="hljs-number">8</span>*(blockIdx.y * blockDim.y + threadIdx.y);

    <span class="hljs-comment">// buffers to cache chunks of the input matrices</span>
    __shared__ <span class="hljs-type">float</span> lhs_s[<span class="hljs-number">128</span>][<span class="hljs-number">32</span>];
    __shared__ <span class="hljs-type">float</span> rhs_s[<span class="hljs-number">128</span>][<span class="hljs-number">32</span>];

    <span class="hljs-comment">// adjust our pointers for the current block</span>
    inp += <span class="hljs-number">128</span> * blockIdx.x * C;
    weight += <span class="hljs-number">128</span> * blockIdx.y * C;
    out += <span class="hljs-number">128</span> * blockIdx.x * OC + <span class="hljs-number">128</span> * blockIdx.y;

    <span class="hljs-type">float</span> vals[<span class="hljs-number">8</span>][<span class="hljs-number">8</span>] = {};
    <span class="hljs-keyword">if</span>(bias != <span class="hljs-literal">NULL</span>) {
        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &#x3C; <span class="hljs-number">8</span>; i++) {
            <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>; j &#x3C; <span class="hljs-number">8</span>; j += <span class="hljs-number">4</span>) {
                float4 b = ld_vec(bias + oc + j);
                vals[i][j+<span class="hljs-number">0</span>] = b.x;
                vals[i][j+<span class="hljs-number">1</span>] = b.y;
                vals[i][j+<span class="hljs-number">2</span>] = b.z;
                vals[i][j+<span class="hljs-number">3</span>] = b.w;
            }
        }
    }

    <span class="hljs-type">int</span> si_start = <span class="hljs-number">4</span>*(<span class="hljs-number">16</span> * threadIdx.y + threadIdx.x);
    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> so = <span class="hljs-number">0</span>; so &#x3C; C; so += <span class="hljs-number">32</span>) {
        __syncthreads();
        <span class="hljs-type">int</span> xmod8 = threadIdx.x % <span class="hljs-number">8</span>;
        <span class="hljs-type">int</span> xby8 = threadIdx.x / <span class="hljs-number">8</span>;
        <span class="hljs-type">int</span> xo = <span class="hljs-number">4</span> * xmod8;
        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> y = <span class="hljs-number">2</span> * threadIdx.y + xby8; y &#x3C; <span class="hljs-number">128</span>; y += <span class="hljs-number">32</span>) {
            st_vec(&#x26;lhs_s[y][xo], ld_vec(inp + y * C + so + xo));
            st_vec(&#x26;rhs_s[y][xo], ld_vec(weight + y * C + so + xo));
        }
        __syncthreads();

        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> si = si_start; si &#x3C; si_start + <span class="hljs-number">32</span>; si += <span class="hljs-number">4</span>) {
            float4 rhs[<span class="hljs-number">8</span>];
            <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> u = <span class="hljs-number">0</span>; u &#x3C; <span class="hljs-number">8</span>; ++u) {
                rhs[u] = ld_vec(&#x26;rhs_s[u + <span class="hljs-number">8</span> * threadIdx.y][si % <span class="hljs-number">32</span>]);
            }

            <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> ii = <span class="hljs-number">0</span>; ii &#x3C; <span class="hljs-number">8</span>; ++ii) {
                float4 lhs = ld_vec(&#x26;lhs_s[ii + <span class="hljs-number">8</span> * threadIdx.x][si % <span class="hljs-number">32</span>]);
                <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> ji = <span class="hljs-number">0</span>; ji &#x3C; <span class="hljs-number">8</span>; ++ji) {
                    vals[ii][ji] += lhs.x * rhs[ji].x;
                    vals[ii][ji] += lhs.y * rhs[ji].y;
                    vals[ii][ji] += lhs.z * rhs[ji].z;
                    vals[ii][ji] += lhs.w * rhs[ji].w;
                }
            }
        }
    }

    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &#x3C; <span class="hljs-number">8</span>; ++i) {
        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>; j &#x3C; <span class="hljs-number">8</span>; j += <span class="hljs-number">4</span>) {
            float4 result;
            result.x = vals[i][j + <span class="hljs-number">0</span>];
            result.y = vals[i][j + <span class="hljs-number">1</span>];
            result.z = vals[i][j + <span class="hljs-number">2</span>];
            result.w = vals[i][j + <span class="hljs-number">3</span>];
            st_vec(out + (<span class="hljs-number">8</span>*threadIdx.x+i) * OC + <span class="hljs-number">8</span>*threadIdx.y + j, result);
        }
    }
}
</code></pre>
<p><a href="https://github.com/karpathy/llm.c/blob/1dafa60ad972ae43d70080e2e9497c60ea31fe42/train_gpt2_fp32.cu#L617-L687">https://github.com/karpathy/llm.c/blob/1dafa60ad972ae43d70080e2e9497c60ea31fe42/train_gpt2_fp32.cu#L617-L687</a></p>
<p>对于 shape 为 [N, C] 和 [C, OC] 的矩阵乘法，每个 thread block 处理 [128, C] 和 [C, 128] 的矩阵乘法。计算 [128, C] * [C, 128] 的时候，每次将两个矩阵的一部分，即 [128, 32] 和 [32, 128] 加载到 shared memory 中，然后计算结果。这对应第一个 for loop</p>
<pre><code class="hljs language-c"><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> so = <span class="hljs-number">0</span>; so &#x3C; C; so += <span class="hljs-number">32</span>) {
    __syncthreads();
    <span class="hljs-type">int</span> xmod8 = threadIdx.x % <span class="hljs-number">8</span>;
    <span class="hljs-type">int</span> xby8 = threadIdx.x / <span class="hljs-number">8</span>;
    <span class="hljs-type">int</span> xo = <span class="hljs-number">4</span> * xmod8;
    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> y = <span class="hljs-number">2</span> * threadIdx.y + xby8; y &#x3C; <span class="hljs-number">128</span>; y += <span class="hljs-number">32</span>) {
        st_vec(&#x26;lhs_s[y][xo], ld_vec(inp + y * C + so + xo));
        st_vec(&#x26;rhs_s[y][xo], ld_vec(weight + y * C + so + xo));
    }
    __syncthreads();
    ...
}
</code></pre>
<p><img src="/llmdotc/image.png" alt="alt text"></p>
<p>每个 thread block 包含 16 * 16 个 thread，thread block 中的 thread 一起协作把 [128, 32] 的两个矩阵分别加载到 shared memory 中。128 * 32 / (16 * 16) = 16，所以每个 thread 负责加载 16 个元素。threadidx (0, 0) 负责加载的元素如下图所示</p>
<p><img src="/llmdotc/image-1.png" alt="alt text"></p>
<p>这里对应的代码如下</p>
<pre><code class="hljs language-c"><span class="hljs-type">int</span> xmod8 = threadIdx.x % <span class="hljs-number">8</span>;
<span class="hljs-type">int</span> xby8 = threadIdx.x / <span class="hljs-number">8</span>;
<span class="hljs-type">int</span> xo = <span class="hljs-number">4</span> * xmod8;
<span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> y = <span class="hljs-number">2</span> * threadIdx.y + xby8; y &#x3C; <span class="hljs-number">128</span>; y += <span class="hljs-number">32</span>) {
    st_vec(&#x26;lhs_s[y][xo], ld_vec(inp + y * C + so + xo));
    st_vec(&#x26;rhs_s[y][xo], ld_vec(weight + y * C + so + xo));
}
</code></pre>
<p>其中 <code>st_vec</code> 和 <code>ld_vec</code> 分别 store 和 load 4 个 float</p>
<pre><code class="hljs language-c">__device__ float4 <span class="hljs-title function_">ld_vec</span><span class="hljs-params">(<span class="hljs-type">const</span> <span class="hljs-type">float</span>* address)</span> {
    <span class="hljs-keyword">return</span> *reinterpret_cast&#x3C;<span class="hljs-type">const</span> float4*>(address);
}

__device__ <span class="hljs-type">void</span> <span class="hljs-title function_">st_vec</span><span class="hljs-params">(<span class="hljs-type">float</span>* address, float4 val)</span> {
    *reinterpret_cast&#x3C;float4*>(address) = val;
}
</code></pre>
<p>一个 thread block 包含 16 * 16 个 thread，每个 thread 负责计算 [8, 32] * [32, 8] 的矩阵乘法。</p>
<p><img src="/llmdotc/image-2.png" alt="alt text"></p>
<p>每个 thread 在计算 [8, 32] * [32, 8] 的时候，分成了更小的块去计算，一次计算 [8, 4] * [4, 8] 的矩阵乘法。</p>
<p><img src="/llmdotc/image-3.png" alt="alt text"></p>
<p>对应如下两个 for loop</p>
<pre><code class="hljs language-c"><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> si = si_start; si &#x3C; si_start + <span class="hljs-number">32</span>; si += <span class="hljs-number">4</span>) {
    float4 rhs[<span class="hljs-number">8</span>];
    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> u = <span class="hljs-number">0</span>; u &#x3C; <span class="hljs-number">8</span>; ++u) {
        rhs[u] = ld_vec(&#x26;rhs_s[u + <span class="hljs-number">8</span> * threadIdx.y][si % <span class="hljs-number">32</span>]);
    }

    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> ii = <span class="hljs-number">0</span>; ii &#x3C; <span class="hljs-number">8</span>; ++ii) {
        float4 lhs = ld_vec(&#x26;lhs_s[ii + <span class="hljs-number">8</span> * threadIdx.x][si % <span class="hljs-number">32</span>]);
        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> ji = <span class="hljs-number">0</span>; ji &#x3C; <span class="hljs-number">8</span>; ++ji) {
            vals[ii][ji] += lhs.x * rhs[ji].x;
            vals[ii][ji] += lhs.y * rhs[ji].y;
            vals[ii][ji] += lhs.z * rhs[ji].z;
            vals[ii][ji] += lhs.w * rhs[ji].w;
        }
    }
}
</code></pre></div></article></main><div class="mt-12"><a href="/">← 返回首页</a></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"postData":{"id":"kernel_in_llmdotc","contentHtml":"\u003ch2\u003eMatmul\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-c\"\u003e__global__ \u003cspan class=\"hljs-type\"\u003evoid\u003c/span\u003e __launch_bounds__(\u003cspan class=\"hljs-number\"\u003e16\u003c/span\u003e*\u003cspan class=\"hljs-number\"\u003e16\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e) matmul_forward_kernel4(\u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e* out,\n                                                                   \u003cspan class=\"hljs-type\"\u003econst\u003c/span\u003e \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e* inp, \u003cspan class=\"hljs-type\"\u003econst\u003c/span\u003e \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e* weight, \u003cspan class=\"hljs-type\"\u003econst\u003c/span\u003e \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e* bias,\n                                                                   \u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e C, \u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e OC) {\n    \u003cspan class=\"hljs-comment\"\u003e// out is (B,T,OC). OC is short for \"output channels\", e.g. OC = 4 * C\u003c/span\u003e\n    \u003cspan class=\"hljs-comment\"\u003e// inp is (B,T,C), weight is (OC, C), bias is (OC)\u003c/span\u003e\n    \u003cspan class=\"hljs-comment\"\u003e// each thread handles 8x8 elements; each block 128 by 128 elements.\u003c/span\u003e\n    \u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e oc = \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e*(blockIdx.y * blockDim.y + threadIdx.y);\n\n    \u003cspan class=\"hljs-comment\"\u003e// buffers to cache chunks of the input matrices\u003c/span\u003e\n    __shared__ \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e lhs_s[\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e][\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e];\n    __shared__ \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e rhs_s[\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e][\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e];\n\n    \u003cspan class=\"hljs-comment\"\u003e// adjust our pointers for the current block\u003c/span\u003e\n    inp += \u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e * blockIdx.x * C;\n    weight += \u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e * blockIdx.y * C;\n    out += \u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e * blockIdx.x * OC + \u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e * blockIdx.y;\n\n    \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e vals[\u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e][\u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e] = {};\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e(bias != \u003cspan class=\"hljs-literal\"\u003eNULL\u003c/span\u003e) {\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e i = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; i \u0026#x3C; \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e; i++) {\n            \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e j = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; j \u0026#x3C; \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e; j += \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e) {\n                float4 b = ld_vec(bias + oc + j);\n                vals[i][j+\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e] = b.x;\n                vals[i][j+\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e] = b.y;\n                vals[i][j+\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e] = b.z;\n                vals[i][j+\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e] = b.w;\n            }\n        }\n    }\n\n    \u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e si_start = \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e*(\u003cspan class=\"hljs-number\"\u003e16\u003c/span\u003e * threadIdx.y + threadIdx.x);\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e so = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; so \u0026#x3C; C; so += \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e) {\n        __syncthreads();\n        \u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e xmod8 = threadIdx.x % \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e;\n        \u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e xby8 = threadIdx.x / \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e;\n        \u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e xo = \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e * xmod8;\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e(\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e y = \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e * threadIdx.y + xby8; y \u0026#x3C; \u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e; y += \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e) {\n            st_vec(\u0026#x26;lhs_s[y][xo], ld_vec(inp + y * C + so + xo));\n            st_vec(\u0026#x26;rhs_s[y][xo], ld_vec(weight + y * C + so + xo));\n        }\n        __syncthreads();\n\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e si = si_start; si \u0026#x3C; si_start + \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e; si += \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e) {\n            float4 rhs[\u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e];\n            \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e u = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; u \u0026#x3C; \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e; ++u) {\n                rhs[u] = ld_vec(\u0026#x26;rhs_s[u + \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e * threadIdx.y][si % \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e]);\n            }\n\n            \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e ii = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; ii \u0026#x3C; \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e; ++ii) {\n                float4 lhs = ld_vec(\u0026#x26;lhs_s[ii + \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e * threadIdx.x][si % \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e]);\n                \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e ji = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; ji \u0026#x3C; \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e; ++ji) {\n                    vals[ii][ji] += lhs.x * rhs[ji].x;\n                    vals[ii][ji] += lhs.y * rhs[ji].y;\n                    vals[ii][ji] += lhs.z * rhs[ji].z;\n                    vals[ii][ji] += lhs.w * rhs[ji].w;\n                }\n            }\n        }\n    }\n\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e i = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; i \u0026#x3C; \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e; ++i) {\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e j = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; j \u0026#x3C; \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e; j += \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e) {\n            float4 result;\n            result.x = vals[i][j + \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e];\n            result.y = vals[i][j + \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e];\n            result.z = vals[i][j + \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e];\n            result.w = vals[i][j + \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e];\n            st_vec(out + (\u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e*threadIdx.x+i) * OC + \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e*threadIdx.y + j, result);\n        }\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/karpathy/llm.c/blob/1dafa60ad972ae43d70080e2e9497c60ea31fe42/train_gpt2_fp32.cu#L617-L687\"\u003ehttps://github.com/karpathy/llm.c/blob/1dafa60ad972ae43d70080e2e9497c60ea31fe42/train_gpt2_fp32.cu#L617-L687\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e对于 shape 为 [N, C] 和 [C, OC] 的矩阵乘法，每个 thread block 处理 [128, C] 和 [C, 128] 的矩阵乘法。计算 [128, C] * [C, 128] 的时候，每次将两个矩阵的一部分，即 [128, 32] 和 [32, 128] 加载到 shared memory 中，然后计算结果。这对应第一个 for loop\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-c\"\u003e\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e so = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; so \u0026#x3C; C; so += \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e) {\n    __syncthreads();\n    \u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e xmod8 = threadIdx.x % \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e;\n    \u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e xby8 = threadIdx.x / \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e;\n    \u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e xo = \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e * xmod8;\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e(\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e y = \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e * threadIdx.y + xby8; y \u0026#x3C; \u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e; y += \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e) {\n        st_vec(\u0026#x26;lhs_s[y][xo], ld_vec(inp + y * C + so + xo));\n        st_vec(\u0026#x26;rhs_s[y][xo], ld_vec(weight + y * C + so + xo));\n    }\n    __syncthreads();\n    ...\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/llmdotc/image.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cp\u003e每个 thread block 包含 16 * 16 个 thread，thread block 中的 thread 一起协作把 [128, 32] 的两个矩阵分别加载到 shared memory 中。128 * 32 / (16 * 16) = 16，所以每个 thread 负责加载 16 个元素。threadidx (0, 0) 负责加载的元素如下图所示\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/llmdotc/image-1.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cp\u003e这里对应的代码如下\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-c\"\u003e\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e xmod8 = threadIdx.x % \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e;\n\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e xby8 = threadIdx.x / \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e;\n\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e xo = \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e * xmod8;\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e(\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e y = \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e * threadIdx.y + xby8; y \u0026#x3C; \u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e; y += \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e) {\n    st_vec(\u0026#x26;lhs_s[y][xo], ld_vec(inp + y * C + so + xo));\n    st_vec(\u0026#x26;rhs_s[y][xo], ld_vec(weight + y * C + so + xo));\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e其中 \u003ccode\u003est_vec\u003c/code\u003e 和 \u003ccode\u003eld_vec\u003c/code\u003e 分别 store 和 load 4 个 float\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-c\"\u003e__device__ float4 \u003cspan class=\"hljs-title function_\"\u003eld_vec\u003c/span\u003e\u003cspan class=\"hljs-params\"\u003e(\u003cspan class=\"hljs-type\"\u003econst\u003c/span\u003e \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e* address)\u003c/span\u003e {\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e *reinterpret_cast\u0026#x3C;\u003cspan class=\"hljs-type\"\u003econst\u003c/span\u003e float4*\u003e(address);\n}\n\n__device__ \u003cspan class=\"hljs-type\"\u003evoid\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003est_vec\u003c/span\u003e\u003cspan class=\"hljs-params\"\u003e(\u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e* address, float4 val)\u003c/span\u003e {\n    *reinterpret_cast\u0026#x3C;float4*\u003e(address) = val;\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e一个 thread block 包含 16 * 16 个 thread，每个 thread 负责计算 [8, 32] * [32, 8] 的矩阵乘法。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/llmdotc/image-2.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cp\u003e每个 thread 在计算 [8, 32] * [32, 8] 的时候，分成了更小的块去计算，一次计算 [8, 4] * [4, 8] 的矩阵乘法。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/llmdotc/image-3.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cp\u003e对应如下两个 for loop\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-c\"\u003e\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e si = si_start; si \u0026#x3C; si_start + \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e; si += \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e) {\n    float4 rhs[\u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e];\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e u = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; u \u0026#x3C; \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e; ++u) {\n        rhs[u] = ld_vec(\u0026#x26;rhs_s[u + \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e * threadIdx.y][si % \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e]);\n    }\n\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e ii = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; ii \u0026#x3C; \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e; ++ii) {\n        float4 lhs = ld_vec(\u0026#x26;lhs_s[ii + \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e * threadIdx.x][si % \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e]);\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e ji = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; ji \u0026#x3C; \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e; ++ji) {\n            vals[ii][ji] += lhs.x * rhs[ji].x;\n            vals[ii][ji] += lhs.y * rhs[ji].y;\n            vals[ii][ji] += lhs.z * rhs[ji].z;\n            vals[ii][ji] += lhs.w * rhs[ji].w;\n        }\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e","title":"LLM.C 中的 CUDA Kernel","date":"2024-07-17"}},"__N_SSG":true},"page":"/posts/[id]","query":{"id":"kernel_in_llmdotc"},"buildId":"wTPBy1ZzrXj1NkuaydzLJ","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>