<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/github.min.css"/><title>LLM.C 中的 CUDA Kernel</title><meta name="next-head-count" content="4"/><link rel="preload" href="/_next/static/css/ab65974685f462b8.css" as="style"/><link rel="stylesheet" href="/_next/static/css/ab65974685f462b8.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-6cbe6e332df95288.js" defer=""></script><script src="/_next/static/chunks/main-26f9f36b33181737.js" defer=""></script><script src="/_next/static/chunks/pages/_app-2d6bf7b3192a8752.js" defer=""></script><script src="/_next/static/chunks/73-96e6cbd54826b874.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bid%5D-4b57e02c440700a5.js" defer=""></script><script src="/_next/static/B6bfnZBrc4hF-4DFzBgHE/_buildManifest.js" defer=""></script><script src="/_next/static/B6bfnZBrc4hF-4DFzBgHE/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="max-w-4xl px-4 mt-12 mb-24 mx-auto"><main><article><h1 class="text-3xl font-medium my-4 border-b-0">LLM.C 中的 CUDA Kernel</h1><div class="text-gray-500 mb-8 pb-2 border-b-2 border-solid border-slate-300"><time dateTime="2024-07-17">July 17, 2024</time></div><div><h2>Matmul</h2>
<pre><code class="hljs language-c">__global__ <span class="hljs-type">void</span> __launch_bounds__(<span class="hljs-number">16</span>*<span class="hljs-number">16</span>, <span class="hljs-number">2</span>) matmul_forward_kernel4(<span class="hljs-type">float</span>* out,
                                                                   <span class="hljs-type">const</span> <span class="hljs-type">float</span>* inp, <span class="hljs-type">const</span> <span class="hljs-type">float</span>* weight, <span class="hljs-type">const</span> <span class="hljs-type">float</span>* bias,
                                                                   <span class="hljs-type">int</span> C, <span class="hljs-type">int</span> OC) {
    <span class="hljs-comment">// out is (B,T,OC). OC is short for "output channels", e.g. OC = 4 * C</span>
    <span class="hljs-comment">// inp is (B,T,C), weight is (OC, C), bias is (OC)</span>
    <span class="hljs-comment">// each thread handles 8x8 elements; each block 128 by 128 elements.</span>
    <span class="hljs-type">int</span> oc = <span class="hljs-number">8</span>*(blockIdx.y * blockDim.y + threadIdx.y);

    <span class="hljs-comment">// buffers to cache chunks of the input matrices</span>
    __shared__ <span class="hljs-type">float</span> lhs_s[<span class="hljs-number">128</span>][<span class="hljs-number">32</span>];
    __shared__ <span class="hljs-type">float</span> rhs_s[<span class="hljs-number">128</span>][<span class="hljs-number">32</span>];

    <span class="hljs-comment">// adjust our pointers for the current block</span>
    inp += <span class="hljs-number">128</span> * blockIdx.x * C;
    weight += <span class="hljs-number">128</span> * blockIdx.y * C;
    out += <span class="hljs-number">128</span> * blockIdx.x * OC + <span class="hljs-number">128</span> * blockIdx.y;

    <span class="hljs-type">float</span> vals[<span class="hljs-number">8</span>][<span class="hljs-number">8</span>] = {};
    <span class="hljs-keyword">if</span>(bias != <span class="hljs-literal">NULL</span>) {
        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &#x3C; <span class="hljs-number">8</span>; i++) {
            <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>; j &#x3C; <span class="hljs-number">8</span>; j += <span class="hljs-number">4</span>) {
                float4 b = ld_vec(bias + oc + j);
                vals[i][j+<span class="hljs-number">0</span>] = b.x;
                vals[i][j+<span class="hljs-number">1</span>] = b.y;
                vals[i][j+<span class="hljs-number">2</span>] = b.z;
                vals[i][j+<span class="hljs-number">3</span>] = b.w;
            }
        }
    }

    <span class="hljs-type">int</span> si_start = <span class="hljs-number">4</span>*(<span class="hljs-number">16</span> * threadIdx.y + threadIdx.x);
    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> so = <span class="hljs-number">0</span>; so &#x3C; C; so += <span class="hljs-number">32</span>) {
        __syncthreads();
        <span class="hljs-type">int</span> xmod8 = threadIdx.x % <span class="hljs-number">8</span>;
        <span class="hljs-type">int</span> xby8 = threadIdx.x / <span class="hljs-number">8</span>;
        <span class="hljs-type">int</span> xo = <span class="hljs-number">4</span> * xmod8;
        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> y = <span class="hljs-number">2</span> * threadIdx.y + xby8; y &#x3C; <span class="hljs-number">128</span>; y += <span class="hljs-number">32</span>) {
            st_vec(&#x26;lhs_s[y][xo], ld_vec(inp + y * C + so + xo));
            st_vec(&#x26;rhs_s[y][xo], ld_vec(weight + y * C + so + xo));
        }
        __syncthreads();

        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> si = si_start; si &#x3C; si_start + <span class="hljs-number">32</span>; si += <span class="hljs-number">4</span>) {
            float4 rhs[<span class="hljs-number">8</span>];
            <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> u = <span class="hljs-number">0</span>; u &#x3C; <span class="hljs-number">8</span>; ++u) {
                rhs[u] = ld_vec(&#x26;rhs_s[u + <span class="hljs-number">8</span> * threadIdx.y][si % <span class="hljs-number">32</span>]);
            }

            <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> ii = <span class="hljs-number">0</span>; ii &#x3C; <span class="hljs-number">8</span>; ++ii) {
                float4 lhs = ld_vec(&#x26;lhs_s[ii + <span class="hljs-number">8</span> * threadIdx.x][si % <span class="hljs-number">32</span>]);
                <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> ji = <span class="hljs-number">0</span>; ji &#x3C; <span class="hljs-number">8</span>; ++ji) {
                    vals[ii][ji] += lhs.x * rhs[ji].x;
                    vals[ii][ji] += lhs.y * rhs[ji].y;
                    vals[ii][ji] += lhs.z * rhs[ji].z;
                    vals[ii][ji] += lhs.w * rhs[ji].w;
                }
            }
        }
    }

    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &#x3C; <span class="hljs-number">8</span>; ++i) {
        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>; j &#x3C; <span class="hljs-number">8</span>; j += <span class="hljs-number">4</span>) {
            float4 result;
            result.x = vals[i][j + <span class="hljs-number">0</span>];
            result.y = vals[i][j + <span class="hljs-number">1</span>];
            result.z = vals[i][j + <span class="hljs-number">2</span>];
            result.w = vals[i][j + <span class="hljs-number">3</span>];
            st_vec(out + (<span class="hljs-number">8</span>*threadIdx.x+i) * OC + <span class="hljs-number">8</span>*threadIdx.y + j, result);
        }
    }
}
</code></pre>
<p><a href="https://github.com/karpathy/llm.c/blob/1dafa60ad972ae43d70080e2e9497c60ea31fe42/train_gpt2_fp32.cu#L617-L687">https://github.com/karpathy/llm.c/blob/1dafa60ad972ae43d70080e2e9497c60ea31fe42/train_gpt2_fp32.cu#L617-L687</a></p>
<p>对于 shape 为 [N, C] 和 [C, OC] 的矩阵乘法，每个 thread block 处理 [128, C] 和 [C, 128] 的矩阵乘法。计算 [128, C] * [C, 128] 的时候，每次将两个矩阵的一部分，即 [128, 32] 和 [32, 128] 加载到 shared memory 中，然后计算结果。这对应第一个 for loop</p>
<pre><code class="hljs language-c"><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> so = <span class="hljs-number">0</span>; so &#x3C; C; so += <span class="hljs-number">32</span>) {
    __syncthreads();
    <span class="hljs-type">int</span> xmod8 = threadIdx.x % <span class="hljs-number">8</span>;
    <span class="hljs-type">int</span> xby8 = threadIdx.x / <span class="hljs-number">8</span>;
    <span class="hljs-type">int</span> xo = <span class="hljs-number">4</span> * xmod8;
    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> y = <span class="hljs-number">2</span> * threadIdx.y + xby8; y &#x3C; <span class="hljs-number">128</span>; y += <span class="hljs-number">32</span>) {
        st_vec(&#x26;lhs_s[y][xo], ld_vec(inp + y * C + so + xo));
        st_vec(&#x26;rhs_s[y][xo], ld_vec(weight + y * C + so + xo));
    }
    __syncthreads();
    ...
}
</code></pre>
<p><img src="/llmdotc/image.png" alt="alt text"></p>
<p>每个 thread block 包含 16 * 16 个 thread，thread block 中的 thread 一起协作把 [128, 32] 的两个矩阵分别加载到 shared memory 中。128 * 32 / (16 * 16) = 16，所以每个 thread 负责加载 16 个元素。threadidx (0, 0) 负责加载的元素如下图所示</p>
<p><img src="/llmdotc/image-1.png" alt="alt text"></p>
<p>这里对应的代码如下</p>
<pre><code class="hljs language-c"><span class="hljs-type">int</span> xmod8 = threadIdx.x % <span class="hljs-number">8</span>;
<span class="hljs-type">int</span> xby8 = threadIdx.x / <span class="hljs-number">8</span>;
<span class="hljs-type">int</span> xo = <span class="hljs-number">4</span> * xmod8;
<span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> y = <span class="hljs-number">2</span> * threadIdx.y + xby8; y &#x3C; <span class="hljs-number">128</span>; y += <span class="hljs-number">32</span>) {
    st_vec(&#x26;lhs_s[y][xo], ld_vec(inp + y * C + so + xo));
    st_vec(&#x26;rhs_s[y][xo], ld_vec(weight + y * C + so + xo));
}
</code></pre>
<p>其中 <code>st_vec</code> 和 <code>ld_vec</code> 分别 store 和 load 4 个 float</p>
<pre><code class="hljs language-c">__device__ float4 <span class="hljs-title function_">ld_vec</span><span class="hljs-params">(<span class="hljs-type">const</span> <span class="hljs-type">float</span>* address)</span> {
    <span class="hljs-keyword">return</span> *reinterpret_cast&#x3C;<span class="hljs-type">const</span> float4*>(address);
}

__device__ <span class="hljs-type">void</span> <span class="hljs-title function_">st_vec</span><span class="hljs-params">(<span class="hljs-type">float</span>* address, float4 val)</span> {
    *reinterpret_cast&#x3C;float4*>(address) = val;
}
</code></pre>
<p>一个 thread block 包含 16 * 16 个 thread，每个 thread 负责计算 [8, 32] * [32, 8] 的矩阵乘法。</p>
<p><img src="/llmdotc/image-2.png" alt="alt text"></p>
<p>每个 thread 在计算 [8, 32] * [32, 8] 的时候，分成了更小的块去计算，一次计算 [8, 4] * [4, 8] 的矩阵乘法。</p>
<p><img src="/llmdotc/image-3.png" alt="alt text"></p>
<p>对应如下两个 for loop</p>
<pre><code class="hljs language-c"><span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> si = si_start; si &#x3C; si_start + <span class="hljs-number">32</span>; si += <span class="hljs-number">4</span>) {
    float4 rhs[<span class="hljs-number">8</span>];
    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> u = <span class="hljs-number">0</span>; u &#x3C; <span class="hljs-number">8</span>; ++u) {
        rhs[u] = ld_vec(&#x26;rhs_s[u + <span class="hljs-number">8</span> * threadIdx.y][si % <span class="hljs-number">32</span>]);
    }

    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> ii = <span class="hljs-number">0</span>; ii &#x3C; <span class="hljs-number">8</span>; ++ii) {
        float4 lhs = ld_vec(&#x26;lhs_s[ii + <span class="hljs-number">8</span> * threadIdx.x][si % <span class="hljs-number">32</span>]);
        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> ji = <span class="hljs-number">0</span>; ji &#x3C; <span class="hljs-number">8</span>; ++ji) {
            vals[ii][ji] += lhs.x * rhs[ji].x;
            vals[ii][ji] += lhs.y * rhs[ji].y;
            vals[ii][ji] += lhs.z * rhs[ji].z;
            vals[ii][ji] += lhs.w * rhs[ji].w;
        }
    }
}
</code></pre>
<h2>LayerNorm</h2>
<pre><code class="hljs language-c">__global__ <span class="hljs-type">void</span> <span class="hljs-title function_">layernorm_forward_kernel3</span><span class="hljs-params">(<span class="hljs-type">float</span>* __restrict__ out, <span class="hljs-type">float</span>* __restrict__ mean, <span class="hljs-type">float</span>* __restrict__ rstd,
                                    <span class="hljs-type">const</span> <span class="hljs-type">float</span>*  __restrict__ inp, <span class="hljs-type">const</span> <span class="hljs-type">float</span>*  __restrict__ weight,
                                    <span class="hljs-type">const</span> <span class="hljs-type">float</span>* __restrict__ bias, <span class="hljs-type">int</span> N, <span class="hljs-type">int</span> C)</span> {
    cg::thread_block block = cg::this_thread_block();
    cg::thread_block_tile&#x3C;<span class="hljs-number">32</span>> warp = cg::tiled_partition&#x3C;<span class="hljs-number">32</span>>(block);
    <span class="hljs-type">int</span> idx = blockIdx.x * warp.meta_group_size() + warp.meta_group_rank();
    <span class="hljs-keyword">if</span>(idx >= N) {
        <span class="hljs-keyword">return</span>;
    }

    <span class="hljs-comment">// the row of input that this group of threads is responsible for</span>
    <span class="hljs-type">const</span> <span class="hljs-type">float</span>* x = inp + idx * C;

    <span class="hljs-comment">// mean</span>
    <span class="hljs-type">float</span> sum = <span class="hljs-number">0.0f</span>;
    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = warp.thread_rank(); i &#x3C; C; i += warp.size()) {
        sum += x[i];
    }
    sum = cg::reduce(warp, sum, cg::plus&#x3C;<span class="hljs-type">float</span>>{});
    <span class="hljs-type">float</span> m = sum / C;
    <span class="hljs-keyword">if</span>(warp.thread_rank() == <span class="hljs-number">0</span> &#x26;&#x26; mean != nullptr) {
        __stcs(mean + idx, m);
    }

    <span class="hljs-comment">// rstd</span>
    sum = <span class="hljs-number">0.0f</span>;
    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = warp.thread_rank(); i &#x3C; C; i += warp.size()) {
        <span class="hljs-type">float</span> diff = x[i] - m;
        sum += diff * diff;
    }
    sum = cg::reduce(warp, sum, cg::plus&#x3C;<span class="hljs-type">float</span>>{});
    <span class="hljs-type">float</span> s = rsqrtf(sum / C + <span class="hljs-number">1e-5</span>f);
    <span class="hljs-keyword">if</span>(warp.thread_rank() == <span class="hljs-number">0</span> &#x26;&#x26; rstd != nullptr) {
        __stcs(rstd + idx, s);
    }

    <span class="hljs-comment">// final normalization and scaling by weight/bias</span>
    <span class="hljs-type">float</span>* o = out + idx * C;
    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> c = warp.thread_rank(); c &#x3C; C; c += warp.size()) {
        <span class="hljs-comment">// load and store using the .cs "streaming" hint to the compiler,</span>
        <span class="hljs-comment">// indicating that this data will not be reused soon, and can be streamed through the caches</span>
        <span class="hljs-comment">// this allows the threads to get more cache-hits for the (shared) weight and bias parameters</span>
        <span class="hljs-type">float</span> n = s * (__ldcs(x+c) - m);
        __stcs(o+c, n * weight[c] + bias[c]);
    }
}

<span class="hljs-type">void</span> <span class="hljs-title function_">layernorm_forward</span><span class="hljs-params">(<span class="hljs-type">float</span>* out, <span class="hljs-type">float</span>* mean, <span class="hljs-type">float</span>* rstd,
                       <span class="hljs-type">float</span>* inp, <span class="hljs-type">float</span>* weight, <span class="hljs-type">float</span>* bias,
                       <span class="hljs-type">int</span> B, <span class="hljs-type">int</span> T, <span class="hljs-type">int</span> C)</span> {
    <span class="hljs-type">const</span> <span class="hljs-type">int</span> block_size = <span class="hljs-number">128</span>;
    <span class="hljs-type">const</span> <span class="hljs-type">int</span> N = B * T;
    <span class="hljs-type">const</span> <span class="hljs-type">int</span> grid_size = CEIL_DIV(N * <span class="hljs-number">32</span>, block_size);
    layernorm_forward_kernel3&#x3C;&#x3C;&#x3C;grid_size, block_size>>>(out, mean, rstd, inp, weight, bias, N, C);
    cudaCheck(cudaGetLastError());
}
</code></pre>
<p><a href="https://github.com/karpathy/llm.c/blob/1dafa60ad972ae43d70080e2e9497c60ea31fe42/train_gpt2_fp32.cu#L116-L161">https://github.com/karpathy/llm.c/blob/1dafa60ad972ae43d70080e2e9497c60ea31fe42/train_gpt2_fp32.cu#L116-L161</a></p>
<p>这个 kernel 需要关注的点是对于 <code>cooperative_groups</code> 这个库的运用，比如下面这段代码</p>
<pre><code class="hljs language-c">cg::thread_block block = cg::this_thread_block();
cg::thread_block_tile&#x3C;<span class="hljs-number">32</span>> warp = cg::tiled_partition&#x3C;<span class="hljs-number">32</span>>(block);
<span class="hljs-type">int</span> idx = blockIdx.x * warp.meta_group_size() + warp.meta_group_rank();
<span class="hljs-keyword">if</span>(idx >= N) {
    <span class="hljs-keyword">return</span>;
}
</code></pre>
<p>这里实现了一个 warp 负责一行的计算，如果用 <code>blockIdx</code>, <code>blockDim</code>, <code>threadIdx</code> 来实现如上的功能，代码差不多像下面这样</p>
<pre><code class="hljs language-c"><span class="hljs-type">int</span> warp_size = <span class="hljs-number">32</span>; 
<span class="hljs-type">int</span> idx = (blockIdx.x * blockDim.x + threadIdx.x) / warp_size;
<span class="hljs-keyword">if</span>(idx >= N) {
    <span class="hljs-keyword">return</span>;
}
</code></pre>
<p>而随后 <code>warp.thread_rank()</code> 可以替代 <code>threadIdx.x % warp_size</code>，整体来说感觉使用 <code>cooperative_groups</code> 可以让代码更加清晰，直观。</p>
<p><code>cooperative_groups</code> 的另一个核心功能是支持更细粒度的 thread group 的 sync，传统使用 <code>__syncthreads()</code> 的时候，整个 block 的所有 thread 都会被阻塞。在我们的这段代码中，<code>cg::thread_block_tile&#x3C;32> warp = cg::tiled_partition&#x3C;32>(block)</code> 把 32 个 thread 分成一个 thread group （warp），在 warp level 上进行 sync，<code>sum = cg::reduce(warp, sum, cg::plus&#x3C;float>{})</code>。</p>
<p><a href="https://developer.nvidia.com/blog/cooperative-groups/">Cooperative Groups: Flexible CUDA Thread Programming</a> 这篇文章对 <code>cooperative_groups</code> 有详细的介绍，强烈推荐阅读。</p></div></article></main><div class="mt-12"><a href="/">← 返回首页</a></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"postData":{"id":"kernel_in_llmdotc","contentHtml":"\u003ch2\u003eMatmul\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-c\"\u003e__global__ \u003cspan class=\"hljs-type\"\u003evoid\u003c/span\u003e __launch_bounds__(\u003cspan class=\"hljs-number\"\u003e16\u003c/span\u003e*\u003cspan class=\"hljs-number\"\u003e16\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e) matmul_forward_kernel4(\u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e* out,\n                                                                   \u003cspan class=\"hljs-type\"\u003econst\u003c/span\u003e \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e* inp, \u003cspan class=\"hljs-type\"\u003econst\u003c/span\u003e \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e* weight, \u003cspan class=\"hljs-type\"\u003econst\u003c/span\u003e \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e* bias,\n                                                                   \u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e C, \u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e OC) {\n    \u003cspan class=\"hljs-comment\"\u003e// out is (B,T,OC). OC is short for \"output channels\", e.g. OC = 4 * C\u003c/span\u003e\n    \u003cspan class=\"hljs-comment\"\u003e// inp is (B,T,C), weight is (OC, C), bias is (OC)\u003c/span\u003e\n    \u003cspan class=\"hljs-comment\"\u003e// each thread handles 8x8 elements; each block 128 by 128 elements.\u003c/span\u003e\n    \u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e oc = \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e*(blockIdx.y * blockDim.y + threadIdx.y);\n\n    \u003cspan class=\"hljs-comment\"\u003e// buffers to cache chunks of the input matrices\u003c/span\u003e\n    __shared__ \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e lhs_s[\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e][\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e];\n    __shared__ \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e rhs_s[\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e][\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e];\n\n    \u003cspan class=\"hljs-comment\"\u003e// adjust our pointers for the current block\u003c/span\u003e\n    inp += \u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e * blockIdx.x * C;\n    weight += \u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e * blockIdx.y * C;\n    out += \u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e * blockIdx.x * OC + \u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e * blockIdx.y;\n\n    \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e vals[\u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e][\u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e] = {};\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e(bias != \u003cspan class=\"hljs-literal\"\u003eNULL\u003c/span\u003e) {\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e i = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; i \u0026#x3C; \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e; i++) {\n            \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e j = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; j \u0026#x3C; \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e; j += \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e) {\n                float4 b = ld_vec(bias + oc + j);\n                vals[i][j+\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e] = b.x;\n                vals[i][j+\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e] = b.y;\n                vals[i][j+\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e] = b.z;\n                vals[i][j+\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e] = b.w;\n            }\n        }\n    }\n\n    \u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e si_start = \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e*(\u003cspan class=\"hljs-number\"\u003e16\u003c/span\u003e * threadIdx.y + threadIdx.x);\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e so = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; so \u0026#x3C; C; so += \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e) {\n        __syncthreads();\n        \u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e xmod8 = threadIdx.x % \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e;\n        \u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e xby8 = threadIdx.x / \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e;\n        \u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e xo = \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e * xmod8;\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e(\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e y = \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e * threadIdx.y + xby8; y \u0026#x3C; \u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e; y += \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e) {\n            st_vec(\u0026#x26;lhs_s[y][xo], ld_vec(inp + y * C + so + xo));\n            st_vec(\u0026#x26;rhs_s[y][xo], ld_vec(weight + y * C + so + xo));\n        }\n        __syncthreads();\n\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e si = si_start; si \u0026#x3C; si_start + \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e; si += \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e) {\n            float4 rhs[\u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e];\n            \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e u = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; u \u0026#x3C; \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e; ++u) {\n                rhs[u] = ld_vec(\u0026#x26;rhs_s[u + \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e * threadIdx.y][si % \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e]);\n            }\n\n            \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e ii = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; ii \u0026#x3C; \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e; ++ii) {\n                float4 lhs = ld_vec(\u0026#x26;lhs_s[ii + \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e * threadIdx.x][si % \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e]);\n                \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e ji = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; ji \u0026#x3C; \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e; ++ji) {\n                    vals[ii][ji] += lhs.x * rhs[ji].x;\n                    vals[ii][ji] += lhs.y * rhs[ji].y;\n                    vals[ii][ji] += lhs.z * rhs[ji].z;\n                    vals[ii][ji] += lhs.w * rhs[ji].w;\n                }\n            }\n        }\n    }\n\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e i = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; i \u0026#x3C; \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e; ++i) {\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e j = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; j \u0026#x3C; \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e; j += \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e) {\n            float4 result;\n            result.x = vals[i][j + \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e];\n            result.y = vals[i][j + \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e];\n            result.z = vals[i][j + \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e];\n            result.w = vals[i][j + \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e];\n            st_vec(out + (\u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e*threadIdx.x+i) * OC + \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e*threadIdx.y + j, result);\n        }\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/karpathy/llm.c/blob/1dafa60ad972ae43d70080e2e9497c60ea31fe42/train_gpt2_fp32.cu#L617-L687\"\u003ehttps://github.com/karpathy/llm.c/blob/1dafa60ad972ae43d70080e2e9497c60ea31fe42/train_gpt2_fp32.cu#L617-L687\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e对于 shape 为 [N, C] 和 [C, OC] 的矩阵乘法，每个 thread block 处理 [128, C] 和 [C, 128] 的矩阵乘法。计算 [128, C] * [C, 128] 的时候，每次将两个矩阵的一部分，即 [128, 32] 和 [32, 128] 加载到 shared memory 中，然后计算结果。这对应第一个 for loop\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-c\"\u003e\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e so = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; so \u0026#x3C; C; so += \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e) {\n    __syncthreads();\n    \u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e xmod8 = threadIdx.x % \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e;\n    \u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e xby8 = threadIdx.x / \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e;\n    \u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e xo = \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e * xmod8;\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e(\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e y = \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e * threadIdx.y + xby8; y \u0026#x3C; \u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e; y += \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e) {\n        st_vec(\u0026#x26;lhs_s[y][xo], ld_vec(inp + y * C + so + xo));\n        st_vec(\u0026#x26;rhs_s[y][xo], ld_vec(weight + y * C + so + xo));\n    }\n    __syncthreads();\n    ...\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"/llmdotc/image.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cp\u003e每个 thread block 包含 16 * 16 个 thread，thread block 中的 thread 一起协作把 [128, 32] 的两个矩阵分别加载到 shared memory 中。128 * 32 / (16 * 16) = 16，所以每个 thread 负责加载 16 个元素。threadidx (0, 0) 负责加载的元素如下图所示\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/llmdotc/image-1.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cp\u003e这里对应的代码如下\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-c\"\u003e\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e xmod8 = threadIdx.x % \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e;\n\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e xby8 = threadIdx.x / \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e;\n\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e xo = \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e * xmod8;\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e(\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e y = \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e * threadIdx.y + xby8; y \u0026#x3C; \u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e; y += \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e) {\n    st_vec(\u0026#x26;lhs_s[y][xo], ld_vec(inp + y * C + so + xo));\n    st_vec(\u0026#x26;rhs_s[y][xo], ld_vec(weight + y * C + so + xo));\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e其中 \u003ccode\u003est_vec\u003c/code\u003e 和 \u003ccode\u003eld_vec\u003c/code\u003e 分别 store 和 load 4 个 float\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-c\"\u003e__device__ float4 \u003cspan class=\"hljs-title function_\"\u003eld_vec\u003c/span\u003e\u003cspan class=\"hljs-params\"\u003e(\u003cspan class=\"hljs-type\"\u003econst\u003c/span\u003e \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e* address)\u003c/span\u003e {\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e *reinterpret_cast\u0026#x3C;\u003cspan class=\"hljs-type\"\u003econst\u003c/span\u003e float4*\u003e(address);\n}\n\n__device__ \u003cspan class=\"hljs-type\"\u003evoid\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003est_vec\u003c/span\u003e\u003cspan class=\"hljs-params\"\u003e(\u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e* address, float4 val)\u003c/span\u003e {\n    *reinterpret_cast\u0026#x3C;float4*\u003e(address) = val;\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e一个 thread block 包含 16 * 16 个 thread，每个 thread 负责计算 [8, 32] * [32, 8] 的矩阵乘法。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/llmdotc/image-2.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cp\u003e每个 thread 在计算 [8, 32] * [32, 8] 的时候，分成了更小的块去计算，一次计算 [8, 4] * [4, 8] 的矩阵乘法。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/llmdotc/image-3.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cp\u003e对应如下两个 for loop\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-c\"\u003e\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e si = si_start; si \u0026#x3C; si_start + \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e; si += \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e) {\n    float4 rhs[\u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e];\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e u = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; u \u0026#x3C; \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e; ++u) {\n        rhs[u] = ld_vec(\u0026#x26;rhs_s[u + \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e * threadIdx.y][si % \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e]);\n    }\n\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e ii = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; ii \u0026#x3C; \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e; ++ii) {\n        float4 lhs = ld_vec(\u0026#x26;lhs_s[ii + \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e * threadIdx.x][si % \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e]);\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e ji = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e; ji \u0026#x3C; \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e; ++ji) {\n            vals[ii][ji] += lhs.x * rhs[ji].x;\n            vals[ii][ji] += lhs.y * rhs[ji].y;\n            vals[ii][ji] += lhs.z * rhs[ji].z;\n            vals[ii][ji] += lhs.w * rhs[ji].w;\n        }\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eLayerNorm\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-c\"\u003e__global__ \u003cspan class=\"hljs-type\"\u003evoid\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003elayernorm_forward_kernel3\u003c/span\u003e\u003cspan class=\"hljs-params\"\u003e(\u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e* __restrict__ out, \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e* __restrict__ mean, \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e* __restrict__ rstd,\n                                    \u003cspan class=\"hljs-type\"\u003econst\u003c/span\u003e \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e*  __restrict__ inp, \u003cspan class=\"hljs-type\"\u003econst\u003c/span\u003e \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e*  __restrict__ weight,\n                                    \u003cspan class=\"hljs-type\"\u003econst\u003c/span\u003e \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e* __restrict__ bias, \u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e N, \u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e C)\u003c/span\u003e {\n    cg::thread_block block = cg::this_thread_block();\n    cg::thread_block_tile\u0026#x3C;\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e\u003e warp = cg::tiled_partition\u0026#x3C;\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e\u003e(block);\n    \u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e idx = blockIdx.x * warp.meta_group_size() + warp.meta_group_rank();\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e(idx \u003e= N) {\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e;\n    }\n\n    \u003cspan class=\"hljs-comment\"\u003e// the row of input that this group of threads is responsible for\u003c/span\u003e\n    \u003cspan class=\"hljs-type\"\u003econst\u003c/span\u003e \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e* x = inp + idx * C;\n\n    \u003cspan class=\"hljs-comment\"\u003e// mean\u003c/span\u003e\n    \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e sum = \u003cspan class=\"hljs-number\"\u003e0.0f\u003c/span\u003e;\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e i = warp.thread_rank(); i \u0026#x3C; C; i += warp.size()) {\n        sum += x[i];\n    }\n    sum = cg::reduce(warp, sum, cg::plus\u0026#x3C;\u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e\u003e{});\n    \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e m = sum / C;\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e(warp.thread_rank() == \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e \u0026#x26;\u0026#x26; mean != nullptr) {\n        __stcs(mean + idx, m);\n    }\n\n    \u003cspan class=\"hljs-comment\"\u003e// rstd\u003c/span\u003e\n    sum = \u003cspan class=\"hljs-number\"\u003e0.0f\u003c/span\u003e;\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e i = warp.thread_rank(); i \u0026#x3C; C; i += warp.size()) {\n        \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e diff = x[i] - m;\n        sum += diff * diff;\n    }\n    sum = cg::reduce(warp, sum, cg::plus\u0026#x3C;\u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e\u003e{});\n    \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e s = rsqrtf(sum / C + \u003cspan class=\"hljs-number\"\u003e1e-5\u003c/span\u003ef);\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e(warp.thread_rank() == \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e \u0026#x26;\u0026#x26; rstd != nullptr) {\n        __stcs(rstd + idx, s);\n    }\n\n    \u003cspan class=\"hljs-comment\"\u003e// final normalization and scaling by weight/bias\u003c/span\u003e\n    \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e* o = out + idx * C;\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e (\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e c = warp.thread_rank(); c \u0026#x3C; C; c += warp.size()) {\n        \u003cspan class=\"hljs-comment\"\u003e// load and store using the .cs \"streaming\" hint to the compiler,\u003c/span\u003e\n        \u003cspan class=\"hljs-comment\"\u003e// indicating that this data will not be reused soon, and can be streamed through the caches\u003c/span\u003e\n        \u003cspan class=\"hljs-comment\"\u003e// this allows the threads to get more cache-hits for the (shared) weight and bias parameters\u003c/span\u003e\n        \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e n = s * (__ldcs(x+c) - m);\n        __stcs(o+c, n * weight[c] + bias[c]);\n    }\n}\n\n\u003cspan class=\"hljs-type\"\u003evoid\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003elayernorm_forward\u003c/span\u003e\u003cspan class=\"hljs-params\"\u003e(\u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e* out, \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e* mean, \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e* rstd,\n                       \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e* inp, \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e* weight, \u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e* bias,\n                       \u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e B, \u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e T, \u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e C)\u003c/span\u003e {\n    \u003cspan class=\"hljs-type\"\u003econst\u003c/span\u003e \u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e block_size = \u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e;\n    \u003cspan class=\"hljs-type\"\u003econst\u003c/span\u003e \u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e N = B * T;\n    \u003cspan class=\"hljs-type\"\u003econst\u003c/span\u003e \u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e grid_size = CEIL_DIV(N * \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e, block_size);\n    layernorm_forward_kernel3\u0026#x3C;\u0026#x3C;\u0026#x3C;grid_size, block_size\u003e\u003e\u003e(out, mean, rstd, inp, weight, bias, N, C);\n    cudaCheck(cudaGetLastError());\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/karpathy/llm.c/blob/1dafa60ad972ae43d70080e2e9497c60ea31fe42/train_gpt2_fp32.cu#L116-L161\"\u003ehttps://github.com/karpathy/llm.c/blob/1dafa60ad972ae43d70080e2e9497c60ea31fe42/train_gpt2_fp32.cu#L116-L161\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e这个 kernel 需要关注的点是对于 \u003ccode\u003ecooperative_groups\u003c/code\u003e 这个库的运用，比如下面这段代码\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-c\"\u003ecg::thread_block block = cg::this_thread_block();\ncg::thread_block_tile\u0026#x3C;\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e\u003e warp = cg::tiled_partition\u0026#x3C;\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e\u003e(block);\n\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e idx = blockIdx.x * warp.meta_group_size() + warp.meta_group_rank();\n\u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e(idx \u003e= N) {\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e;\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e这里实现了一个 warp 负责一行的计算，如果用 \u003ccode\u003eblockIdx\u003c/code\u003e, \u003ccode\u003eblockDim\u003c/code\u003e, \u003ccode\u003ethreadIdx\u003c/code\u003e 来实现如上的功能，代码差不多像下面这样\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-c\"\u003e\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e warp_size = \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e; \n\u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e idx = (blockIdx.x * blockDim.x + threadIdx.x) / warp_size;\n\u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e(idx \u003e= N) {\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e;\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e而随后 \u003ccode\u003ewarp.thread_rank()\u003c/code\u003e 可以替代 \u003ccode\u003ethreadIdx.x % warp_size\u003c/code\u003e，整体来说感觉使用 \u003ccode\u003ecooperative_groups\u003c/code\u003e 可以让代码更加清晰，直观。\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ecooperative_groups\u003c/code\u003e 的另一个核心功能是支持更细粒度的 thread group 的 sync，传统使用 \u003ccode\u003e__syncthreads()\u003c/code\u003e 的时候，整个 block 的所有 thread 都会被阻塞。在我们的这段代码中，\u003ccode\u003ecg::thread_block_tile\u0026#x3C;32\u003e warp = cg::tiled_partition\u0026#x3C;32\u003e(block)\u003c/code\u003e 把 32 个 thread 分成一个 thread group （warp），在 warp level 上进行 sync，\u003ccode\u003esum = cg::reduce(warp, sum, cg::plus\u0026#x3C;float\u003e{})\u003c/code\u003e。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://developer.nvidia.com/blog/cooperative-groups/\"\u003eCooperative Groups: Flexible CUDA Thread Programming\u003c/a\u003e 这篇文章对 \u003ccode\u003ecooperative_groups\u003c/code\u003e 有详细的介绍，强烈推荐阅读。\u003c/p\u003e","title":"LLM.C 中的 CUDA Kernel","date":"2024-07-17"}},"__N_SSG":true},"page":"/posts/[id]","query":{"id":"kernel_in_llmdotc"},"buildId":"B6bfnZBrc4hF-4DFzBgHE","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>