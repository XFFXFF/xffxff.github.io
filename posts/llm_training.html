<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/github.min.css"/><title>大模型训练优化——我的问题合集</title><meta name="next-head-count" content="4"/><link rel="preload" href="/_next/static/css/ab65974685f462b8.css" as="style"/><link rel="stylesheet" href="/_next/static/css/ab65974685f462b8.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-6cbe6e332df95288.js" defer=""></script><script src="/_next/static/chunks/main-26f9f36b33181737.js" defer=""></script><script src="/_next/static/chunks/pages/_app-2d6bf7b3192a8752.js" defer=""></script><script src="/_next/static/chunks/73-96e6cbd54826b874.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bid%5D-4b57e02c440700a5.js" defer=""></script><script src="/_next/static/wTPBy1ZzrXj1NkuaydzLJ/_buildManifest.js" defer=""></script><script src="/_next/static/wTPBy1ZzrXj1NkuaydzLJ/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="max-w-4xl px-4 mt-12 mb-24 mx-auto"><main><article><h1 class="text-3xl font-medium my-4 border-b-0">大模型训练优化——我的问题合集</h1><div class="text-gray-500 mb-8 pb-2 border-b-2 border-solid border-slate-300"><time dateTime="2024-04-19">April 19, 2024</time></div><div><p><strong>为什么 Megatron-LM 要在 PP 的第一 stage 和最后一个 stage 的 rank 上 build dataset 呢？</strong></p>
<p>看代码可以发现 Megatron-LM 在 build dataset 的时候，会根据 rank 来决定是否 build dataset，如果 PP 是第一个 stage 或者最后一个 stage 的 rank，那么就会 build dataset。</p>
<pre><code class="hljs language-python"><span class="hljs-comment"># https://github.com/NVIDIA/Megatron-LM/blob/2196398f5252ead6f036b06d45f7acb89b1308da/pretrain_gpt.py#L168-L169</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">is_dataset_built_on_rank</span>():
    <span class="hljs-keyword">return</span> (mpu.is_pipeline_first_stage() <span class="hljs-keyword">or</span> mpu.is_pipeline_last_stage()) <span class="hljs-keyword">and</span> mpu.get_tensor_model_parallel_rank() == <span class="hljs-number">0</span>
</code></pre>
<p>我的疑问其实是只用在第一个 stage build dataset 不就好了吗？其他 stage 的输入数据都是上一个 stage 的输出，通过网络传输过来的，为什么还要在最后一个 stage build dataset 呢？</p>
<p>这么做的原因是一个 batch 的数据包含 tokens, labels, loss_mask, attention_mask, position_ids，其中 labels, loss_mask 只在计算 loss 的时候用到，而 loss 的计算是在最后一个 stage 进行的，这些数据从第一个 stage 传到最后一个 stage 的过程中，会占用一些带宽，所以在最后一个 stage build dataset 可以减少数据传输的带宽。</p>
<p><img src="/get_batch.png" alt="alt text">
<a href="https://github.com/NVIDIA/Megatron-LM/blob/2196398f5252ead6f036b06d45f7acb89b1308da/megatron/training/utils.py#L276-L314">code link</a></p>
<hr>
<p><strong>Megatron-LM 的数据 .bin 和 .idx 文件存储了什么数据？分别有什么作用？</strong></p>
<p>bin 文件存储的是对文本进行 tokenize 之后的 token ids，idx 存储 bin 文件的一些 metadata，比如 version，数据的 dtype，有多少个 sequence，每个 sequence 对应的 offset。总之，可以通过 idx 文件区分 bin 文件中哪些 token ids 是同一个 sequence 的。</p></div></article></main><div class="mt-12"><a href="/">← 返回首页</a></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"postData":{"id":"llm_training","contentHtml":"\u003cp\u003e\u003cstrong\u003e为什么 Megatron-LM 要在 PP 的第一 stage 和最后一个 stage 的 rank 上 build dataset 呢？\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e看代码可以发现 Megatron-LM 在 build dataset 的时候，会根据 rank 来决定是否 build dataset，如果 PP 是第一个 stage 或者最后一个 stage 的 rank，那么就会 build dataset。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-python\"\u003e\u003cspan class=\"hljs-comment\"\u003e# https://github.com/NVIDIA/Megatron-LM/blob/2196398f5252ead6f036b06d45f7acb89b1308da/pretrain_gpt.py#L168-L169\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eis_dataset_built_on_rank\u003c/span\u003e():\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e (mpu.is_pipeline_first_stage() \u003cspan class=\"hljs-keyword\"\u003eor\u003c/span\u003e mpu.is_pipeline_last_stage()) \u003cspan class=\"hljs-keyword\"\u003eand\u003c/span\u003e mpu.get_tensor_model_parallel_rank() == \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e我的疑问其实是只用在第一个 stage build dataset 不就好了吗？其他 stage 的输入数据都是上一个 stage 的输出，通过网络传输过来的，为什么还要在最后一个 stage build dataset 呢？\u003c/p\u003e\n\u003cp\u003e这么做的原因是一个 batch 的数据包含 tokens, labels, loss_mask, attention_mask, position_ids，其中 labels, loss_mask 只在计算 loss 的时候用到，而 loss 的计算是在最后一个 stage 进行的，这些数据从第一个 stage 传到最后一个 stage 的过程中，会占用一些带宽，所以在最后一个 stage build dataset 可以减少数据传输的带宽。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/get_batch.png\" alt=\"alt text\"\u003e\n\u003ca href=\"https://github.com/NVIDIA/Megatron-LM/blob/2196398f5252ead6f036b06d45f7acb89b1308da/megatron/training/utils.py#L276-L314\"\u003ecode link\u003c/a\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e\u003cstrong\u003eMegatron-LM 的数据 .bin 和 .idx 文件存储了什么数据？分别有什么作用？\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ebin 文件存储的是对文本进行 tokenize 之后的 token ids，idx 存储 bin 文件的一些 metadata，比如 version，数据的 dtype，有多少个 sequence，每个 sequence 对应的 offset。总之，可以通过 idx 文件区分 bin 文件中哪些 token ids 是同一个 sequence 的。\u003c/p\u003e","title":"大模型训练优化——我的问题合集","date":"2024-04-19"}},"__N_SSG":true},"page":"/posts/[id]","query":{"id":"llm_training"},"buildId":"wTPBy1ZzrXj1NkuaydzLJ","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>