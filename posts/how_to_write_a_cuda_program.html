<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/github.min.css"/><title>笔记：How To Write A CUDA Program: The Ninja Edition</title><meta name="next-head-count" content="4"/><link rel="preload" href="/_next/static/css/ab65974685f462b8.css" as="style"/><link rel="stylesheet" href="/_next/static/css/ab65974685f462b8.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/_next/static/chunks/webpack-8fa1640cc84ba8fe.js" defer=""></script><script src="/_next/static/chunks/framework-6cbe6e332df95288.js" defer=""></script><script src="/_next/static/chunks/main-26f9f36b33181737.js" defer=""></script><script src="/_next/static/chunks/pages/_app-2d6bf7b3192a8752.js" defer=""></script><script src="/_next/static/chunks/73-96e6cbd54826b874.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bid%5D-4b57e02c440700a5.js" defer=""></script><script src="/_next/static/4Vgqy559KM2HjheYodWT6/_buildManifest.js" defer=""></script><script src="/_next/static/4Vgqy559KM2HjheYodWT6/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="max-w-4xl px-4 mt-12 mb-24 mx-auto"><main><article><h1 class="text-3xl font-medium my-4 border-b-0">笔记：How To Write A CUDA Program: The Ninja Edition</h1><div class="text-gray-500 mb-8 pb-2 border-b-2 border-solid border-slate-300"><time dateTime="2024-07-30">July 30, 2024</time></div><div><p><a href="https://www.nvidia.com/en-us/on-demand/session/gtc24-s62401/">How To Write A CUDA Program: The Ninja Edition</a> 是 GTC 2024 的一个 talk。收获很多，记录一下，值得反复观看。</p>
<h2>Wave Quantization</h2>
<p>当 thread block 的数量不是 SM 数量的整数倍时，在执行剩余的 thread block 时，一些 SM 会空闲。</p>
<p><img src="/how_to_write_a_cuda_program/image.png" alt="alt text"></p>
<p>对于 task A -> B -> C，每一个阶段都需要一个 extra partial wave。</p>
<p><img src="/how_to_write_a_cuda_program/image-2.png" alt="alt text"></p>
<blockquote>
<p>Don't map threads to data; map data to threads.</p>
</blockquote>
<p>这句话应该怎么理解呢？</p>
<p><img src="/how_to_write_a_cuda_program/image-1.png" alt="alt text"></p>
<p>在上面的例子中，图片大小是 1024x1024，将图片划分为一些 block，你想着划分为 16x16 挺好，每个 block 有 64x64 个像素，然后给每个 block 分配一些 thread 来处理，这就是 map threads to data。H100 有 132 个 SM，这样设计的话，需要两个 wave 才能处理完所有 block。</p>
<p>Map data to threads 的思路是，先考虑 SM 的数量，根据 SM 的数量设计 block 的划分，尽量不让 SM 空闲。</p>
<blockquote>
<p>这里可能和 occupancy 容易混淆，occupancy 是指分配给一个 SM 的 warp 数与其支持的最大 warp 数的比例</p>
</blockquote>
<p>但是有些情况没办法做到 single-wave 或者 integer-wave：</p>
<ol>
<li>一些算法需要特定大小的 tile</li>
<li>必须考虑不同型号的 GPU（比如 RTX-3090/80/70/60）</li>
<li>由于使用非 constant 的 tile size 而增加代码复杂度</li>
<li>不同 block load balance 的问题，可能不会比增加一个 extra partial wave 好多少</li>
</ol>
<h2>Task Parallelism</h2>
<p>在执行 A 的 extra partial wave 时，有很多 SM 空闲，B 是依赖 A 的结果的，所以 B 这时没法利用这些空闲的 SM。但是如果现在有另一个和 A 不相关的的任务，可以让这些 SM 去执行这个任务，这就是 task parallelism。</p>
<p><img src="/how_to_write_a_cuda_program/image-4.png" alt="alt text"></p>
<p>这样做整体的吞吐量会提高，但是单一任务的 latency 并不一定会降低，比如图中的 Task 1: A -> B -> C 的 latency 比之前变高了。</p>
<p><img src="/how_to_write_a_cuda_program/image-5.png" alt="alt text"></p>
<p>Task parallelism 有一个很大的问题，就是 thrashing cache。同时执行的 task 变多，每个 task 能分配到的 cache 就会变少，需要更频繁的和 global memory 交互。如果 task 是一个 memory-bound 的任务，task parallelism 也不一定会提高整体的吞吐。</p>
<h2>Keep Data in Cache</h2>
<blockquote>
<p>L2 cache == shared memory</p>
</blockquote>
<p>对于 task A -> B -> C，有如下过程：</p>
<ol>
<li>A 从 global memory 读取数据到 shared memory</li>
<li>A 计算</li>
<li>A 将结果写回 global memory</li>
<li>B 从 global memory 读取数据到 shared memory</li>
<li>B 计算</li>
<li>B 将结果写回 global memory</li>
<li>C 从 global memory 读取数据到 shared memory</li>
<li>C 计算</li>
<li>C 将结果写回 global memory</li>
</ol>
<p>设想一下如果 task 足够小，小到中间计算结果完全可以放在 shared memory 中，那么就不需要这么多次 global memory 的读写了。执行 A -> B -> C 的过程就变成了：</p>
<ol>
<li>A 从 global memory 读取数据到 shared memory</li>
<li>A 计算</li>
<li>B 从 shared memory 读取 A 的结果</li>
<li>B 计算</li>
<li>C 从 shared memory 读取 B 的结果</li>
<li>C 计算</li>
<li>C 将结果写回 global memory</li>
</ol>
<p>当然，并不是所有任务都可以这样做，但是如果能找到一种方式将 task 拆分为更小的 task，使得这些小 task 可以完全放在 shared memory 中，让这些小 task 串行执行。</p>
<p><img src="/how_to_write_a_cuda_program/image-8.png" alt="alt text"></p>
<p>这其实就是 flash attention 的优化思路。</p></div></article></main><div class="mt-12"><a href="/">← 返回首页</a></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"postData":{"id":"how_to_write_a_cuda_program","contentHtml":"\u003cp\u003e\u003ca href=\"https://www.nvidia.com/en-us/on-demand/session/gtc24-s62401/\"\u003eHow To Write A CUDA Program: The Ninja Edition\u003c/a\u003e 是 GTC 2024 的一个 talk。收获很多，记录一下，值得反复观看。\u003c/p\u003e\n\u003ch2\u003eWave Quantization\u003c/h2\u003e\n\u003cp\u003e当 thread block 的数量不是 SM 数量的整数倍时，在执行剩余的 thread block 时，一些 SM 会空闲。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/how_to_write_a_cuda_program/image.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cp\u003e对于 task A -\u003e B -\u003e C，每一个阶段都需要一个 extra partial wave。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/how_to_write_a_cuda_program/image-2.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eDon't map threads to data; map data to threads.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e这句话应该怎么理解呢？\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/how_to_write_a_cuda_program/image-1.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cp\u003e在上面的例子中，图片大小是 1024x1024，将图片划分为一些 block，你想着划分为 16x16 挺好，每个 block 有 64x64 个像素，然后给每个 block 分配一些 thread 来处理，这就是 map threads to data。H100 有 132 个 SM，这样设计的话，需要两个 wave 才能处理完所有 block。\u003c/p\u003e\n\u003cp\u003eMap data to threads 的思路是，先考虑 SM 的数量，根据 SM 的数量设计 block 的划分，尽量不让 SM 空闲。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e这里可能和 occupancy 容易混淆，occupancy 是指分配给一个 SM 的 warp 数与其支持的最大 warp 数的比例\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e但是有些情况没办法做到 single-wave 或者 integer-wave：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e一些算法需要特定大小的 tile\u003c/li\u003e\n\u003cli\u003e必须考虑不同型号的 GPU（比如 RTX-3090/80/70/60）\u003c/li\u003e\n\u003cli\u003e由于使用非 constant 的 tile size 而增加代码复杂度\u003c/li\u003e\n\u003cli\u003e不同 block load balance 的问题，可能不会比增加一个 extra partial wave 好多少\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003eTask Parallelism\u003c/h2\u003e\n\u003cp\u003e在执行 A 的 extra partial wave 时，有很多 SM 空闲，B 是依赖 A 的结果的，所以 B 这时没法利用这些空闲的 SM。但是如果现在有另一个和 A 不相关的的任务，可以让这些 SM 去执行这个任务，这就是 task parallelism。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/how_to_write_a_cuda_program/image-4.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cp\u003e这样做整体的吞吐量会提高，但是单一任务的 latency 并不一定会降低，比如图中的 Task 1: A -\u003e B -\u003e C 的 latency 比之前变高了。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/how_to_write_a_cuda_program/image-5.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cp\u003eTask parallelism 有一个很大的问题，就是 thrashing cache。同时执行的 task 变多，每个 task 能分配到的 cache 就会变少，需要更频繁的和 global memory 交互。如果 task 是一个 memory-bound 的任务，task parallelism 也不一定会提高整体的吞吐。\u003c/p\u003e\n\u003ch2\u003eKeep Data in Cache\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eL2 cache == shared memory\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e对于 task A -\u003e B -\u003e C，有如下过程：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eA 从 global memory 读取数据到 shared memory\u003c/li\u003e\n\u003cli\u003eA 计算\u003c/li\u003e\n\u003cli\u003eA 将结果写回 global memory\u003c/li\u003e\n\u003cli\u003eB 从 global memory 读取数据到 shared memory\u003c/li\u003e\n\u003cli\u003eB 计算\u003c/li\u003e\n\u003cli\u003eB 将结果写回 global memory\u003c/li\u003e\n\u003cli\u003eC 从 global memory 读取数据到 shared memory\u003c/li\u003e\n\u003cli\u003eC 计算\u003c/li\u003e\n\u003cli\u003eC 将结果写回 global memory\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e设想一下如果 task 足够小，小到中间计算结果完全可以放在 shared memory 中，那么就不需要这么多次 global memory 的读写了。执行 A -\u003e B -\u003e C 的过程就变成了：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eA 从 global memory 读取数据到 shared memory\u003c/li\u003e\n\u003cli\u003eA 计算\u003c/li\u003e\n\u003cli\u003eB 从 shared memory 读取 A 的结果\u003c/li\u003e\n\u003cli\u003eB 计算\u003c/li\u003e\n\u003cli\u003eC 从 shared memory 读取 B 的结果\u003c/li\u003e\n\u003cli\u003eC 计算\u003c/li\u003e\n\u003cli\u003eC 将结果写回 global memory\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e当然，并不是所有任务都可以这样做，但是如果能找到一种方式将 task 拆分为更小的 task，使得这些小 task 可以完全放在 shared memory 中，让这些小 task 串行执行。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/how_to_write_a_cuda_program/image-8.png\" alt=\"alt text\"\u003e\u003c/p\u003e\n\u003cp\u003e这其实就是 flash attention 的优化思路。\u003c/p\u003e","title":"笔记：How To Write A CUDA Program: The Ninja Edition","date":"2024-07-30"}},"__N_SSG":true},"page":"/posts/[id]","query":{"id":"how_to_write_a_cuda_program"},"buildId":"4Vgqy559KM2HjheYodWT6","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>