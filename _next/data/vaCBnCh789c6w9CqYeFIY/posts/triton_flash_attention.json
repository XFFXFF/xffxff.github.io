{"pageProps":{"postData":{"id":"triton_flash_attention","contentHtml":"<p>受 <a href=\"https://github.com/srush/Triton-Puzzles\">Triton Puzzles</a> 启发，把 flash attention 的实现过程拆分成一系列的 puzzle，逐步用 triton 去实现，以一个更平滑的学习曲线来学习 triton 和 flash attention。但是</p>\n<ul>\n<li>本文不会介绍 flash attention 的原理，阅读本文最好是对 flash attention 有一定了解，但不是必须的。</li>\n<li>本文不会介绍 triton 的一些基本用法，阅读本文至少得看懂 triton 官方的第一个 tutorial：<a href=\"https://triton-lang.org/main/getting-started/tutorials/01-vector-add.html\">Vector Addition</a></li>\n</ul>\n<p>Attention 的计算公式如下</p>\n<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>Attention</mtext><mo stretchy=\"false\">(</mo><mi>Q</mi><mo separator=\"true\">,</mo><mi>K</mi><mo separator=\"true\">,</mo><mi>V</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mtext>softmax</mtext><mo stretchy=\"false\">(</mo><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><mo stretchy=\"false\">)</mo><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">\\text{Attention}(Q, K, V) = \\text{softmax}({QK^T})V</annotation></semantics></math></span>\n<p>flash attention 的核心思路是通过分块计算，将中间计算 fuse 在一起，避免来回读写中间结果（<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">QK^T</annotation></semantics></math></span>, softmax)，减少访问 HBM 的次数，提高计算效率。</p>\n<p><img src=\"/image-6.png\" alt=\"alt text\"></p>\n<p>我们的最终目标是用 triton 实现 flash attention v2 的 forward，即下面这个伪代码</p>\n<h3>Puzzle 1：softmax</h3>\n<p>对一个 2D 矩阵，计算 softmax</p>\n<p>我们先用 pytorch 实现</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">softmax</span>(<span class=\"hljs-params\">x</span>):\n    x_max = x.<span class=\"hljs-built_in\">max</span>(dim=-<span class=\"hljs-number\">1</span>, keepdim=<span class=\"hljs-literal\">True</span>)[<span class=\"hljs-number\">0</span>]\n    x = x - x_max\n    x_exp = x.exp()\n    x_exp_sum = x_exp.<span class=\"hljs-built_in\">sum</span>(dim=-<span class=\"hljs-number\">1</span>, keepdim=<span class=\"hljs-literal\">True</span>)\n    <span class=\"hljs-keyword\">return</span> x_exp / x_exp_sum\n</code></pre>\n<p><img src=\"/image-3.png\" alt=\"alt text\"></p>\n<p>每个 program instance 计算一行的 softmax</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-meta\">@triton.jit</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">softmax_kernel</span>(<span class=\"hljs-params\">x_ptr, output_ptr, row_stride, n_cols, BLOCK_SIZE: tl.constexpr</span>):\n    pid = tl.program_id(<span class=\"hljs-number\">0</span>)\n\n    <span class=\"hljs-comment\"># BLOCK_SIZE is bigger than the number of columns</span>\n    col_range = tl.arange(<span class=\"hljs-number\">0</span>, BLOCK_SIZE)\n    col_mask = col_range &#x3C; n_cols\n    x = tl.load(x_ptr + pid * row_stride + col_range, mask=col_mask)\n    x_max = tl.<span class=\"hljs-built_in\">max</span>(x, axis=-<span class=\"hljs-number\">1</span>)\n    x = x - x_max\n    x_exp = tl.exp(x)\n    x_exp_sum = tl.<span class=\"hljs-built_in\">sum</span>(x_exp, axis=-<span class=\"hljs-number\">1</span>)\n    tl.store(output_ptr + pid * row_stride + col_range, x_exp / x_exp_sum, mask=col_mask)\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">triton_softmax</span>(<span class=\"hljs-params\">x</span>):\n    n_rows, n_cols = x.shape\n    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n    output = torch.empty_like(x)\n    softmax_kernel[(n_rows,)](\n        x,\n        output,\n        x.stride(<span class=\"hljs-number\">0</span>),\n        n_cols,\n        BLOCK_SIZE,\n    )\n    <span class=\"hljs-keyword\">return</span> output\n</code></pre>\n<p>这个实现非常简单，可以是看做对 pytorch 代码的一对一翻译</p>\n<p><img src=\"/torch_to_triton.jpg\" alt=\"\"></p>\n<h3>Puzzle 2: 分块算 softmax</h3>\n<p>上面我们一次 load 了一整行的数据，如果我们一次只 load 一行的一部分数据呢？</p>\n<p><img src=\"/image-4.png\" alt=\"alt text\"></p>\n<p>是不是简单加个 for loop 就行了？</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-meta\">@triton.jit</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">softmax_kernel</span>(<span class=\"hljs-params\">x_ptr, output_ptr, row_stride, n_cols, BLOCK_SIZE: tl.constexpr</span>):\n    pid = tl.program_id(<span class=\"hljs-number\">0</span>)\n\n    <span class=\"hljs-keyword\">for</span> offset <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-number\">0</span>, n_cols, BLOCK_SIZE):\n        col_range = tl.arange(<span class=\"hljs-number\">0</span>, BLOCK_SIZE)\n        col_mask = col_range + offset &#x3C; n_cols\n        x = tl.load(x_ptr + pid * row_stride + col_range + offset, mask=col_mask)\n        x_max = tl.<span class=\"hljs-built_in\">max</span>(x, axis=-<span class=\"hljs-number\">1</span>)\n        x = x - x_max\n        x_exp = tl.exp(x)\n        x_exp_sum = tl.<span class=\"hljs-built_in\">sum</span>(x_exp, axis=-<span class=\"hljs-number\">1</span>)\n        tl.store(output_ptr + pid * row_stride + col_range + offset, x_exp / x_exp_sum, mask=col_mask)\n</code></pre>\n<p>稍加思考，我们就会发现这个实现是有问题的，因为我们计算 x_max 和 x_exp_sum 的时候只考虑了当前的 block，而期望得到的是整行的 x_max 和 x_exp_sum。</p>\n<p>我们再用两个 for loop，一个计算 x_max，一个计算 x_exp_sum，然后再计算 softmax</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-meta\">@triton.jit</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">softmax_kernel_v2</span>(<span class=\"hljs-params\">x_ptr, output_ptr, row_stride, n_cols, BLOCK_SIZE: tl.constexpr</span>):\n    pid = tl.program_id(<span class=\"hljs-number\">0</span>)\n\n    x_max = -<span class=\"hljs-built_in\">float</span>(<span class=\"hljs-string\">'inf'</span>)\n    <span class=\"hljs-keyword\">for</span> offset <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-number\">0</span>, n_cols, BLOCK_SIZE):\n        col_range = tl.arange(<span class=\"hljs-number\">0</span>, BLOCK_SIZE)\n        col_mask = col_range + offset &#x3C; n_cols\n        x = tl.load(x_ptr + pid * row_stride + col_range + offset, mask=col_mask, other=-<span class=\"hljs-built_in\">float</span>(<span class=\"hljs-string\">'inf'</span>))\n        x_max = tl.maximum(x_max, tl.<span class=\"hljs-built_in\">max</span>(x, axis=-<span class=\"hljs-number\">1</span>))\n    \n    x_exp_sum = <span class=\"hljs-number\">0.0</span>\n    <span class=\"hljs-keyword\">for</span> offset <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-number\">0</span>, n_cols, BLOCK_SIZE):\n        col_range = tl.arange(<span class=\"hljs-number\">0</span>, BLOCK_SIZE)\n        col_mask = col_range + offset &#x3C; n_cols\n        x = tl.load(x_ptr + pid * row_stride + col_range + offset, mask=col_mask, other=-<span class=\"hljs-built_in\">float</span>(<span class=\"hljs-string\">'inf'</span>))\n        x_exp_sum = x_exp_sum + tl.<span class=\"hljs-built_in\">sum</span>(tl.exp(x - x_max), axis=-<span class=\"hljs-number\">1</span>)\n    \n    <span class=\"hljs-keyword\">for</span> offset <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-number\">0</span>, n_cols, BLOCK_SIZE):\n        col_range = tl.arange(<span class=\"hljs-number\">0</span>, BLOCK_SIZE)\n        col_mask = col_range + offset &#x3C; n_cols\n        x = tl.load(x_ptr + pid * row_stride + col_range + offset, mask=col_mask)\n        x_exp = tl.exp(x - x_max)\n        tl.store(output_ptr + pid * row_stride + col_range + offset, x_exp / x_exp_sum, mask=col_mask)\n\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">triton_softmax_v2</span>(<span class=\"hljs-params\">x</span>):\n    n_rows, n_cols = x.shape\n    output = torch.empty_like(x)\n    BLOCK_SIZE = <span class=\"hljs-number\">256</span>\n    softmax_kernel_v2[(n_rows,)](\n        x,\n        output,\n        x.stride(<span class=\"hljs-number\">0</span>),\n        n_cols,\n        BLOCK_SIZE\n    )\n    <span class=\"hljs-keyword\">return</span> output\n</code></pre>\n<h3>Puzzle 3: online softmax</h3>\n<p>一个更聪明的做法是，我们可以在一次 for loop 中计算 x_max 和 x_exp_sum，这样我们就可以减少一次 for loop。参考 paper <a href=\"https://arxiv.org/pdf/1805.02867.pdf\">Online normalizer calculation for softmax</a></p>\n<p><img src=\"/image-1.png\" alt=\"alt text\"></p>\n<p>我们可以使用归纳法证明这个算法的正确性。</p>\n<p>当 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>V</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">V = 1</annotation></semantics></math></span> 时（即向量只有一个元素），<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>m</mi><mn>1</mn></msub><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">m_1 = x_1</annotation></semantics></math></span> 是输入向量中的最大值，<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>d</mi><mn>1</mn></msub><mo>=</mo><msup><mi>e</mi><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>−</mo><msub><mi>m</mi><mn>1</mn></msub></mrow></msup><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">d_1 = e^{x_1 - m_1} = 1</annotation></semantics></math></span>，符合softmax函数的定义。</p>\n<p>假设对于 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>V</mi><mo>=</mo><mi>S</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">V = S - 1</annotation></semantics></math></span>，上述方法正确地计算了 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>m</mi><mrow><mi>S</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><msubsup><mrow><mi>max</mi><mo>⁡</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>S</mi><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mi>x</mi><mi>k</mi></msub></mrow><annotation encoding=\"application/x-tex\">m_{S-1} = \\max_{k=1}^{S-1} x_k</annotation></semantics></math></span> 和 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>d</mi><mrow><mi>S</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>S</mi><mo>−</mo><mn>1</mn></mrow></msubsup><msup><mi>e</mi><mrow><msub><mi>x</mi><mi>j</mi></msub><mo>−</mo><msub><mi>m</mi><mrow><mi>S</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">d_{S-1} = \\sum_{j=1}^{S-1} e^{x_j - m_{S-1}}</annotation></semantics></math></span>。</p>\n<p>然后对于 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>V</mi><mo>=</mo><mi>S</mi></mrow><annotation encoding=\"application/x-tex\">V = S</annotation></semantics></math></span>，我们需要证明算法也能准确计算 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>m</mi><mi>S</mi></msub></mrow><annotation encoding=\"application/x-tex\">m_S</annotation></semantics></math></span> 和 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>d</mi><mi>S</mi></msub></mrow><annotation encoding=\"application/x-tex\">d_S</annotation></semantics></math></span>。</p>\n<p><span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>m</mi><mi>S</mi></msub><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><msub><mi>m</mi><mrow><mi>S</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator=\"true\">,</mo><msub><mi>x</mi><mi>S</mi></msub><mo stretchy=\"false\">)</mo><mo>=</mo><msubsup><mrow><mi>max</mi><mo>⁡</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></msubsup><msub><mi>x</mi><mi>k</mi></msub></mrow><annotation encoding=\"application/x-tex\">m_S = \\max(m_{S-1}, x_S) = \\max_{k=1}^{S} x_k</annotation></semantics></math></span></p>\n<p>这表明更新后的 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>m</mi><mi>S</mi></msub></mrow><annotation encoding=\"application/x-tex\">m_S</annotation></semantics></math></span> 正确地表示了前 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>S</mi></mrow><annotation encoding=\"application/x-tex\">S</annotation></semantics></math></span> 个输入元素的最大值。</p>\n<p><span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>d</mi><mi>S</mi></msub><mo>=</mo><msub><mi>d</mi><mrow><mi>S</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>×</mo><msup><mi>e</mi><mrow><msub><mi>m</mi><mrow><mi>S</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>−</mo><msub><mi>m</mi><mi>S</mi></msub></mrow></msup><mo>+</mo><msup><mi>e</mi><mrow><msub><mi>x</mi><mi>S</mi></msub><mo>−</mo><msub><mi>m</mi><mi>S</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">d_S = d_{S-1} \\times e^{m_{S-1} - m_S} + e^{x_S - m_S}</annotation></semantics></math></span>\n根据归纳假设，可以将 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>d</mi><mrow><mi>S</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">d_{S-1}</annotation></semantics></math></span> 展开为 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>S</mi><mo>−</mo><mn>1</mn></mrow></msubsup><msup><mi>e</mi><mrow><msub><mi>x</mi><mi>j</mi></msub><mo>−</mo><msub><mi>m</mi><mrow><mi>S</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\sum_{j=1}^{S-1} e^{x_j - m_{S-1}}</annotation></semantics></math></span>，通过替换和简化，我们可以得到：\n<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>d</mi><mi>S</mi></msub><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></msubsup><msup><mi>e</mi><mrow><msub><mi>x</mi><mi>j</mi></msub><mo>−</mo><msub><mi>m</mi><mi>S</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">d_S = \\sum_{j=1}^{S} e^{x_j - m_S}</annotation></semantics></math></span></p>\n<p>这表明更新后的 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>d</mi><mi>S</mi></msub></mrow><annotation encoding=\"application/x-tex\">d_S</annotation></semantics></math></span> 正确地计算了前 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>S</mi></mrow><annotation encoding=\"application/x-tex\">S</annotation></semantics></math></span> 个元素，相对于新的最大值 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>m</mi><mi>S</mi></msub></mrow><annotation encoding=\"application/x-tex\">m_S</annotation></semantics></math></span> 的归一化因子。</p>\n<p>对应的 triton 代码如下：</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-meta\">@triton.jit</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">softmax_kernel_v3</span>(<span class=\"hljs-params\">x_ptr, output_ptr, row_stride, n_cols, BLOCK_SIZE: tl.constexpr</span>):\n    pid = tl.program_id(<span class=\"hljs-number\">0</span>)\n\n    x_max = -<span class=\"hljs-built_in\">float</span>(<span class=\"hljs-string\">'inf'</span>)\n    x_exp_sum = <span class=\"hljs-number\">0.0</span>\n    <span class=\"hljs-keyword\">for</span> offset <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-number\">0</span>, n_cols, BLOCK_SIZE):\n        col_range = tl.arange(<span class=\"hljs-number\">0</span>, BLOCK_SIZE)\n        col_mask = col_range + offset &#x3C; n_cols\n        x = tl.load(x_ptr + pid * row_stride + col_range + offset, mask=col_mask, other=-<span class=\"hljs-built_in\">float</span>(<span class=\"hljs-string\">'inf'</span>))\n        x_max_new = tl.maximum(x_max, tl.<span class=\"hljs-built_in\">max</span>(x, axis=-<span class=\"hljs-number\">1</span>))\n        x_exp_sum = x_exp_sum * tl.exp(x_max - x_max_new) + tl.<span class=\"hljs-built_in\">sum</span>(tl.exp(x - x_max_new), axis=-<span class=\"hljs-number\">1</span>)\n        x_max = x_max_new\n    \n    <span class=\"hljs-keyword\">for</span> offset <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-number\">0</span>, n_cols, BLOCK_SIZE):\n        col_range = tl.arange(<span class=\"hljs-number\">0</span>, BLOCK_SIZE)\n        col_mask = col_range + offset &#x3C; n_cols\n        x = tl.load(x_ptr + pid * row_stride + col_range + offset, mask=col_mask)\n        x_exp = tl.exp(x - x_max)\n        tl.store(output_ptr + pid * row_stride + col_range + offset, x_exp / x_exp_sum, mask=col_mask)\n\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">triton_softmax_v3</span>(<span class=\"hljs-params\">x</span>):\n    n_rows, n_cols = x.shape\n    output = torch.empty_like(x)\n    BLOCK_SIZE = <span class=\"hljs-number\">256</span>\n    softmax_kernel_v3[(n_rows,)](\n        x,\n        output,\n        x.stride(<span class=\"hljs-number\">0</span>),\n        n_cols,\n        BLOCK_SIZE\n    )\n    <span class=\"hljs-keyword\">return</span> output\n</code></pre>\n<h3>Puzzle 4: 更通用的分块策略</h3>\n<p>前面我们都是按行分块，更通用的分块策略是按行和列都分块</p>\n<p><img src=\"/image-5.png\" alt=\"alt text\"></p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-meta\">@triton.jit</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">softmax_kernel_v4</span>(<span class=\"hljs-params\">x_ptr, output_ptr, row_stride, n_rows, n_cols, BLOCK_ROW: tl.constexpr, BLOCK_COL: tl.constexpr</span>):\n    pid = tl.program_id(<span class=\"hljs-number\">0</span>)\n\n    x_max = tl.full((BLOCK_ROW, ), -<span class=\"hljs-built_in\">float</span>(<span class=\"hljs-string\">'inf'</span>), dtype=tl.float32)\n    x_exp_sum = tl.full((BLOCK_ROW, ), <span class=\"hljs-number\">0.0</span>, dtype=tl.float32)\n    <span class=\"hljs-keyword\">for</span> offset <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-number\">0</span>, n_cols, BLOCK_COL):\n        row_range = tl.arange(<span class=\"hljs-number\">0</span>, BLOCK_ROW) + pid * BLOCK_ROW\n        col_range = tl.arange(<span class=\"hljs-number\">0</span>, BLOCK_COL) + offset\n        x_mask = (row_range[:, <span class=\"hljs-literal\">None</span>] &#x3C; n_rows) &#x26; (col_range &#x3C; n_cols)\n        x_range = row_range[:, <span class=\"hljs-literal\">None</span>] * row_stride + col_range\n        x = tl.load(x_ptr + x_range, mask=x_mask, other=-<span class=\"hljs-built_in\">float</span>(<span class=\"hljs-string\">'inf'</span>))\n        x_max_new = tl.maximum(x_max, tl.<span class=\"hljs-built_in\">max</span>(x, axis=-<span class=\"hljs-number\">1</span>))\n        x_exp_sum = tl.exp(x_max - x_max_new) * x_exp_sum + tl.<span class=\"hljs-built_in\">sum</span>(tl.exp(x - x_max_new[:, <span class=\"hljs-literal\">None</span>]), axis=-<span class=\"hljs-number\">1</span>)\n        x_max = x_max_new\n    \n    <span class=\"hljs-keyword\">for</span> offset <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-number\">0</span>, n_cols, BLOCK_COL):\n        row_range = tl.arange(<span class=\"hljs-number\">0</span>, BLOCK_ROW) + pid * BLOCK_ROW\n        col_range = tl.arange(<span class=\"hljs-number\">0</span>, BLOCK_COL) + offset\n        x_mask = (row_range[:, <span class=\"hljs-literal\">None</span>] &#x3C; n_rows) &#x26; (col_range &#x3C; n_cols)\n        x_range = row_range[:, <span class=\"hljs-literal\">None</span>] * row_stride + col_range\n        x = tl.load(x_ptr + x_range, mask=x_mask)\n        x_exp = tl.exp(x - x_max[:, <span class=\"hljs-literal\">None</span>])\n        tl.store(output_ptr + x_range, x_exp / x_exp_sum[:, <span class=\"hljs-literal\">None</span>], mask=x_mask)\n\n\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">triton_softmax_v4</span>(<span class=\"hljs-params\">x</span>):\n    n_rows, n_cols = x.shape\n    output = torch.empty_like(x)\n    grid = <span class=\"hljs-keyword\">lambda</span> meta: (triton.cdiv(n_rows, meta[<span class=\"hljs-string\">'BLOCK_ROW'</span>]), )\n    softmax_kernel_v4[grid](\n        x,\n        output,\n        x.stride(<span class=\"hljs-number\">0</span>),\n        n_rows,\n        n_cols,\n        <span class=\"hljs-number\">32</span>,\n        <span class=\"hljs-number\">256</span>\n    )\n    <span class=\"hljs-keyword\">return</span> output\n</code></pre>\n<h3>flash attention 的实现</h3>\n<p>最后，我们可以按照 flash attention 的伪代码实现 forward</p>\n<p><img src=\"/flash_attention_forward_pseudocode.png\" alt=\"\"></p>\n<p><img src=\"/image.png\" alt=\"alt text\"></p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-meta\">@triton.jit</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">flash_attention_kernel</span>(<span class=\"hljs-params\">q_ptr, k_ptr, v_ptr, output_ptr, n, d: tl.constexpr, BR: tl.constexpr, BC: tl.constexpr</span>):\n    pid = tl.program_id(<span class=\"hljs-number\">0</span>)\n\n    q_row_range = tl.arange(<span class=\"hljs-number\">0</span>, BR) + pid * BR\n    q_range = q_row_range[:, <span class=\"hljs-literal\">None</span>] * d + tl.arange(<span class=\"hljs-number\">0</span>, d)\n    q_mask = (q_row_range[:, <span class=\"hljs-literal\">None</span>] &#x3C; n) &#x26; (tl.arange(<span class=\"hljs-number\">0</span>, d) &#x3C; d)\n    q = tl.load(q_ptr + q_range, mask=q_mask)\n\n    o = tl.full((BR, d), <span class=\"hljs-number\">0</span>, dtype=tl.float32)\n    l = tl.full((BR,), <span class=\"hljs-number\">0</span>, dtype=tl.float32)\n    m = tl.full((BR,), -<span class=\"hljs-built_in\">float</span>(<span class=\"hljs-string\">'inf'</span>), dtype=tl.float32)\n\n    <span class=\"hljs-keyword\">for</span> j <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-number\">0</span>, n, BC):\n        kv_row_range = tl.arange(<span class=\"hljs-number\">0</span>, BC) + j\n        kv_range = kv_row_range[:, <span class=\"hljs-literal\">None</span>] * d + tl.arange(<span class=\"hljs-number\">0</span>, d)\n        kv_mask = (kv_row_range[:, <span class=\"hljs-literal\">None</span>] &#x3C; n) &#x26; (tl.arange(<span class=\"hljs-number\">0</span>, d) &#x3C; d)\n        k = tl.load(k_ptr + kv_range, mask=kv_mask, other=<span class=\"hljs-number\">0</span>)\n        v = tl.load(v_ptr + kv_range, mask=kv_mask, other=<span class=\"hljs-number\">0</span>)\n        s = tl.dot(q, tl.trans(k))\n        s_mask = (q_row_range[:, <span class=\"hljs-literal\">None</span>] &#x3C; n) &#x26; (kv_row_range &#x3C; n)\n        s = tl.where(s_mask, s, -<span class=\"hljs-built_in\">float</span>(<span class=\"hljs-string\">'inf'</span>))\n        m_new = tl.maximum(m, tl.<span class=\"hljs-built_in\">max</span>(s, axis=-<span class=\"hljs-number\">1</span>))\n        p = tl.exp(s - m_new[:, <span class=\"hljs-literal\">None</span>])\n        l = tl.exp(m - m_new) * l + tl.<span class=\"hljs-built_in\">sum</span>(p, axis=-<span class=\"hljs-number\">1</span>)\n        o = tl.exp(m - m_new)[:, <span class=\"hljs-literal\">None</span>] * o + tl.dot(p, v)\n        m = m_new\n\n    tl.store(output_ptr + q_range, o / l[:, <span class=\"hljs-literal\">None</span>], mask=q_mask)\n\n\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">flash_attention</span>(<span class=\"hljs-params\">q, k, v</span>):\n    n, d = q.shape\n    BR = <span class=\"hljs-number\">32</span>\n    BC = <span class=\"hljs-number\">64</span>\n    output = torch.empty((n, d), device=q.device)\n    grid = <span class=\"hljs-keyword\">lambda</span> meta: (triton.cdiv(n, BR),)\n    flash_attention_kernel[grid](\n        q,\n        k,\n        v,\n        output,\n        n,\n        d,\n        BR,\n        BC,\n    )\n    <span class=\"hljs-keyword\">return</span> output\n</code></pre>","title":"Triton Puzzles: 从 softmax 到 flash attention","date":"2024-03-29"}},"__N_SSG":true}