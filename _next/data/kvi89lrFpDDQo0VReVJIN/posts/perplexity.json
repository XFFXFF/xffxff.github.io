{"pageProps":{"postData":{"id":"perplexity","contentHtml":"<p>在大模型评测中，经常会看到 perplexity 这个指标，我只是知道 perplexity 越小越好，但是不知道它的具体含义，本文尝试深入理解 perplexity。</p>\n<h2>直观理解“困惑”</h2>\n<p>现在有两个句子</p>\n<ul>\n<li>The cat sat on the mat</li>\n<li>The cat sat on the Thursday</li>\n</ul>\n<p>显然第一个句子很容易理解，第二个句子让人困惑。如果 model 生成了第二个句子，那么这个 model 是不尽如人意的。</p>\n<h2>设计一个评测</h2>\n<p>给 model 一段高质量的文本，比如 wikipedia 的一段文章，去测试 model 对这段文本的困惑程度，一个优秀的 model 应该对这段文本的“困惑程度”很小。</p>\n<p>怎么去量化 model 对一段文本的困惑程度呢？</p>\n<h2>如何量化“困惑”</h2>\n<p>最容易想到的就是计算 model 生成这个句子的概率。如果 model 生成某一文本的概率大，就说 model 对这一文本的困惑度小，反之困惑度大。</p>\n<p>现在基于 decoder only 架构的大模型生成一个新的 token 会依赖于之前生成的 token，如果我们想要计算 model 生成某一句子的概率，就是一个联合概率的问题，即：</p>\n<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mrow><mn>1</mn><mo>:</mo><mi>L</mi></mrow></msub><mo stretchy=\"false\">)</mo><mo>=</mo><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi>p</mi><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant=\"normal\">∣</mi><msub><mi>x</mi><mrow><mn>1</mn><mo>:</mo><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(x_{1:L}) = \\prod_{i=1}^{L} p(x_i|x_{1:i-1})</annotation></semantics></math></span>\n<p>但是这样做有一个问题，就是如果句子很长，那 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mrow><mn>1</mn><mo>:</mo><mi>L</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(x_{1:L})</annotation></semantics></math></span> 就会很小，这样的话，我们就很难比较两个句子的概率了。</p>\n<p>那如果给每个 token 的概率 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant=\"normal\">∣</mi><msub><mi>x</mi><mrow><mn>1</mn><mo>:</mo><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(x_i|x_{1:i-1})</annotation></semantics></math></span> 取平均值呢？考虑句子 “the cat sat on the mat”，假设每个 token 的概率都是 0.5，那么用平均值计算出来的结果就是 0.5，现在假设 model 认为 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>m</mi><mi>a</mi><mi>t</mi><mi mathvariant=\"normal\">∣</mi><mi>t</mi><mi>h</mi><mi>e</mi><mspace width=\"1em\"></mspace><mi>c</mi><mi>a</mi><mi>t</mi><mspace width=\"1em\"></mspace><mi>s</mi><mi>a</mi><mi>t</mi><mspace width=\"1em\"></mspace><mi>o</mi><mi>n</mi><mspace width=\"1em\"></mspace><mi>t</mi><mi>h</mi><mi>e</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">p(mat|the \\quad cat \\quad sat \\quad on \\quad the) = 0</annotation></semantics></math></span>，这时候用平均值计算出来的结果就是 (0.5 * 5 + 0) / 6 = 0.4167，这个值确实比 0.5 小，但是这个指标并没有给到 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>m</mi><mi>a</mi><mi>t</mi><mi mathvariant=\"normal\">∣</mi><mi>t</mi><mi>h</mi><mi>e</mi><mspace width=\"1em\"></mspace><mi>c</mi><mi>a</mi><mi>t</mi><mspace width=\"1em\"></mspace><mi>s</mi><mi>a</mi><mi>t</mi><mspace width=\"1em\"></mspace><mi>o</mi><mi>n</mi><mspace width=\"1em\"></mspace><mi>t</mi><mi>h</mi><mi>e</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">p(mat|the \\quad cat \\quad sat \\quad on \\quad the) = 0</annotation></semantics></math></span> 这个显然不合理的概率一个足够大的惩罚。即这个句子应该是很有可能出现的，并没有什么让人困惑的地方，如果对这个句子感到困惑，就应该在评测指标上给予足够的惩罚。</p>\n<p>几何平均是一个更合理的计算方式，几何平均值的计算公式是：</p>\n<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mroot><mrow><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi>p</mi><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant=\"normal\">∣</mi><msub><mi>x</mi><mrow><mn>1</mn><mo>:</mo><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mi>L</mi></mroot></mrow><annotation encoding=\"application/x-tex\">\\sqrt[L]{\\prod_{i=1}^{L} p(x_i|x_{1:i-1})}</annotation></semantics></math></span>\n<p>困惑度应该和生成概率成反比，对概率取倒数，就是 perplexity 的定义：</p>\n<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>Perplexity</mtext><mo>=</mo><mfrac><mn>1</mn><mroot><mrow><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi>p</mi><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant=\"normal\">∣</mi><msub><mi>x</mi><mrow><mn>1</mn><mo>:</mo><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mi>L</mi></mroot></mfrac></mrow><annotation encoding=\"application/x-tex\">\\text{Perplexity} = \\frac{1}{\\sqrt[L]{\\prod_{i=1}^{L} p(x_i|x_{1:i-1})}}</annotation></semantics></math></span>\n<p>如果某一 token 的概率很小，比如说为 0，那么 perplexity 就会变得无穷大，这样就给了这个 token 一个足够大的惩罚。</p>\n<h2>Perplexity 的信息论解释</h2>\n<p>对 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mi>e</mi><mi>r</mi><mi>p</mi><mi>l</mi><mi>e</mi><mi>x</mi><mi>i</mi><mi>t</mi><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">Perplexity</annotation></semantics></math></span> 取对数，然后指数化，可以得到：</p>\n<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mtable rowspacing=\"0.25em\" columnalign=\"right left\" columnspacing=\"0em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mtext>Perplexity</mtext></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mrow><mrow></mrow><mo>=</mo><mfrac><mn>1</mn><mroot><mrow><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi>p</mi><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant=\"normal\">∣</mi><msub><mi>x</mi><mrow><mn>1</mn><mo>:</mo><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mi>L</mi></mroot></mfrac></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mrow><mrow></mrow><mo>=</mo><mi>exp</mi><mo>⁡</mo><mrow><mo fence=\"true\">(</mo><mi>log</mi><mo>⁡</mo><mrow><mo fence=\"true\">(</mo><mfrac><mn>1</mn><mroot><mrow><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi>p</mi><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant=\"normal\">∣</mi><msub><mi>x</mi><mrow><mn>1</mn><mo>:</mo><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mi>L</mi></mroot></mfrac><mo fence=\"true\">)</mo></mrow><mo fence=\"true\">)</mo></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mrow><mrow></mrow><mo>=</mo><mi>exp</mi><mo>⁡</mo><mrow><mo fence=\"true\">(</mo><mi>log</mi><mo>⁡</mo><mrow><mo fence=\"true\">(</mo><msup><mrow><mo fence=\"true\">(</mo><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi>p</mi><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant=\"normal\">∣</mi><msub><mi>x</mi><mrow><mn>1</mn><mo>:</mo><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo><mo fence=\"true\">)</mo></mrow><mrow><mo>−</mo><mfrac><mn>1</mn><mi>L</mi></mfrac></mrow></msup><mo fence=\"true\">)</mo></mrow><mo fence=\"true\">)</mo></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mrow><mrow></mrow><mo>=</mo><mi>exp</mi><mo>⁡</mo><mrow><mo fence=\"true\">(</mo><mo>−</mo><mfrac><mn>1</mn><mi>L</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi>log</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant=\"normal\">∣</mi><msub><mi>x</mi><mrow><mn>1</mn><mo>:</mo><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo><mo fence=\"true\">)</mo></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mrow><mrow></mrow><mo>=</mo><mi>exp</mi><mo>⁡</mo><mrow><mo fence=\"true\">(</mo><mfrac><mn>1</mn><mi>L</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi>log</mi><mo>⁡</mo><mfrac><mn>1</mn><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant=\"normal\">∣</mi><msub><mi>x</mi><mrow><mn>1</mn><mo>:</mo><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mfrac><mo fence=\"true\">)</mo></mrow></mrow></mstyle></mtd></mtr></mtable><annotation encoding=\"application/x-tex\">\\begin{align*}\n\\text{Perplexity} &#x26;= \\frac{1}{\\sqrt[L]{\\prod_{i=1}^{L} p(x_i|x_{1:i-1})}} \\\\\n&#x26;= \\exp\\left(\\log\\left(\\frac{1}{\\sqrt[L]{\\prod_{i=1}^{L} p(x_i|x_{1:i-1})}}\\right)\\right) \\\\\n&#x26;= \\exp\\left(\\log\\left(\\left(\\prod_{i=1}^{L} p(x_i|x_{1:i-1})\\right)^{-\\frac{1}{L}}\\right)\\right) \\\\\n&#x26;= \\exp\\left(-\\frac{1}{L} \\sum_{i=1}^{L} \\log(p(x_i|x_{1:i-1}))\\right) \\\\\n&#x26;= \\exp\\left(\\frac{1}{L} \\sum_{i=1}^{L} \\log \\frac{1}{p(x_i|x_{1:i-1})}\\right)\n\\end{align*}</annotation></semantics></math></span>\n<p>其中 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mfrac><mn>1</mn><mi>L</mi></mfrac><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></msubsup><mi>log</mi><mo>⁡</mo><mfrac><mn>1</mn><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant=\"normal\">∣</mi><msub><mi>x</mi><mrow><mn>1</mn><mo>:</mo><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mfrac><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\exp(\\frac{1}{L} \\sum_{i=1}^{L} \\log \\frac{1}{p(x_i|x_{1:i-1})})</annotation></semantics></math></span> 就是交叉熵，即：</p>\n<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>Perplexity</mtext><mo>=</mo><mi>exp</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mtext>CrossEntropy</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\text{Perplexity} = \\exp(\\text{CrossEntropy})</annotation></semantics></math></span>\n<p>在信息论中，一个事件的信息量与该事件发生的概率成反比在信息论中，概率的倒数被称为概率的“信息量”。信息论告诉我们，对于一个特定事件，其最优编码长度应该与其信息量相等。所以 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mfrac><mn>1</mn><mi>L</mi></mfrac><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></msubsup><mi>log</mi><mo>⁡</mo><mfrac><mn>1</mn><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant=\"normal\">∣</mi><msub><mi>x</mi><mrow><mn>1</mn><mo>:</mo><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\frac{1}{L} \\sum_{i=1}^{L} \\log \\frac{1}{p(x_i|x_{1:i-1})}</annotation></semantics></math></span> 表示使用 model 预测的分布来编码的平均长度。<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy=\"false\">(</mo><mi>a</mi><mi>v</mi><mi>e</mi><mi>r</mi><mi>a</mi><mi>g</mi><mi>e</mi><mi>c</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi><mi>e</mi><mi>n</mi><mi>g</mi><mi>t</mi><mi>h</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">exp(average code length)</annotation></semantics></math></span> 可以看作是 model 在预测下一个 token 时平均有多少种选择。类似排列组合问题，每一位都有 0 1 两种选择，一共有 3 位，那么一共有 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mn>2</mn><mn>3</mn></msup></mrow><annotation encoding=\"application/x-tex\">2^3</annotation></semantics></math></span> 种选择。</p>\n<h2>参考资料</h2>\n<p><a href=\"https://stanford-cs324.github.io/winter2022/lectures/capabilities/#language-modeling\">https://stanford-cs324.github.io/winter2022/lectures/capabilities/#language-modeling</a></p>\n<p><a href=\"https://www.zhihu.com/tardis/zm/ans/244557337?source_id=1003\">为什么交叉熵（cross-entropy）可以用于计算代价？</a></p>","title":"Perplexity","date":"2023-12-24"}},"__N_SSG":true}