{"pageProps":{"postData":{"id":"kernel_in_llmdotc","contentHtml":"<h2>Matmul</h2>\n<pre><code class=\"hljs language-c\">__global__ <span class=\"hljs-type\">void</span> __launch_bounds__(<span class=\"hljs-number\">16</span>*<span class=\"hljs-number\">16</span>, <span class=\"hljs-number\">2</span>) matmul_forward_kernel4(<span class=\"hljs-type\">float</span>* out,\n                                                                   <span class=\"hljs-type\">const</span> <span class=\"hljs-type\">float</span>* inp, <span class=\"hljs-type\">const</span> <span class=\"hljs-type\">float</span>* weight, <span class=\"hljs-type\">const</span> <span class=\"hljs-type\">float</span>* bias,\n                                                                   <span class=\"hljs-type\">int</span> C, <span class=\"hljs-type\">int</span> OC) {\n    <span class=\"hljs-comment\">// out is (B,T,OC). OC is short for \"output channels\", e.g. OC = 4 * C</span>\n    <span class=\"hljs-comment\">// inp is (B,T,C), weight is (OC, C), bias is (OC)</span>\n    <span class=\"hljs-comment\">// each thread handles 8x8 elements; each block 128 by 128 elements.</span>\n    <span class=\"hljs-type\">int</span> oc = <span class=\"hljs-number\">8</span>*(blockIdx.y * blockDim.y + threadIdx.y);\n\n    <span class=\"hljs-comment\">// buffers to cache chunks of the input matrices</span>\n    __shared__ <span class=\"hljs-type\">float</span> lhs_s[<span class=\"hljs-number\">128</span>][<span class=\"hljs-number\">32</span>];\n    __shared__ <span class=\"hljs-type\">float</span> rhs_s[<span class=\"hljs-number\">128</span>][<span class=\"hljs-number\">32</span>];\n\n    <span class=\"hljs-comment\">// adjust our pointers for the current block</span>\n    inp += <span class=\"hljs-number\">128</span> * blockIdx.x * C;\n    weight += <span class=\"hljs-number\">128</span> * blockIdx.y * C;\n    out += <span class=\"hljs-number\">128</span> * blockIdx.x * OC + <span class=\"hljs-number\">128</span> * blockIdx.y;\n\n    <span class=\"hljs-type\">float</span> vals[<span class=\"hljs-number\">8</span>][<span class=\"hljs-number\">8</span>] = {};\n    <span class=\"hljs-keyword\">if</span>(bias != <span class=\"hljs-literal\">NULL</span>) {\n        <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> i = <span class=\"hljs-number\">0</span>; i &#x3C; <span class=\"hljs-number\">8</span>; i++) {\n            <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> j = <span class=\"hljs-number\">0</span>; j &#x3C; <span class=\"hljs-number\">8</span>; j += <span class=\"hljs-number\">4</span>) {\n                float4 b = ld_vec(bias + oc + j);\n                vals[i][j+<span class=\"hljs-number\">0</span>] = b.x;\n                vals[i][j+<span class=\"hljs-number\">1</span>] = b.y;\n                vals[i][j+<span class=\"hljs-number\">2</span>] = b.z;\n                vals[i][j+<span class=\"hljs-number\">3</span>] = b.w;\n            }\n        }\n    }\n\n    <span class=\"hljs-type\">int</span> si_start = <span class=\"hljs-number\">4</span>*(<span class=\"hljs-number\">16</span> * threadIdx.y + threadIdx.x);\n    <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> so = <span class=\"hljs-number\">0</span>; so &#x3C; C; so += <span class=\"hljs-number\">32</span>) {\n        __syncthreads();\n        <span class=\"hljs-type\">int</span> xmod8 = threadIdx.x % <span class=\"hljs-number\">8</span>;\n        <span class=\"hljs-type\">int</span> xby8 = threadIdx.x / <span class=\"hljs-number\">8</span>;\n        <span class=\"hljs-type\">int</span> xo = <span class=\"hljs-number\">4</span> * xmod8;\n        <span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">int</span> y = <span class=\"hljs-number\">2</span> * threadIdx.y + xby8; y &#x3C; <span class=\"hljs-number\">128</span>; y += <span class=\"hljs-number\">32</span>) {\n            st_vec(&#x26;lhs_s[y][xo], ld_vec(inp + y * C + so + xo));\n            st_vec(&#x26;rhs_s[y][xo], ld_vec(weight + y * C + so + xo));\n        }\n        __syncthreads();\n\n        <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> si = si_start; si &#x3C; si_start + <span class=\"hljs-number\">32</span>; si += <span class=\"hljs-number\">4</span>) {\n            float4 rhs[<span class=\"hljs-number\">8</span>];\n            <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> u = <span class=\"hljs-number\">0</span>; u &#x3C; <span class=\"hljs-number\">8</span>; ++u) {\n                rhs[u] = ld_vec(&#x26;rhs_s[u + <span class=\"hljs-number\">8</span> * threadIdx.y][si % <span class=\"hljs-number\">32</span>]);\n            }\n\n            <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> ii = <span class=\"hljs-number\">0</span>; ii &#x3C; <span class=\"hljs-number\">8</span>; ++ii) {\n                float4 lhs = ld_vec(&#x26;lhs_s[ii + <span class=\"hljs-number\">8</span> * threadIdx.x][si % <span class=\"hljs-number\">32</span>]);\n                <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> ji = <span class=\"hljs-number\">0</span>; ji &#x3C; <span class=\"hljs-number\">8</span>; ++ji) {\n                    vals[ii][ji] += lhs.x * rhs[ji].x;\n                    vals[ii][ji] += lhs.y * rhs[ji].y;\n                    vals[ii][ji] += lhs.z * rhs[ji].z;\n                    vals[ii][ji] += lhs.w * rhs[ji].w;\n                }\n            }\n        }\n    }\n\n    <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> i = <span class=\"hljs-number\">0</span>; i &#x3C; <span class=\"hljs-number\">8</span>; ++i) {\n        <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> j = <span class=\"hljs-number\">0</span>; j &#x3C; <span class=\"hljs-number\">8</span>; j += <span class=\"hljs-number\">4</span>) {\n            float4 result;\n            result.x = vals[i][j + <span class=\"hljs-number\">0</span>];\n            result.y = vals[i][j + <span class=\"hljs-number\">1</span>];\n            result.z = vals[i][j + <span class=\"hljs-number\">2</span>];\n            result.w = vals[i][j + <span class=\"hljs-number\">3</span>];\n            st_vec(out + (<span class=\"hljs-number\">8</span>*threadIdx.x+i) * OC + <span class=\"hljs-number\">8</span>*threadIdx.y + j, result);\n        }\n    }\n}\n</code></pre>\n<p><a href=\"https://github.com/karpathy/llm.c/blob/1dafa60ad972ae43d70080e2e9497c60ea31fe42/train_gpt2_fp32.cu#L617-L687\">https://github.com/karpathy/llm.c/blob/1dafa60ad972ae43d70080e2e9497c60ea31fe42/train_gpt2_fp32.cu#L617-L687</a></p>\n<p>对于 shape 为 [N, C] 和 [C, OC] 的矩阵乘法，每个 thread block 处理 [128, C] 和 [C, 128] 的矩阵乘法。计算 [128, C] * [C, 128] 的时候，每次将两个矩阵的一部分，即 [128, 32] 和 [32, 128] 加载到 shared memory 中，然后计算结果。这对应第一个 for loop</p>\n<pre><code class=\"hljs language-c\"><span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> so = <span class=\"hljs-number\">0</span>; so &#x3C; C; so += <span class=\"hljs-number\">32</span>) {\n    __syncthreads();\n    <span class=\"hljs-type\">int</span> xmod8 = threadIdx.x % <span class=\"hljs-number\">8</span>;\n    <span class=\"hljs-type\">int</span> xby8 = threadIdx.x / <span class=\"hljs-number\">8</span>;\n    <span class=\"hljs-type\">int</span> xo = <span class=\"hljs-number\">4</span> * xmod8;\n    <span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">int</span> y = <span class=\"hljs-number\">2</span> * threadIdx.y + xby8; y &#x3C; <span class=\"hljs-number\">128</span>; y += <span class=\"hljs-number\">32</span>) {\n        st_vec(&#x26;lhs_s[y][xo], ld_vec(inp + y * C + so + xo));\n        st_vec(&#x26;rhs_s[y][xo], ld_vec(weight + y * C + so + xo));\n    }\n    __syncthreads();\n    ...\n}\n</code></pre>\n<p><img src=\"/llmdotc/image.png\" alt=\"alt text\"></p>\n<p>每个 thread block 包含 16 * 16 个 thread，thread block 中的 thread 一起协作把 [128, 32] 的两个矩阵分别加载到 shared memory 中。128 * 32 / (16 * 16) = 16，所以每个 thread 负责加载 16 个元素。threadidx (0, 0) 负责加载的元素如下图所示</p>\n<p><img src=\"/llmdotc/image-1.png\" alt=\"alt text\"></p>\n<p>这里对应的代码如下</p>\n<pre><code class=\"hljs language-c\"><span class=\"hljs-type\">int</span> xmod8 = threadIdx.x % <span class=\"hljs-number\">8</span>;\n<span class=\"hljs-type\">int</span> xby8 = threadIdx.x / <span class=\"hljs-number\">8</span>;\n<span class=\"hljs-type\">int</span> xo = <span class=\"hljs-number\">4</span> * xmod8;\n<span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">int</span> y = <span class=\"hljs-number\">2</span> * threadIdx.y + xby8; y &#x3C; <span class=\"hljs-number\">128</span>; y += <span class=\"hljs-number\">32</span>) {\n    st_vec(&#x26;lhs_s[y][xo], ld_vec(inp + y * C + so + xo));\n    st_vec(&#x26;rhs_s[y][xo], ld_vec(weight + y * C + so + xo));\n}\n</code></pre>\n<p>其中 <code>st_vec</code> 和 <code>ld_vec</code> 分别 store 和 load 4 个 float</p>\n<pre><code class=\"hljs language-c\">__device__ float4 <span class=\"hljs-title function_\">ld_vec</span><span class=\"hljs-params\">(<span class=\"hljs-type\">const</span> <span class=\"hljs-type\">float</span>* address)</span> {\n    <span class=\"hljs-keyword\">return</span> *reinterpret_cast&#x3C;<span class=\"hljs-type\">const</span> float4*>(address);\n}\n\n__device__ <span class=\"hljs-type\">void</span> <span class=\"hljs-title function_\">st_vec</span><span class=\"hljs-params\">(<span class=\"hljs-type\">float</span>* address, float4 val)</span> {\n    *reinterpret_cast&#x3C;float4*>(address) = val;\n}\n</code></pre>\n<p>一个 thread block 包含 16 * 16 个 thread，每个 thread 负责计算 [8, 32] * [32, 8] 的矩阵乘法。</p>\n<p><img src=\"/llmdotc/image-2.png\" alt=\"alt text\"></p>\n<p>每个 thread 在计算 [8, 32] * [32, 8] 的时候，分成了更小的块去计算，一次计算 [8, 4] * [4, 8] 的矩阵乘法。</p>\n<p><img src=\"/llmdotc/image-3.png\" alt=\"alt text\"></p>\n<p>对应如下两个 for loop</p>\n<pre><code class=\"hljs language-c\"><span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> si = si_start; si &#x3C; si_start + <span class=\"hljs-number\">32</span>; si += <span class=\"hljs-number\">4</span>) {\n    float4 rhs[<span class=\"hljs-number\">8</span>];\n    <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> u = <span class=\"hljs-number\">0</span>; u &#x3C; <span class=\"hljs-number\">8</span>; ++u) {\n        rhs[u] = ld_vec(&#x26;rhs_s[u + <span class=\"hljs-number\">8</span> * threadIdx.y][si % <span class=\"hljs-number\">32</span>]);\n    }\n\n    <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> ii = <span class=\"hljs-number\">0</span>; ii &#x3C; <span class=\"hljs-number\">8</span>; ++ii) {\n        float4 lhs = ld_vec(&#x26;lhs_s[ii + <span class=\"hljs-number\">8</span> * threadIdx.x][si % <span class=\"hljs-number\">32</span>]);\n        <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> ji = <span class=\"hljs-number\">0</span>; ji &#x3C; <span class=\"hljs-number\">8</span>; ++ji) {\n            vals[ii][ji] += lhs.x * rhs[ji].x;\n            vals[ii][ji] += lhs.y * rhs[ji].y;\n            vals[ii][ji] += lhs.z * rhs[ji].z;\n            vals[ii][ji] += lhs.w * rhs[ji].w;\n        }\n    }\n}\n</code></pre>\n<h2>LayerNorm</h2>\n<pre><code class=\"hljs language-c\">__global__ <span class=\"hljs-type\">void</span> <span class=\"hljs-title function_\">layernorm_forward_kernel3</span><span class=\"hljs-params\">(<span class=\"hljs-type\">float</span>* __restrict__ out, <span class=\"hljs-type\">float</span>* __restrict__ mean, <span class=\"hljs-type\">float</span>* __restrict__ rstd,\n                                    <span class=\"hljs-type\">const</span> <span class=\"hljs-type\">float</span>*  __restrict__ inp, <span class=\"hljs-type\">const</span> <span class=\"hljs-type\">float</span>*  __restrict__ weight,\n                                    <span class=\"hljs-type\">const</span> <span class=\"hljs-type\">float</span>* __restrict__ bias, <span class=\"hljs-type\">int</span> N, <span class=\"hljs-type\">int</span> C)</span> {\n    cg::thread_block block = cg::this_thread_block();\n    cg::thread_block_tile&#x3C;<span class=\"hljs-number\">32</span>> warp = cg::tiled_partition&#x3C;<span class=\"hljs-number\">32</span>>(block);\n    <span class=\"hljs-type\">int</span> idx = blockIdx.x * warp.meta_group_size() + warp.meta_group_rank();\n    <span class=\"hljs-keyword\">if</span>(idx >= N) {\n        <span class=\"hljs-keyword\">return</span>;\n    }\n\n    <span class=\"hljs-comment\">// the row of input that this group of threads is responsible for</span>\n    <span class=\"hljs-type\">const</span> <span class=\"hljs-type\">float</span>* x = inp + idx * C;\n\n    <span class=\"hljs-comment\">// mean</span>\n    <span class=\"hljs-type\">float</span> sum = <span class=\"hljs-number\">0.0f</span>;\n    <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> i = warp.thread_rank(); i &#x3C; C; i += warp.size()) {\n        sum += x[i];\n    }\n    sum = cg::reduce(warp, sum, cg::plus&#x3C;<span class=\"hljs-type\">float</span>>{});\n    <span class=\"hljs-type\">float</span> m = sum / C;\n    <span class=\"hljs-keyword\">if</span>(warp.thread_rank() == <span class=\"hljs-number\">0</span> &#x26;&#x26; mean != nullptr) {\n        __stcs(mean + idx, m);\n    }\n\n    <span class=\"hljs-comment\">// rstd</span>\n    sum = <span class=\"hljs-number\">0.0f</span>;\n    <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> i = warp.thread_rank(); i &#x3C; C; i += warp.size()) {\n        <span class=\"hljs-type\">float</span> diff = x[i] - m;\n        sum += diff * diff;\n    }\n    sum = cg::reduce(warp, sum, cg::plus&#x3C;<span class=\"hljs-type\">float</span>>{});\n    <span class=\"hljs-type\">float</span> s = rsqrtf(sum / C + <span class=\"hljs-number\">1e-5</span>f);\n    <span class=\"hljs-keyword\">if</span>(warp.thread_rank() == <span class=\"hljs-number\">0</span> &#x26;&#x26; rstd != nullptr) {\n        __stcs(rstd + idx, s);\n    }\n\n    <span class=\"hljs-comment\">// final normalization and scaling by weight/bias</span>\n    <span class=\"hljs-type\">float</span>* o = out + idx * C;\n    <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> c = warp.thread_rank(); c &#x3C; C; c += warp.size()) {\n        <span class=\"hljs-comment\">// load and store using the .cs \"streaming\" hint to the compiler,</span>\n        <span class=\"hljs-comment\">// indicating that this data will not be reused soon, and can be streamed through the caches</span>\n        <span class=\"hljs-comment\">// this allows the threads to get more cache-hits for the (shared) weight and bias parameters</span>\n        <span class=\"hljs-type\">float</span> n = s * (__ldcs(x+c) - m);\n        __stcs(o+c, n * weight[c] + bias[c]);\n    }\n}\n\n<span class=\"hljs-type\">void</span> <span class=\"hljs-title function_\">layernorm_forward</span><span class=\"hljs-params\">(<span class=\"hljs-type\">float</span>* out, <span class=\"hljs-type\">float</span>* mean, <span class=\"hljs-type\">float</span>* rstd,\n                       <span class=\"hljs-type\">float</span>* inp, <span class=\"hljs-type\">float</span>* weight, <span class=\"hljs-type\">float</span>* bias,\n                       <span class=\"hljs-type\">int</span> B, <span class=\"hljs-type\">int</span> T, <span class=\"hljs-type\">int</span> C)</span> {\n    <span class=\"hljs-type\">const</span> <span class=\"hljs-type\">int</span> block_size = <span class=\"hljs-number\">128</span>;\n    <span class=\"hljs-type\">const</span> <span class=\"hljs-type\">int</span> N = B * T;\n    <span class=\"hljs-type\">const</span> <span class=\"hljs-type\">int</span> grid_size = CEIL_DIV(N * <span class=\"hljs-number\">32</span>, block_size);\n    layernorm_forward_kernel3&#x3C;&#x3C;&#x3C;grid_size, block_size>>>(out, mean, rstd, inp, weight, bias, N, C);\n    cudaCheck(cudaGetLastError());\n}\n</code></pre>\n<p><a href=\"https://github.com/karpathy/llm.c/blob/1dafa60ad972ae43d70080e2e9497c60ea31fe42/train_gpt2_fp32.cu#L116-L161\">https://github.com/karpathy/llm.c/blob/1dafa60ad972ae43d70080e2e9497c60ea31fe42/train_gpt2_fp32.cu#L116-L161</a></p>\n<p>这个 kernel 需要关注的点是对于 <code>cooperative_groups</code> 这个库的运用，比如下面这段代码</p>\n<pre><code class=\"hljs language-c\">cg::thread_block block = cg::this_thread_block();\ncg::thread_block_tile&#x3C;<span class=\"hljs-number\">32</span>> warp = cg::tiled_partition&#x3C;<span class=\"hljs-number\">32</span>>(block);\n<span class=\"hljs-type\">int</span> idx = blockIdx.x * warp.meta_group_size() + warp.meta_group_rank();\n<span class=\"hljs-keyword\">if</span>(idx >= N) {\n    <span class=\"hljs-keyword\">return</span>;\n}\n</code></pre>\n<p>这里实现了一个 warp 负责一行的计算，如果用 <code>blockIdx</code>, <code>blockDim</code>, <code>threadIdx</code> 来实现如上的功能，代码差不多像下面这样</p>\n<pre><code class=\"hljs language-c\"><span class=\"hljs-type\">int</span> warp_size = <span class=\"hljs-number\">32</span>; \n<span class=\"hljs-type\">int</span> idx = (blockIdx.x * blockDim.x + threadIdx.x) / warp_size;\n<span class=\"hljs-keyword\">if</span>(idx >= N) {\n    <span class=\"hljs-keyword\">return</span>;\n}\n</code></pre>\n<p>而随后 <code>warp.thread_rank()</code> 可以替代 <code>threadIdx.x % warp_size</code>，整体来说感觉使用 <code>cooperative_groups</code> 可以让代码更加清晰，直观。</p>\n<p><code>cooperative_groups</code> 的另一个核心功能是支持更细粒度的 thread group 的 sync，传统使用 <code>__syncthreads()</code> 的时候，整个 block 的所有 thread 都会被阻塞。在我们的这段代码中，<code>cg::thread_block_tile&#x3C;32> warp = cg::tiled_partition&#x3C;32>(block)</code> 把 32 个 thread 分成一个 thread group （warp），在 warp level 上进行 sync，<code>sum = cg::reduce(warp, sum, cg::plus&#x3C;float>{})</code>。</p>\n<p><a href=\"https://developer.nvidia.com/blog/cooperative-groups/\">Cooperative Groups: Flexible CUDA Thread Programming</a> 这篇文章对 <code>cooperative_groups</code> 有详细的介绍，强烈推荐阅读。</p>","title":"LLM.C 中的 CUDA Kernel","date":"2024-07-17"}},"__N_SSG":true}