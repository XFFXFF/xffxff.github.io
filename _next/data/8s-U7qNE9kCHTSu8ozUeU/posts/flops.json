{"pageProps":{"postData":{"id":"flops","contentHtml":"<p>模型训练过程中大多数浮点运算都是矩阵乘法，对于一个 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">m \\times n</annotation></semantics></math></span> 的矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span> 和一个 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi><mo>×</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">n \\times p</annotation></semantics></math></span> 的矩阵 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>B</mi></mrow><annotation encoding=\"application/x-tex\">B</annotation></semantics></math></span>，<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi><mo>×</mo><mi>B</mi></mrow><annotation encoding=\"application/x-tex\">A \\times B</annotation></semantics></math></span> 需要 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>m</mi><mo>×</mo><mi>n</mi><mo>×</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">m \\times n \\times p</annotation></semantics></math></span> 次乘法和 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>m</mi><mo>×</mo><mi>n</mi><mo>×</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">m \\times n \\times p</annotation></semantics></math></span> 次加法，即需要 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>2</mn><mi>m</mi><mi>n</mi><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">2mnp</annotation></semantics></math></span> FLOPs。</p>\n<h2>Transformer Architecture 的 FLOPs 计算</h2>\n<p><img src=\"/transformer.png\" alt=\"alt text\"></p>\n<p><img src=\"/attention.png\" alt=\"alt text\"></p>\n<h3>Attention</h3>\n<p>Q，K，V transformation:  <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>3</mn><mo>×</mo><mn>2</mn><mi>B</mi><mi>s</mi><msup><mi>h</mi><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">3 \\times 2Bsh^2</annotation></semantics></math></span></p>\n<p><span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">QK^T</annotation></semantics></math></span>: <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>2</mn><mi>B</mi><msup><mi>s</mi><mn>2</mn></msup><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">2Bs^2h</annotation></semantics></math></span></p>\n<p>attention over values: <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>2</mn><mi>B</mi><msup><mi>s</mi><mn>2</mn></msup><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">2Bs^2h</annotation></semantics></math></span></p>\n<p>post-attention linear projection: <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>2</mn><mi>B</mi><mi>s</mi><msup><mi>h</mi><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">2Bsh^2</annotation></semantics></math></span></p>\n<h3>Feed Forward Network</h3>\n<p>linear h->4h: <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>8</mn><mi>B</mi><mi>s</mi><msup><mi>h</mi><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">8Bsh^2</annotation></semantics></math></span></p>\n<p>linear 4h->h: <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>8</mn><mi>B</mi><mi>s</mi><msup><mi>h</mi><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">8Bsh^2</annotation></semantics></math></span></p>\n<h3>Total</h3>\n<p>forward: <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mn>6</mn><mo>+</mo><mn>2</mn><mo>+</mo><mn>8</mn><mo>+</mo><mn>8</mn><mo stretchy=\"false\">)</mo><mi>B</mi><mi>s</mi><msup><mi>h</mi><mn>2</mn></msup><mo>+</mo><mo stretchy=\"false\">(</mo><mn>2</mn><mo>+</mo><mn>2</mn><mo stretchy=\"false\">)</mo><mi>B</mi><msup><mi>s</mi><mn>2</mn></msup><mi>h</mi><mo>=</mo><mn>24</mn><mi>B</mi><mi>s</mi><msup><mi>h</mi><mn>2</mn></msup><mo>+</mo><mn>4</mn><mi>B</mi><msup><mi>s</mi><mn>2</mn></msup><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">(6 + 2 + 8 + 8)Bsh^2 + (2 + 2)Bs^2h = 24Bsh^2 + 4Bs^2h</annotation></semantics></math></span></p>\n<p>backward 的 FLOPs 大致是 forward 的 2 倍，所以 forward + backward 的 FLOPs 大致是 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>72</mn><mi>B</mi><mi>s</mi><msup><mi>h</mi><mn>2</mn></msup><mo>+</mo><mn>12</mn><mi>B</mi><msup><mi>s</mi><mn>2</mn></msup><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">72Bsh^2 + 12Bs^2h</annotation></semantics></math></span></p>\n<p>参考 <a href=\"https://arxiv.org/pdf/2104.04473.pdf\">Megatron-LM 2</a> APPENDIX</p>","title":"FLOPs 的计算","date":"2024-04-18"}},"__N_SSG":true}