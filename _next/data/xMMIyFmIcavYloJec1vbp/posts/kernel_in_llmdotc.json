{"pageProps":{"postData":{"id":"kernel_in_llmdotc","contentHtml":"<h2>Matmul</h2>\n<pre><code class=\"hljs language-c\">__global__ <span class=\"hljs-type\">void</span> __launch_bounds__(<span class=\"hljs-number\">16</span>*<span class=\"hljs-number\">16</span>, <span class=\"hljs-number\">2</span>) matmul_forward_kernel4(<span class=\"hljs-type\">float</span>* out,\n                                                                   <span class=\"hljs-type\">const</span> <span class=\"hljs-type\">float</span>* inp, <span class=\"hljs-type\">const</span> <span class=\"hljs-type\">float</span>* weight, <span class=\"hljs-type\">const</span> <span class=\"hljs-type\">float</span>* bias,\n                                                                   <span class=\"hljs-type\">int</span> C, <span class=\"hljs-type\">int</span> OC) {\n    <span class=\"hljs-comment\">// out is (B,T,OC). OC is short for \"output channels\", e.g. OC = 4 * C</span>\n    <span class=\"hljs-comment\">// inp is (B,T,C), weight is (OC, C), bias is (OC)</span>\n    <span class=\"hljs-comment\">// each thread handles 8x8 elements; each block 128 by 128 elements.</span>\n    <span class=\"hljs-type\">int</span> oc = <span class=\"hljs-number\">8</span>*(blockIdx.y * blockDim.y + threadIdx.y);\n\n    <span class=\"hljs-comment\">// buffers to cache chunks of the input matrices</span>\n    __shared__ <span class=\"hljs-type\">float</span> lhs_s[<span class=\"hljs-number\">128</span>][<span class=\"hljs-number\">32</span>];\n    __shared__ <span class=\"hljs-type\">float</span> rhs_s[<span class=\"hljs-number\">128</span>][<span class=\"hljs-number\">32</span>];\n\n    <span class=\"hljs-comment\">// adjust our pointers for the current block</span>\n    inp += <span class=\"hljs-number\">128</span> * blockIdx.x * C;\n    weight += <span class=\"hljs-number\">128</span> * blockIdx.y * C;\n    out += <span class=\"hljs-number\">128</span> * blockIdx.x * OC + <span class=\"hljs-number\">128</span> * blockIdx.y;\n\n    <span class=\"hljs-type\">float</span> vals[<span class=\"hljs-number\">8</span>][<span class=\"hljs-number\">8</span>] = {};\n    <span class=\"hljs-keyword\">if</span>(bias != <span class=\"hljs-literal\">NULL</span>) {\n        <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> i = <span class=\"hljs-number\">0</span>; i &#x3C; <span class=\"hljs-number\">8</span>; i++) {\n            <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> j = <span class=\"hljs-number\">0</span>; j &#x3C; <span class=\"hljs-number\">8</span>; j += <span class=\"hljs-number\">4</span>) {\n                float4 b = ld_vec(bias + oc + j);\n                vals[i][j+<span class=\"hljs-number\">0</span>] = b.x;\n                vals[i][j+<span class=\"hljs-number\">1</span>] = b.y;\n                vals[i][j+<span class=\"hljs-number\">2</span>] = b.z;\n                vals[i][j+<span class=\"hljs-number\">3</span>] = b.w;\n            }\n        }\n    }\n\n    <span class=\"hljs-type\">int</span> si_start = <span class=\"hljs-number\">4</span>*(<span class=\"hljs-number\">16</span> * threadIdx.y + threadIdx.x);\n    <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> so = <span class=\"hljs-number\">0</span>; so &#x3C; C; so += <span class=\"hljs-number\">32</span>) {\n        __syncthreads();\n        <span class=\"hljs-type\">int</span> xmod8 = threadIdx.x % <span class=\"hljs-number\">8</span>;\n        <span class=\"hljs-type\">int</span> xby8 = threadIdx.x / <span class=\"hljs-number\">8</span>;\n        <span class=\"hljs-type\">int</span> xo = <span class=\"hljs-number\">4</span> * xmod8;\n        <span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">int</span> y = <span class=\"hljs-number\">2</span> * threadIdx.y + xby8; y &#x3C; <span class=\"hljs-number\">128</span>; y += <span class=\"hljs-number\">32</span>) {\n            st_vec(&#x26;lhs_s[y][xo], ld_vec(inp + y * C + so + xo));\n            st_vec(&#x26;rhs_s[y][xo], ld_vec(weight + y * C + so + xo));\n        }\n        __syncthreads();\n\n        <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> si = si_start; si &#x3C; si_start + <span class=\"hljs-number\">32</span>; si += <span class=\"hljs-number\">4</span>) {\n            float4 rhs[<span class=\"hljs-number\">8</span>];\n            <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> u = <span class=\"hljs-number\">0</span>; u &#x3C; <span class=\"hljs-number\">8</span>; ++u) {\n                rhs[u] = ld_vec(&#x26;rhs_s[u + <span class=\"hljs-number\">8</span> * threadIdx.y][si % <span class=\"hljs-number\">32</span>]);\n            }\n\n            <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> ii = <span class=\"hljs-number\">0</span>; ii &#x3C; <span class=\"hljs-number\">8</span>; ++ii) {\n                float4 lhs = ld_vec(&#x26;lhs_s[ii + <span class=\"hljs-number\">8</span> * threadIdx.x][si % <span class=\"hljs-number\">32</span>]);\n                <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> ji = <span class=\"hljs-number\">0</span>; ji &#x3C; <span class=\"hljs-number\">8</span>; ++ji) {\n                    vals[ii][ji] += lhs.x * rhs[ji].x;\n                    vals[ii][ji] += lhs.y * rhs[ji].y;\n                    vals[ii][ji] += lhs.z * rhs[ji].z;\n                    vals[ii][ji] += lhs.w * rhs[ji].w;\n                }\n            }\n        }\n    }\n\n    <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> i = <span class=\"hljs-number\">0</span>; i &#x3C; <span class=\"hljs-number\">8</span>; ++i) {\n        <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> j = <span class=\"hljs-number\">0</span>; j &#x3C; <span class=\"hljs-number\">8</span>; j += <span class=\"hljs-number\">4</span>) {\n            float4 result;\n            result.x = vals[i][j + <span class=\"hljs-number\">0</span>];\n            result.y = vals[i][j + <span class=\"hljs-number\">1</span>];\n            result.z = vals[i][j + <span class=\"hljs-number\">2</span>];\n            result.w = vals[i][j + <span class=\"hljs-number\">3</span>];\n            st_vec(out + (<span class=\"hljs-number\">8</span>*threadIdx.x+i) * OC + <span class=\"hljs-number\">8</span>*threadIdx.y + j, result);\n        }\n    }\n}\n</code></pre>\n<p><a href=\"https://github.com/karpathy/llm.c/blob/1dafa60ad972ae43d70080e2e9497c60ea31fe42/train_gpt2_fp32.cu#L617-L687\">https://github.com/karpathy/llm.c/blob/1dafa60ad972ae43d70080e2e9497c60ea31fe42/train_gpt2_fp32.cu#L617-L687</a></p>\n<p>对于 shape 为 [N, C] 和 [C, OC] 的矩阵乘法，每个 thread block 处理 [128, C] 和 [C, 128] 的矩阵乘法。计算 [128, C] * [C, 128] 的时候，每次将两个矩阵的一部分，即 [128, 32] 和 [32, 128] 加载到 shared memory 中，然后计算结果。这对应第一个 for loop</p>\n<pre><code class=\"hljs language-c\"><span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> so = <span class=\"hljs-number\">0</span>; so &#x3C; C; so += <span class=\"hljs-number\">32</span>) {\n    __syncthreads();\n    <span class=\"hljs-type\">int</span> xmod8 = threadIdx.x % <span class=\"hljs-number\">8</span>;\n    <span class=\"hljs-type\">int</span> xby8 = threadIdx.x / <span class=\"hljs-number\">8</span>;\n    <span class=\"hljs-type\">int</span> xo = <span class=\"hljs-number\">4</span> * xmod8;\n    <span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">int</span> y = <span class=\"hljs-number\">2</span> * threadIdx.y + xby8; y &#x3C; <span class=\"hljs-number\">128</span>; y += <span class=\"hljs-number\">32</span>) {\n        st_vec(&#x26;lhs_s[y][xo], ld_vec(inp + y * C + so + xo));\n        st_vec(&#x26;rhs_s[y][xo], ld_vec(weight + y * C + so + xo));\n    }\n    __syncthreads();\n    ...\n}\n</code></pre>\n<p><img src=\"/llmdotc/image.png\" alt=\"alt text\"></p>\n<p>每个 thread block 包含 16 * 16 个 thread，thread block 中的 thread 一起协作把 [128, 32] 的两个矩阵分别加载到 shared memory 中。128 * 32 / (16 * 16) = 16，所以每个 thread 负责加载 16 个元素。threadidx (0, 0) 负责加载的元素如下图所示</p>\n<p><img src=\"/llmdotc/image-1.png\" alt=\"alt text\"></p>\n<p>这里对应的代码如下</p>\n<pre><code class=\"hljs language-c\"><span class=\"hljs-type\">int</span> xmod8 = threadIdx.x % <span class=\"hljs-number\">8</span>;\n<span class=\"hljs-type\">int</span> xby8 = threadIdx.x / <span class=\"hljs-number\">8</span>;\n<span class=\"hljs-type\">int</span> xo = <span class=\"hljs-number\">4</span> * xmod8;\n<span class=\"hljs-keyword\">for</span>(<span class=\"hljs-type\">int</span> y = <span class=\"hljs-number\">2</span> * threadIdx.y + xby8; y &#x3C; <span class=\"hljs-number\">128</span>; y += <span class=\"hljs-number\">32</span>) {\n    st_vec(&#x26;lhs_s[y][xo], ld_vec(inp + y * C + so + xo));\n    st_vec(&#x26;rhs_s[y][xo], ld_vec(weight + y * C + so + xo));\n}\n</code></pre>\n<p>其中 <code>st_vec</code> 和 <code>ld_vec</code> 分别 store 和 load 4 个 float</p>\n<pre><code class=\"hljs language-c\">__device__ float4 <span class=\"hljs-title function_\">ld_vec</span><span class=\"hljs-params\">(<span class=\"hljs-type\">const</span> <span class=\"hljs-type\">float</span>* address)</span> {\n    <span class=\"hljs-keyword\">return</span> *reinterpret_cast&#x3C;<span class=\"hljs-type\">const</span> float4*>(address);\n}\n\n__device__ <span class=\"hljs-type\">void</span> <span class=\"hljs-title function_\">st_vec</span><span class=\"hljs-params\">(<span class=\"hljs-type\">float</span>* address, float4 val)</span> {\n    *reinterpret_cast&#x3C;float4*>(address) = val;\n}\n</code></pre>\n<p>一个 thread block 包含 16 * 16 个 thread，每个 thread 负责计算 [8, 32] * [32, 8] 的矩阵乘法。</p>\n<p><img src=\"/llmdotc/image-2.png\" alt=\"alt text\"></p>\n<p>每个 thread 在计算 [8, 32] * [32, 8] 的时候，分成了更小的块去计算，一次计算 [8, 4] * [4, 8] 的矩阵乘法。</p>\n<p><img src=\"/llmdotc/image-3.png\" alt=\"alt text\"></p>\n<p>对应如下两个 for loop</p>\n<pre><code class=\"hljs language-c\"><span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> si = si_start; si &#x3C; si_start + <span class=\"hljs-number\">32</span>; si += <span class=\"hljs-number\">4</span>) {\n    float4 rhs[<span class=\"hljs-number\">8</span>];\n    <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> u = <span class=\"hljs-number\">0</span>; u &#x3C; <span class=\"hljs-number\">8</span>; ++u) {\n        rhs[u] = ld_vec(&#x26;rhs_s[u + <span class=\"hljs-number\">8</span> * threadIdx.y][si % <span class=\"hljs-number\">32</span>]);\n    }\n\n    <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> ii = <span class=\"hljs-number\">0</span>; ii &#x3C; <span class=\"hljs-number\">8</span>; ++ii) {\n        float4 lhs = ld_vec(&#x26;lhs_s[ii + <span class=\"hljs-number\">8</span> * threadIdx.x][si % <span class=\"hljs-number\">32</span>]);\n        <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> ji = <span class=\"hljs-number\">0</span>; ji &#x3C; <span class=\"hljs-number\">8</span>; ++ji) {\n            vals[ii][ji] += lhs.x * rhs[ji].x;\n            vals[ii][ji] += lhs.y * rhs[ji].y;\n            vals[ii][ji] += lhs.z * rhs[ji].z;\n            vals[ii][ji] += lhs.w * rhs[ji].w;\n        }\n    }\n}\n</code></pre>","title":"LLM.C 中的 CUDA Kernel","date":"2024-07-17"}},"__N_SSG":true}