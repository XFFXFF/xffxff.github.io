{"pageProps":{"postData":{"id":"rwkv","contentHtml":"<p>Transformer self-attention 的时间复杂度是 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>T</mi><mn>2</mn></msup><mi>d</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(T^2d)</annotation></semantics></math></span>， 空间复杂度是 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>T</mi><mn>2</mn></msup><mo>+</mo><mi>T</mi><mi>d</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(T^2 + Td)</annotation></semantics></math></span>， T 是序列长度，d 是 hidden size。而 RWKV 可以将<strong>推理</strong>的时间复杂度降低到 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><mi>T</mi><mi>d</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(Td)</annotation></semantics></math></span>，空间复杂度降低到 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><mi>d</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(d)</annotation></semantics></math></span>。</p>\n<blockquote>\n<p>这里强调了“推理”，训练的话，我还搞不清楚</p>\n</blockquote>\n<p>在 RWKV 之前，有一些对不同 Transformer 架构也是为了降低 attention 的时间复杂度或者空间复杂度。比如 Linear Transformer 和 AFT（Attention Free Transformer）。</p>\n<p><img src=\"/different_transformers.png\" alt=\"\">\n<em>图片来源：<a href=\"https://arxiv.org/pdf/2305.13048.pdf\">https://arxiv.org/pdf/2305.13048.pdf</a></em></p>\n<p>Linear Transformer 的核心思想去掉 scaled-dot attention 中的 softmax，这样计算 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">QK^TV</annotation></semantics></math></span> 时，可以先算 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>K</mi><mi>T</mi></msup><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">K^TV</annotation></semantics></math></span>，这样时间复杂度就变成了 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><mi>T</mi><msup><mi>d</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(Td^2)</annotation></semantics></math></span>。建议阅读 <a href=\"https://spaces.ac.cn/archives/7546\">线性Attention的探索：Attention必须有个Softmax吗？</a></p>\n<p>AFT 的一个改变是在做 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>Q</mi></mrow><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math></span>，<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span>，<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">V</annotation></semantics></math></span> 的相关计算时，不要用 dot product 了，而是改用 element-wise product。</p>\n<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>Y</mi><mi>t</mi></msub><mo>=</mo><msub><mi>σ</mi><mi>q</mi></msub><mo stretchy=\"false\">(</mo><msub><mi>Q</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo><mo>⊙</mo><mfrac><mrow><munderover><mo>∑</mo><mrow><msup><mi>t</mi><mo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\">′</mo></msup><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mi>exp</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><msub><mi>K</mi><msup><mi>t</mi><mo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\">′</mo></msup></msub><mo>+</mo><msub><mi>w</mi><mrow><mi>t</mi><mo separator=\"true\">,</mo><msup><mi>t</mi><mo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\">′</mo></msup></mrow></msub><mo stretchy=\"false\">)</mo><mo>⊙</mo><msub><mi>V</mi><msup><mi>t</mi><mo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\">′</mo></msup></msub></mrow><mrow><munderover><mo>∑</mo><mrow><msup><mi>t</mi><mo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\">′</mo></msup><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mi>exp</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><msub><mi>K</mi><msup><mi>t</mi><mo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\">′</mo></msup></msub><mo>+</mo><msub><mi>w</mi><mrow><mi>t</mi><mo separator=\"true\">,</mo><msup><mi>t</mi><mo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\">′</mo></msup></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">Y_t = \\sigma_q(Q_t) \\odot \\frac{\\sum_{t'=1}^T \\exp(K_{t'} + w_{t,t'}) \\odot V_{t'}}{\\sum_{t'=1}^T \\exp(K_{t'} + w_{t,t'})}</annotation></semantics></math></span>\n<p>这样做相对于 Transformer 的时间复杂度并没有改变，仍然是 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>T</mi><mn>2</mn></msup><mi>d</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(T^2d)</annotation></semantics></math></span>（<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>K</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">K_t</annotation></semantics></math></span> 和 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>V</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">V_t</annotation></semantics></math></span> element wise 的乘法是 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><mi>d</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(d)</annotation></semantics></math></span>，对 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">T</annotation></semantics></math></span> 求和是 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><mi>T</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(T)</annotation></semantics></math></span>，有 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">T</annotation></semantics></math></span> 个 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>Y</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">Y_t</annotation></semantics></math></span>，所以是 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>T</mi><mn>2</mn></msup><mi>d</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(T^2d)</annotation></semantics></math></span>），但是空间复杂度降低到 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><mi>T</mi><mi>d</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(Td)</annotation></semantics></math></span>，因为不需要保存 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">QK^T</annotation></semantics></math></span> 生成的 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>T</mi><mo>×</mo><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">T \\times T</annotation></semantics></math></span> 的矩阵。建议阅读 <a href=\"https://zhuanlan.zhihu.com/p/614311961\">RWKV的RNN CNN二象性</a> 中关于 AFT 的部分。</p>\n<blockquote>\n<p>只有在 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">T</annotation></semantics></math></span> 足够大的时候，Transformer 的 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>T</mi><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">T^2</annotation></semantics></math></span> 时间复杂度和空间复杂度才是值得重视的问题，我们上面都是在分析 Transformer attention 相关的复杂度，实际上在 LLM 中，当 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">T</annotation></semantics></math></span> 没有足够大的时候，FFN 层的计算量可能会比 attention 层的计算量大很多。推荐阅读：<a href=\"https://spaces.ac.cn/archives/8610\">线性Transformer应该不是你要等的那个模型</a></p>\n</blockquote>\n<h2>RWKV</h2>\n<h3>RWKV 和 RNN 的关系</h3>\n<p><em>Reinventing RNNs for the Transformer Era</em>，RWKV 的标题非常霸气，RWKV 真的是一个传统 RNN 模型吗？</p>\n<p>RWKV 的新颖之处在于它的 “attention”（RWKV 中叫做 WKV） 可以写成 RNN 的形式</p>\n<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>w</mi><mi>k</mi><msub><mi>v</mi><mi>t</mi></msub><mo>=</mo><mfrac><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></munderover><msup><mi>e</mi><mrow><mo>−</mo><mo stretchy=\"false\">(</mo><mi>t</mi><mo>−</mo><mn>1</mn><mo>−</mo><mi>i</mi><mo stretchy=\"false\">)</mo><mi>w</mi><mo>+</mo><msub><mi>k</mi><mi>i</mi></msub></mrow></msup><mo>⊙</mo><msub><mi>v</mi><mi>i</mi></msub><mo>+</mo><msup><mi>e</mi><mrow><mi>u</mi><mo>+</mo><msub><mi>k</mi><mi>t</mi></msub></mrow></msup><mo>⊙</mo><msub><mi>v</mi><mi>t</mi></msub></mrow><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></munderover><msup><mi>e</mi><mrow><mo>−</mo><mo stretchy=\"false\">(</mo><mi>t</mi><mo>−</mo><mn>1</mn><mo>−</mo><mi>i</mi><mo stretchy=\"false\">)</mo><mi>w</mi><mo>+</mo><msub><mi>k</mi><mi>i</mi></msub></mrow></msup><mo>+</mo><msup><mi>e</mi><mrow><mi>u</mi><mo>+</mo><msub><mi>k</mi><mi>t</mi></msub></mrow></msup></mrow></mfrac><mi mathvariant=\"normal\">.</mi></mrow><annotation encoding=\"application/x-tex\">wk v_t = \\frac{\\sum_{i=1}^{t-1} e^{-(t-1-i)w+k_i} \\odot v_i + e^{u+k_t} \\odot v_t}{\\sum_{i=1}^{t-1} e^{-(t-1-i)w+k_i} + e^{u+k_t}}.</annotation></semantics></math></span>\n<p>令</p>\n<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>α</mi><mi>t</mi></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></munderover><msup><mi>e</mi><mrow><mo>−</mo><mo stretchy=\"false\">(</mo><mi>t</mi><mo>−</mo><mi>i</mi><mo stretchy=\"false\">)</mo><mi>w</mi><mo>+</mo><msub><mi>k</mi><mi>i</mi></msub></mrow></msup><mo>⊙</mo><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\alpha_{t} = \\sum_{i=1}^{t} e^{-(t-i)w+k_i} \\odot v_i </annotation></semantics></math></span>\n<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>β</mi><mi>t</mi></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></munderover><msup><mi>e</mi><mrow><mo>−</mo><mo stretchy=\"false\">(</mo><mi>t</mi><mo>−</mo><mi>i</mi><mo stretchy=\"false\">)</mo><mi>w</mi><mo>+</mo><msub><mi>k</mi><mi>i</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\beta_{t} = \\sum_{i=1}^{t} e^{-(t-i)w+k_i}</annotation></semantics></math></span>\n<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>w</mi><mi>k</mi><msub><mi>v</mi><mi>t</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>α</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><msup><mi>e</mi><mrow><mi>u</mi><mo>+</mo><msub><mi>k</mi><mi>t</mi></msub></mrow></msup><mo>⊙</mo><msub><mi>v</mi><mi>t</mi></msub></mrow><mrow><msub><mi>β</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><msup><mi>e</mi><mrow><mi>u</mi><mo>+</mo><msub><mi>k</mi><mi>t</mi></msub></mrow></msup></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">wk v_t = \\frac{\\alpha_{t-1} + e^{u+k_t} \\odot v_t}{\\beta_{t-1} + e^{u+k_t}}</annotation></semantics></math></span>\n<p>写成 RNN 形式的好处是计算 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>w</mi><mi>k</mi><msub><mi>v</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">wk v_t</annotation></semantics></math></span> 的时候可以利用之前的状态 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>α</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\alpha_{t-1}</annotation></semantics></math></span> 和 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>β</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\beta_{t-1}</annotation></semantics></math></span>，这样计算 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>w</mi><mi>k</mi><msub><mi>v</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">wk v_t</annotation></semantics></math></span> 的时间复杂度就是 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><mi>d</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(d)</annotation></semantics></math></span>，空间复杂度也是 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><mi>d</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(d)</annotation></semantics></math></span>。</p>\n<p>推荐阅读 <a href=\"https://zhuanlan.zhihu.com/p/614311961\">RWKV的RNN CNN二象性</a> 中关于 RWKV 的部分，作者清晰的解释了 RWKV 和 AFT 的关系，以及如何直观理解 wkv。</p>\n<h3>Token Shift</h3>\n<p>传统的 Transformer 在 self-attention 之前会对输入 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span> 做 linear projections 得到 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>Q</mi></mrow><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math></span>，<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span>，<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">V</annotation></semantics></math></span>。RWKV 的不同之处在于并不是直接对 <span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span> 做 linear projection，而是对 current inputs 和 previous inputs 做一个线性插值后再做 linear projection。这个线性插值的过程就是 token shift。即：</p>\n<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>r</mi><mi>t</mi></msub><mo>=</mo><msub><mi>W</mi><mi>r</mi></msub><mo>⋅</mo><mo stretchy=\"false\">(</mo><msub><mi>μ</mi><mi>r</mi></msub><mo>⊙</mo><msub><mi>x</mi><mi>t</mi></msub><mo>+</mo><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><msub><mi>μ</mi><mi>r</mi></msub><mo stretchy=\"false\">)</mo><mo>⊙</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">r_t = W_{r} \\cdot (\\mu_r \\odot x_t + (1 - \\mu_r) \\odot x_{t-1})</annotation></semantics></math></span>\n<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>k</mi><mi>t</mi></msub><mo>=</mo><msub><mi>W</mi><mi>k</mi></msub><mo>⋅</mo><mo stretchy=\"false\">(</mo><msub><mi>μ</mi><mi>k</mi></msub><mo>⊙</mo><msub><mi>x</mi><mi>t</mi></msub><mo>+</mo><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><msub><mi>μ</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo><mo>⊙</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">k_t = W_{k} \\cdot (\\mu_k \\odot x_t + (1 - \\mu_k) \\odot x_{t-1})</annotation></semantics></math></span>\n<span class=\"katex\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>v</mi><mi>t</mi></msub><mo>=</mo><msub><mi>W</mi><mi>v</mi></msub><mo>⋅</mo><mo stretchy=\"false\">(</mo><msub><mi>μ</mi><mi>v</mi></msub><mo>⊙</mo><msub><mi>x</mi><mi>t</mi></msub><mo>+</mo><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><msub><mi>μ</mi><mi>v</mi></msub><mo stretchy=\"false\">)</mo><mo>⊙</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">v_t = W_{v} \\cdot (\\mu_v \\odot x_t + (1 - \\mu_v) \\odot x_{t-1})</annotation></semantics></math></span>\n<p>感觉 token shift 很像 kernel size 为 2 的卷积</p>\n<p><img src=\"/token_shift.png\" alt=\"\"></p>\n<p>作者几年前就提出了 token shift 的想法，参见 <a href=\"https://zhuanlan.zhihu.com/p/399480671\">Time-shift: 一行代码，免费提高 Transformer 性能（无参数，无耗时）</a></p>\n<h3>局限性</h3>\n<ul>\n<li>超长上下文任务上效果受限：RWKV 这种递归架构限制了它回顾之前 token 的能力，不像 self-attention 可以保留所有 token 的信息。</li>\n<li>对 prompt 比较敏感：用苏剑林的话说，RWKV 只会做闭卷考试，不会做开卷考试（不会往前翻书），prompt 中在一开始描述任务比较好，带着问题去阅读后续内容。Prompt \"For the document below do X\" 好于 \"For the document above do X\"。参考<a href=\"https://www.zhihu.com/question/602564718/answer/3062973388\">苏剑林的回答</a></li>\n</ul>","title":"RWKV","date":"2024-01-06"}},"__N_SSG":true}