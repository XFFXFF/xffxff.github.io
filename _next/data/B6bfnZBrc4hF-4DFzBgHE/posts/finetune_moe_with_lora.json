{"pageProps":{"postData":{"id":"finetune_moe_with_lora","contentHtml":"<h2>Finetune MoE with LoRA</h2>\n<p>用 LoRA 训练 MoE 非常的慢，在 H 卡上 gpu 利用率不到 20%，profile 后发现主要是因为 experts 的计算太慢了。每一层有 64 个 expert，每个 expert 都是一个 mlp，包含一个 gate linear layer、up linear layer 和 down linear layer。</p>\n<p><img src=\"/moe_lora/image.png\" alt=\"alt text\"></p>\n<p><img src=\"/moe_lora/image-2.png\" alt=\"alt text\"></p>\n<p>对于每一个 linear layer，LoRA 会再注入两个小 linear layer，如下图。比如原本的是 <code>nn.Linear(1024, 1024)</code>，rank 为 16 的 LoRA 会注入 <code>nn.Linear(1024, 16)</code> 和 <code>nn.Linear(16, 1024)</code>。</p>\n<p><img src=\"/moe_lora/image-1.png\" alt=\"alt text\"></p>\n<p><img src=\"/moe_lora/image-4.png\" alt=\"alt text\"></p>\n<p>LoRA 的 linear 是一些更小的 linear layer，对于 GPU 来说是 memory bound 的，导致 GPU 的利用率很低。</p>\n<p>优化这一块，很容易想到使用 grouped gemm，这也是 MoE 的基操。</p>\n<h2>Grouped GEMM</h2>\n<p>当前 experts 是一个 <code>SequentialMLP</code>，假设有 64 个 expert，for 循环这些 expert，且分配给每个 expert 的 token 数不一样， 使得 expert 接收的输入 shape 不一样</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">SequentialMLP</span>(nn.Module):\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self, config</span>):\n        <span class=\"hljs-built_in\">super</span>().__init__()\n        self.config = config\n        self.experts = nn.ModuleList(\n            [MLP(config) <span class=\"hljs-keyword\">for</span> _ <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(config.moe_num_experts)]\n        )\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">forward</span>(<span class=\"hljs-params\">self, permuted_tokens, tokens_per_expert</span>):\n        output = torch.zeros_like(permuted_tokens)\n\n        cumsum_num_tokens = torch.cumsum(tokens_per_expert, dim=<span class=\"hljs-number\">0</span>)\n        <span class=\"hljs-comment\"># Insert zero at the begining for offset index's convenience</span>\n        zero_tensor = torch.zeros(<span class=\"hljs-number\">1</span>, dtype=torch.long, device=cumsum_num_tokens.device)\n        cumsum_num_tokens = torch.cat((zero_tensor, cumsum_num_tokens))\n\n        <span class=\"hljs-keyword\">for</span> expert_num, expert <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">enumerate</span>(self.experts):\n            start = cumsum_num_tokens[expert_num]\n            end = cumsum_num_tokens[expert_num + <span class=\"hljs-number\">1</span>]\n            tokens = permuted_tokens[start:end]\n\n            out = expert(tokens)\n            output[start:end] = out\n        <span class=\"hljs-keyword\">return</span> output\n</code></pre>\n<p>这样做有两个性能问题：一是非常多的 kernel launch，二是每个 expert 接收的输入 shape 不一样，每个 expert 的 workload 不一样，而且使用 LoRA 后这个问题更加严重，\n每个 LoRA layer 都是很小的 linear，会导致更多的 SM 空闲。这里的问题就是我们上一篇文章中讨论的 <a href=\"https://xffxff.github.io/posts/how_to_write_a_cuda_program\">Wave Quantization</a></p>\n<p><img src=\"/moe_lora/image-5.png\" alt=\"alt text\"></p>\n<p>那如果我们所有 expert 的 workload 合到一起，只需一次 kernel launch，work load 也变大了，在 SM 上有更好的 load balance，就可以极大减少 SM 的空闲。</p>\n<p><img src=\"/moe_lora/image-6.png\" alt=\"alt text\"></p>\n<p>可以参考 <a href=\"https://triton-lang.org/main/getting-started/tutorials/08-grouped-gemm.html\">Grouped GEMM</a> 的 triton 实现，Grouped GEMM 非常符合 <a href=\"https://xffxff.github.io/posts/how_to_write_a_cuda_program\">上一篇文章</a> 中 “Don't map threads to data; map data to threads” 的思想，map data to SM！</p>\n<h2>Finetune with Grouped GEMM</h2>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">GroupedMLP</span>(nn.Module):\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self, config: MoYIConfig</span>):\n        <span class=\"hljs-built_in\">super</span>().__init__()\n        self.config = config\n\n        fc1_output_size = config.moe_intermediate_size * config.moe_num_experts\n        <span class=\"hljs-keyword\">if</span> config.hidden_act == <span class=\"hljs-string\">\"silu\"</span>:\n            fc1_output_size *= <span class=\"hljs-number\">2</span>\n\n        fc2_input_size = config.moe_intermediate_size * config.moe_num_experts\n        self.weight1 = nn.Parameter(torch.empty(config.hidden_size, fc1_output_size))\n\n        self.weight2 = nn.Parameter(torch.empty(fc2_input_size, config.hidden_size))\n\n        <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">glu</span>(<span class=\"hljs-params\">x</span>):\n            x = torch.chunk(x, <span class=\"hljs-number\">2</span>, dim=-<span class=\"hljs-number\">1</span>)\n            <span class=\"hljs-keyword\">return</span> F.silu(x[<span class=\"hljs-number\">0</span>]) * x[<span class=\"hljs-number\">1</span>]\n\n        self.activation_func = glu\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">forward</span>(<span class=\"hljs-params\">self, permuted_tokens, tokens_per_expert</span>):\n        <span class=\"hljs-keyword\">from</span> grouped_gemm <span class=\"hljs-keyword\">import</span> ops\n\n        w1 = self.weight1.view(self.config.moe_num_experts, self.config.hidden_size, -<span class=\"hljs-number\">1</span>)\n        w2 = self.weight2.view(self.config.moe_num_experts, -<span class=\"hljs-number\">1</span>, self.config.hidden_size)\n\n        fc1_output = ops.gmm(permuted_tokens, w1, tokens_per_expert, trans_b=<span class=\"hljs-literal\">False</span>)\n\n        fc1_output = self.activation_func(fc1_output)\n\n        fc2_output = ops.gmm(fc1_output, w2, tokens_per_expert, trans_b=<span class=\"hljs-literal\">False</span>)\n        <span class=\"hljs-keyword\">return</span> fc2_output\n</code></pre>\n<p>但是使用 Grouped GEMM 的话，<a href=\"https://huggingface.co/docs/peft/index\">PEFT</a> 这个库的 LoRA 不支持 <code>GroupedMLP</code> 这个 moudle。尝试使用 <a href=\"https://huggingface.co/docs/peft/v0.12.0/en/developer_guides/custom_models#experimental-support-for-dynamic-dispatch-of-custom-modules-in-lora\">custom models</a>，经过一番尝试，我们对 <code>GroupedMLP</code> 进行了修改</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">GroupedGEMM</span>(nn.Module):\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self, in_features, out_features, groups</span>):\n        <span class=\"hljs-built_in\">super</span>().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.groups = groups\n        self.weight = nn.Parameter(torch.empty(groups, in_features, out_features))\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">forward</span>(<span class=\"hljs-params\">self, <span class=\"hljs-built_in\">input</span>, tokens_per_expert</span>):\n        <span class=\"hljs-keyword\">from</span> grouped_gemm <span class=\"hljs-keyword\">import</span> ops\n        <span class=\"hljs-keyword\">return</span> ops.gmm(<span class=\"hljs-built_in\">input</span>, self.weight, tokens_per_expert)\n\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">GroupedMLP</span>(nn.Module):\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self, config: MoYIConfig</span>) -> <span class=\"hljs-literal\">None</span>:\n        <span class=\"hljs-built_in\">super</span>().__init__()\n        self.config = config\n        self.fc1 = GroupedGEMM(\n            config.hidden_size, config.moe_intermediate_size * <span class=\"hljs-number\">2</span>, config.moe_num_experts\n        )\n        self.fc2 = GroupedGEMM(\n            config.moe_intermediate_size, config.hidden_size, config.moe_num_experts\n        )\n\n        <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">glu</span>(<span class=\"hljs-params\">x</span>):\n            x = torch.chunk(x, <span class=\"hljs-number\">2</span>, dim=-<span class=\"hljs-number\">1</span>)\n            <span class=\"hljs-keyword\">return</span> F.silu(x[<span class=\"hljs-number\">0</span>]) * x[<span class=\"hljs-number\">1</span>]\n\n        self.activation_func = glu\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">forward</span>(<span class=\"hljs-params\">self, permuted_tokens, tokens_per_expert</span>):\n        fc1_output = self.fc1(permuted_tokens, tokens_per_expert)\n        fc1_output = self.activation_func(fc1_output)\n        fc2_output = self.fc2(fc1_output, tokens_per_expert)\n        <span class=\"hljs-keyword\">return</span> fc2_output\n</code></pre>\n<p>定义了一个 <code>GroupedGEMM</code>，这个 module 和 <code>nn.Linear</code> 的接口相似，下面就可以仿照 <a href=\"https://github.com/huggingface/peft/blob/850eeb5c3a5cf692f5612c7c733b13fde184e05d/src/peft/tuners/lora/layer.py#L374\"><code>lora.layer.Linear</code></a> 去定义 <code>GroupedGEMM</code> 的 LoRA 了。</p>\n<h2>Reference</h2>\n<ul>\n<li><a href=\"https://triton-lang.org/main/getting-started/tutorials/08-grouped-gemm.html\">Grouped GEMM implemented in Triton</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=_rrhYbvNIx0\">video: Write Grouped GEMMs in Triton Nvidia</a></li>\n</ul>","title":"Finetune MoE with LoRA","date":"2024-08-29"}},"__N_SSG":true}