{"pageProps":{"postData":{"id":"grouped_gemm","contentHtml":"<p>最近瞅了一眼 <a href=\"https://github.com/tgale96/grouped_gemm\">grouped gemm</a> 的代码，发现和我理解的 grouped gemm 有很大差异（我<a href=\"./finetune_moe_with_lora.md\">上篇博客</a>中有大概介绍 grouped gemm 的原理）。这里 grouped gemm 的实现就是一个简单的 for 循环，然后调用 cublas 的 gemm 函数。</p>\n<pre><code class=\"hljs language-cpp\"><span class=\"hljs-function\"><span class=\"hljs-type\">void</span> <span class=\"hljs-title\">CublasGroupedGemm</span><span class=\"hljs-params\">(torch::Tensor a,\n\t\t       torch::Tensor b,\n\t\t       torch::Tensor c,\n\t\t       torch::Tensor batch_sizes,\n\t\t       <span class=\"hljs-type\">bool</span> trans_b)</span> </span>{\n  <span class=\"hljs-type\">int64_t</span> bs = batch_sizes.<span class=\"hljs-built_in\">size</span>(<span class=\"hljs-number\">0</span>), k = a.<span class=\"hljs-built_in\">size</span>(<span class=\"hljs-number\">1</span>);\n  <span class=\"hljs-type\">int64_t</span> n = trans_b ? b.<span class=\"hljs-built_in\">size</span>(<span class=\"hljs-number\">1</span>) : b.<span class=\"hljs-built_in\">size</span>(<span class=\"hljs-number\">2</span>);\n  <span class=\"hljs-type\">int64_t</span> b_rows = b.<span class=\"hljs-built_in\">size</span>(<span class=\"hljs-number\">1</span>), b_cols = b.<span class=\"hljs-built_in\">size</span>(<span class=\"hljs-number\">2</span>);\n  c10::BFloat16* a_ptr = a.<span class=\"hljs-built_in\">data_ptr</span>&#x3C;c10::BFloat16>();\n  c10::BFloat16* b_ptr = b.<span class=\"hljs-built_in\">data_ptr</span>&#x3C;c10::BFloat16>();\n  c10::BFloat16* c_ptr = c.<span class=\"hljs-built_in\">data_ptr</span>&#x3C;c10::BFloat16>();\n  <span class=\"hljs-keyword\">for</span> (<span class=\"hljs-type\">int</span> i = <span class=\"hljs-number\">0</span>; i &#x3C; bs; ++i) {\n    <span class=\"hljs-type\">int64_t</span> m = batch_sizes.<span class=\"hljs-built_in\">data_ptr</span>&#x3C;<span class=\"hljs-type\">int64_t</span>>()[i];\n    <span class=\"hljs-built_in\">CublasGemm</span>(a_ptr, m, k, <span class=\"hljs-comment\">/*trans_a=*/</span><span class=\"hljs-literal\">false</span>,\n\t       b_ptr, b_rows, b_cols, trans_b,\n\t       c_ptr, m, n);\n    a_ptr += m * k;\n    b_ptr += b_rows * b_cols;\n    c_ptr += m * n;\n  }\n}\n</code></pre>\n<p>这个在逻辑上和下面的 PyTorch 实现是等价的：</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">sequential_gemm</span>(<span class=\"hljs-params\"><span class=\"hljs-built_in\">input</span>, weight, batch_sizes</span>):\n    n = <span class=\"hljs-built_in\">input</span>.shape[<span class=\"hljs-number\">0</span>]\n    out_features = weight.shape[-<span class=\"hljs-number\">1</span>]\n    output = torch.zeros(\n        n, out_features, dtype=<span class=\"hljs-built_in\">input</span>.dtype, device=<span class=\"hljs-built_in\">input</span>.device\n    )\n\n    cumsum_batch_sizes = torch.cumsum(batch_sizes, dim=<span class=\"hljs-number\">0</span>)\n    <span class=\"hljs-comment\"># Insert zero at the beginning for offset index's convenience</span>\n    zero_tensor = torch.zeros(<span class=\"hljs-number\">1</span>, dtype=torch.long, device=cumsum_batch_sizes.device)\n    cumsum_batch_sizes = torch.cat((zero_tensor, cumsum_batch_sizes))\n\n    <span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(weight.shape[<span class=\"hljs-number\">0</span>]):\n        start = cumsum_batch_sizes[i]\n        end = cumsum_batch_sizes[i + <span class=\"hljs-number\">1</span>]\n        input_for_this_batch = <span class=\"hljs-built_in\">input</span>[start:end]\n\n        out = torch.matmul(input_for_this_batch, weight[i])\n        output[start:end] = out\n    <span class=\"hljs-keyword\">return</span> output\n</code></pre>\n<p>我们可以对这两个实现进行 benchmark：</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> grouped_gemm <span class=\"hljs-keyword\">import</span> ops\n<span class=\"hljs-keyword\">import</span> triton\n<span class=\"hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\">import</span> triton.testing\n\n\n<span class=\"hljs-meta\">@triton.testing.perf_report(<span class=\"hljs-params\">\n    triton.testing.Benchmark(<span class=\"hljs-params\">\n        x_names=[<span class=\"hljs-string\">'num_groups'</span>],\n        x_vals=[<span class=\"hljs-number\">2</span>**i <span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-params\"><span class=\"hljs-number\">3</span>, <span class=\"hljs-number\">8</span></span>)],\n        line_arg=<span class=\"hljs-string\">'provider'</span>,\n        line_vals=[<span class=\"hljs-string\">'sequential'</span>, <span class=\"hljs-string\">'grouped'</span>],\n        line_names=[<span class=\"hljs-string\">\"Sequential GEMM\"</span>, <span class=\"hljs-string\">\"Grouped GEMM\"</span>],\n        styles=[(<span class=\"hljs-params\"><span class=\"hljs-string\">'green'</span>, <span class=\"hljs-string\">'-'</span></span>), (<span class=\"hljs-params\"><span class=\"hljs-string\">'blue'</span>, <span class=\"hljs-string\">'-'</span></span>)],\n        ylabel=<span class=\"hljs-string\">\"runtime(ms)\"</span>,\n        plot_name=<span class=\"hljs-string\">\"sequential-vs-grouped-gemm-performance\"</span>,\n        args={},\n    </span>)</span>)</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">benchmark</span>(<span class=\"hljs-params\">num_groups, provider</span>):\n    num_groups = num_groups\n    n = <span class=\"hljs-number\">24576</span>\n    hidden_size = <span class=\"hljs-number\">1024</span>\n\n    a = torch.randn(n, hidden_size).view(-<span class=\"hljs-number\">1</span>, hidden_size)\n    b = torch.randn(num_groups, hidden_size, hidden_size)\n\n    dist = torch.rand(num_groups, )\n    dist /= dist.<span class=\"hljs-built_in\">sum</span>()\n    batch_sizes = (dist * n).to(torch.long)\n    error = n - batch_sizes.<span class=\"hljs-built_in\">sum</span>()\n    batch_sizes[-<span class=\"hljs-number\">1</span>] += error\n    <span class=\"hljs-keyword\">assert</span> batch_sizes.<span class=\"hljs-built_in\">sum</span>() == n\n\n    a = a.to(torch.bfloat16).cuda()\n    b = b.to(torch.bfloat16).cuda()\n\n    quantiles = [<span class=\"hljs-number\">0.5</span>, <span class=\"hljs-number\">0.2</span>, <span class=\"hljs-number\">0.8</span>]\n    <span class=\"hljs-keyword\">if</span> provider == <span class=\"hljs-string\">'sequential'</span>:\n        ms, min_ms, max_ms = triton.testing.do_bench(<span class=\"hljs-keyword\">lambda</span>: sequential_gemm(a, b, batch_sizes), quantiles=quantiles)\n    <span class=\"hljs-keyword\">if</span> provider == <span class=\"hljs-string\">'grouped'</span>:\n        ms, min_ms, max_ms = triton.testing.do_bench(<span class=\"hljs-keyword\">lambda</span>: ops.gmm(a, b, batch_sizes), quantiles=quantiles)\n    <span class=\"hljs-keyword\">return</span> ms, max_ms, min_ms\n\nbenchmark.run(show_plots=<span class=\"hljs-literal\">True</span>, print_data=<span class=\"hljs-literal\">True</span>)\n</code></pre>\n<p>得到的结果如下：</p>\n<pre><code>sequential-vs-grouped-gemm-performance:\n   num_groups  Sequential GEMM  Grouped GEMM\n0         8.0         0.241248      0.151296\n1        16.0         0.464256      0.229712\n2        32.0         0.914784      0.367968\n3        64.0         1.717008      0.613440\n4       128.0         3.228896      1.117088\n</code></pre>\n<p>可以看到，即使这两个实现在逻辑上是等价的，PyTorch 底层也是调用的 cublas 的 gemm 函数，grouped gemm 的性能要比 sequential gemm 的性能好很多。</p>\n<p><img src=\"/grouped_gemm/image.png\" alt=\"sequential gemm\"></p>\n<p><img src=\"/grouped_gemm/image-1.png\" alt=\"grouped gemm\"></p>\n<p>通过 profile 可以看到，grouped gemm 的主要耗时在 kernel launch 以及 gemm 的计算，而 sequential gemm 会有很多额外的 overhead，包括几个 select，slice 操作，以及最后的 memory copy。另外，<code>aten::matmul</code> 相比直接调用 cublas 的 gemm 函数，也有一些额外的 overhead。</p>\n<p><a href=\"https://github.com/fanshiqing/grouped_gemm\">这个仓库</a>是 fork 的 <a href=\"https://github.com/tgale96/grouped_gemm\">tgale96/grouped_gemm</a>，<a href=\"https://github.com/fanshiqing/grouped_gemm/blob/172fada89fa7364fe5d026b3a0dfab58b591ffdd/csrc/grouped_gemm.cu#L288-L310\">使用 multi stream 做了优化</a>，benchmark 结果如下：</p>\n<pre><code>   num_groups  Sequential GEMM  Grouped GEMM\n0         8.0         0.227760      0.145312\n1        16.0         0.480816      0.183040\n2        32.0         0.982000      0.280992\n3        64.0         1.731872      0.467136\n4       128.0         3.198544      0.803168\n</code></pre>\n<p>可以看到，使用 multi stream 优化后，性能有明显提升。</p>\n<p><img src=\"/grouped_gemm/image-2.png\" alt=\"alt text\">\n（这里使用了 4 个 stream）</p>\n<p>这里的场景是非常适合使用 multi stream 的，因为每个 gemm 都是独立的，由于 gemm 的大小又不一样，每个 gemm 的 workload 不一样，single stream 很容易有一些 GPU compute units 是空闲的。multi stream 可以增大 GPU compute units 的利用率，空闲的 compute units 可以计算其他 stream 中的 gemm。参考下面这张图</p>\n<p><img src=\"/how_to_write_a_cuda_program/image-4.png\" alt=\"\"></p>","title":"Grouped GEMM","date":"2024-09-27"}},"__N_SSG":true}