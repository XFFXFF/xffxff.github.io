{"pageProps":{"postData":{"id":"how_to_write_a_cuda_program","contentHtml":"<p><a href=\"https://www.nvidia.com/en-us/on-demand/session/gtc24-s62401/\">How To Write A CUDA Program: The Ninja Edition</a> 是 GTC 2024 的一个 talk。收获很多，记录一下，值得反复观看。</p>\n<h2>Wave Quantization</h2>\n<p>当 thread block 的数量不是 SM 数量的整数倍时，在执行剩余的 thread block 时，一些 SM 会空闲。</p>\n<p><img src=\"/how_to_write_a_cuda_program/image.png\" alt=\"alt text\"></p>\n<p>对于 task A -> B -> C，每一个阶段都需要一个 extra parity wave。</p>\n<p><img src=\"/how_to_write_a_cuda_program/image-2.png\" alt=\"alt text\"></p>\n<blockquote>\n<p>Don't map threads to data; map data to threads.</p>\n</blockquote>\n<p>这句话应该怎么理解呢？</p>\n<p><img src=\"/how_to_write_a_cuda_program/image-1.png\" alt=\"alt text\"></p>\n<p>在上面的例子中，图片大小是 1024x1024，将图片划分为一些 block，你想着划分为 16x16 挺好，每个 block 有 64x64 个像素，然后给每个 block 分配一些 thread 来处理，这就是 map threads to data。H100 有 132 个 SM，这样设计的话，需要两个 wave 才能处理完所有 block。</p>\n<p>Map data to threads 的思路是，先考虑 SM 的数量，根据 SM 的数量设计 block 的划分，尽量不让 SM 空闲。</p>\n<blockquote>\n<p>这里可能和 occupancy 容易混淆，occupancy 是指分配给一个 SM 的 warp 数与其支持的最大 warp 数的比例</p>\n</blockquote>\n<p>但是有些情况没办法做到 single-wave 或者 integer-wave：</p>\n<ol>\n<li>一些算法需要特定大小的 tile</li>\n<li>必须考虑不同型号的 GPU（比如 RTX-3090/80/70/60）</li>\n<li>由于使用非 constant 的 tile size 而增加代码复杂度</li>\n<li>不同 block load balance 的问题，可能不会比增加一个 extra partial wave 好多少</li>\n</ol>\n<h2>Task Parallelism</h2>\n<p>在执行 A 的 extra partial wave 时，有很多 SM 空闲，B 是依赖 A 的结果的，所以 B 这时没法利用这些空闲的 SM。但是如果现在有另一个和 A 不相关的的任务，可以让这些 SM 去执行这个任务，这就是 task parallelism。</p>\n<p><img src=\"/how_to_write_a_cuda_program/image-4.png\" alt=\"alt text\"></p>\n<p>这样做整体的吞吐量会提高，但是单一任务的 latency 并不一定会降低，比如图中的 Task 1: A -> B -> C 的 latency 比之前变高了。</p>\n<p><img src=\"/how_to_write_a_cuda_program/image-5.png\" alt=\"alt text\"></p>\n<p>Task parallelism 有一个很大的问题，就是 thrashing cache。同时执行的 task 变多，每个 task 能分配到的 cache 就会变少，需要更频繁的和 global memory 交互。如果 task 是一个 memory-bound 的任务，task parallelism 也不一定会提高整体的吞吐。</p>\n<h2>Keep Data in Cache</h2>\n<blockquote>\n<p>L2 cache == shared memory</p>\n</blockquote>\n<p>对于 task A -> B -> C，有如下过程：</p>\n<ol>\n<li>A 从 global memory 读取数据到 shared memory</li>\n<li>A 计算</li>\n<li>A 将结果写回 global memory</li>\n<li>B 从 global memory 读取数据到 shared memory</li>\n<li>B 计算</li>\n<li>B 将结果写回 global memory</li>\n<li>C 从 global memory 读取数据到 shared memory</li>\n<li>C 计算</li>\n<li>C 将结果写回 global memory</li>\n</ol>\n<p>设想一下如果 task 足够小，小到中间计算结果完全可以放在 shared memory 中，那么就不需要这么多次 global memory 的读写了。执行 A -> B -> C 的过程就变成了：</p>\n<ol>\n<li>A 从 global memory 读取数据到 shared memory</li>\n<li>A 计算</li>\n<li>B 从 shared memory 读取 A 的结果</li>\n<li>B 计算</li>\n<li>C 从 shared memory 读取 B 的结果</li>\n<li>C 计算</li>\n<li>C 将结果写回 global memory</li>\n</ol>\n<p>当然，并不是所有任务都可以这样做，但是如果能找到一种方式将 task 拆分为更小的 task，使得这些小 task 可以完全放在 shared memory 中，让这些小 task 串行执行。</p>\n<p><img src=\"/how_to_write_a_cuda_program/image-8.png\" alt=\"alt text\"></p>\n<p>这其实就是 flash attention 的优化思路。</p>","title":"笔记：How To Write A CUDA Program: The Ninja Edition","date":"2024-07-30"}},"__N_SSG":true}